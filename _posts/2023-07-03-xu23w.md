---
title: 'Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks'
openreview: rNLHeKckZc
abstract: Neural architecture search (NAS) for Graph neural networks (GNNs), called
  NAS-GNNs, has achieved significant performance over manually designed GNN architectures.
  However, these methods inherit issues from the conventional NAS methods, such as
  high computational cost and optimization difficulty. More importantly, previous
  NAS methods have ignored the uniqueness of GNNs, where GNNs possess expressive power
  without training. With the randomly-initialized weights, we can then seek the optimal
  architecture parameters via the sparse coding objective and derive a novel NAS-GNNs
  method, namely neural architecture coding (NAC). Consequently, our NAC holds a no-update
  scheme on GNNs and can efficiently compute in linear time. Empirical evaluations
  on multiple GNN benchmark datasets demonstrate that our approach leads to state-of-the-art
  performance, which is up to $200\times$ faster and $18.8%$ more accurate than the
  strong baselines.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu23w
month: 0
tex_title: 'Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks'
firstpage: 38826
lastpage: 38847
page: 38826-38847
order: 38826
cycles: false
bibtex_author: Xu, Peng and Zhang, Lin and Liu, Xuanzhou and Sun, Jiaqi and Zhao,
  Yue and Yang, Haiqin and Yu, Bei
author:
- given: Peng
  family: Xu
- given: Lin
  family: Zhang
- given: Xuanzhou
  family: Liu
- given: Jiaqi
  family: Sun
- given: Yue
  family: Zhao
- given: Haiqin
  family: Yang
- given: Bei
  family: Yu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/xu23w/xu23w.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
