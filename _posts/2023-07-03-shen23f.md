---
title: Auxiliary Modality Learning with Generalized Curriculum Distillation
openreview: PhSiZuHiVn
abstract: Driven by the need from real-world applications, Auxiliary Modality Learning
  (AML) offers the possibility to utilize more information from auxiliary data in
  training, while only requiring data from one or fewer modalities in test, to save
  the overall computational cost and reduce the amount of input data for inferencing.
  In this work, we formally define “Auxiliary Modality Learning” (AML), systematically
  classify types of auxiliary modality (in visual computing) and architectures for
  AML, and analyze their performance. We also analyze the conditions under which AML
  works well from the optimization and data distribution perspectives. To guide various
  choices to achieve optimal performance using AML, we propose a novel method to assist
  in choosing the best auxiliary modality and estimating an upper bound performance
  before executing AML. In addition, we propose a new AML method using generalized
  curriculum distillation to enable more effective curriculum learning. Our method
  achieves the best performance compared to other SOTA methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shen23f
month: 0
tex_title: Auxiliary Modality Learning with Generalized Curriculum Distillation
firstpage: 31057
lastpage: 31076
page: 31057-31076
order: 31057
cycles: false
bibtex_author: Shen, Yu and Wang, Xijun and Gao, Peng and Lin, Ming
author:
- given: Yu
  family: Shen
- given: Xijun
  family: Wang
- given: Peng
  family: Gao
- given: Ming
  family: Lin
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/shen23f/shen23f.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
