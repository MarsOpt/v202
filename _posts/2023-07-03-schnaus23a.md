---
title: Learning Expressive Priors for Generalization and Uncertainty Estimation in
  Neural Networks
openreview: YzSeC2HsMz
abstract: 'In this work, we propose a novel prior learning method for advancing generalization
  and uncertainty estimation in deep neural networks. The key idea is to exploit scalable
  and structured posteriors of neural networks as informative priors with generalization
  guarantees. Our learned priors provide expressive probabilistic representations
  at large scale, like Bayesian counterparts of pre-trained models on ImageNet, and
  further produce non-vacuous generalization bounds. We also extend this idea to a
  continual learning framework, where the favorable properties of our priors are desirable.
  Major enablers are our technical contributions: (1) the sums-of-Kronecker-product
  computations, and (2) the derivations and optimizations of tractable objectives
  that lead to improved generalization bounds. Empirically, we exhaustively show the
  effectiveness of this method for uncertainty estimation and generalization.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: schnaus23a
month: 0
tex_title: Learning Expressive Priors for Generalization and Uncertainty Estimation
  in Neural Networks
firstpage: 30252
lastpage: 30284
page: 30252-30284
order: 30252
cycles: false
bibtex_author: Schnaus, Dominik and Lee, Jongseok and Cremers, Daniel and Triebel,
  Rudolph
author:
- given: Dominik
  family: Schnaus
- given: Jongseok
  family: Lee
- given: Daniel
  family: Cremers
- given: Rudolph
  family: Triebel
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/schnaus23a/schnaus23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
