---
title: Quantized Distributed Training of Large Models with Convergence Guarantees
openreview: Nqp8A5IDzq
abstract: 'Communication-reduction techniques are a popular way to improve scalability
  in data-parallel training of deep neural networks (DNNs). The recent emergence of
  large language models such as GPT has created the need for new approaches to exploit
  data-parallelism. Among these, fully-sharded data parallel (FSDP) training is highly
  popular, yet it still encounters scalability bottlenecks. One reason is that applying
  compression techniques to FSDP is challenging: as the vast majority of the communication
  involves the modelâ€™s weights, direct compression alters convergence and leads to
  accuracy loss. We present QSDP, a variant of FSDP which supports both gradient and
  weight quantization with theoretical guarantees, is simple to implement and has
  essentially no overheads. To derive QSDP we prove that a natural modification of
  SGD achieves convergence even when we only maintain quantized weights, and thus
  the domain over which we train consists of quantized points and is, therefore, highly
  non-convex. We validate this approach by training GPT-family models with up to 1.3
  billion parameters on a multi-node cluster. Experiments show that QSDP preserves
  model accuracy, while completely removing the communication bottlenecks of FSDP,
  providing end-to-end speedups of up to 2.2x.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: markov23a
month: 0
tex_title: Quantized Distributed Training of Large Models with Convergence Guarantees
firstpage: 24020
lastpage: 24044
page: 24020-24044
order: 24020
cycles: false
bibtex_author: Markov, Ilia and Vladu, Adrian and Guo, Qi and Alistarh, Dan
author:
- given: Ilia
  family: Markov
- given: Adrian
  family: Vladu
- given: Qi
  family: Guo
- given: Dan
  family: Alistarh
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/markov23a/markov23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
