---
title: 'SpeedDETR: Speed-aware Transformers for End-to-end Object Detection'
openreview: 5VdcSxrlTK
abstract: Vision Transformers (ViTs) have continuously achieved new milestones in
  object detection. However, the considerable computation and memory burden compromise
  their efficiency and generalization of deployment on resource-constraint devices.
  Besides, efficient transformer-based detectors designed by existing works can hardly
  achieve a realistic speedup, especially on multi-core processors (e.g., GPUs). The
  main issue is that the current literature solely concentrates on building algorithms
  with minimal computation, oblivious that the practical latency can also be affected
  by the memory access cost and the degree of parallelism. Therefore, we propose SpeedDETR,
  a novel speed-aware transformer for end-to-end object detectors, achieving high-speed
  inference on multiple devices. Specifically, we design a latency prediction model
  which can directly and accurately estimate the network latency by analyzing network
  properties, hardware memory access pattern, and degree of parallelism. Following
  the effective local-to-global visual modeling process and the guidance of the latency
  prediction model, we build our hardware-oriented architecture design and develop
  a new family of SpeedDETR. Experiments on the MS COCO dataset show SpeedDETR outperforms
  current DETR-based methods on Tesla V100. Even acceptable speed inference can be
  achieved on edge GPUs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dong23b
month: 0
tex_title: "{S}peed{DETR}: Speed-aware Transformers for End-to-end Object Detection"
firstpage: 8227
lastpage: 8243
page: 8227-8243
order: 8227
cycles: false
bibtex_author: Dong, Peiyan and Kong, Zhenglun and Meng, Xin and Zhang, Peng and Tang,
  Hao and Wang, Yanzhi and Chou, Chih-Hsien
author:
- given: Peiyan
  family: Dong
- given: Zhenglun
  family: Kong
- given: Xin
  family: Meng
- given: Peng
  family: Zhang
- given: Hao
  family: Tang
- given: Yanzhi
  family: Wang
- given: Chih-Hsien
  family: Chou
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/dong23b/dong23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
