---
title: Sample Complexity of Probability Divergences under Group Symmetry
openreview: lZUSxrYoOY
abstract: We rigorously quantify the improvement in the sample complexity of variational
  divergence estimations for group-invariant distributions. In the cases of the Wasserstein-1
  metric and the Lipschitz-regularized $\alpha$-divergences, the reduction of sample
  complexity is proportional to an ambient-dimension-dependent power of the group
  size. For the maximum mean discrepancy (MMD), the improvement of sample complexity
  is more nuanced, as it depends on not only the group size but also the choice of
  kernel. Numerical simulations verify our theories.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen23p
month: 0
tex_title: Sample Complexity of Probability Divergences under Group Symmetry
firstpage: 4713
lastpage: 4734
page: 4713-4734
order: 4713
cycles: false
bibtex_author: Chen, Ziyu and Katsoulakis, Markos and Rey-Bellet, Luc and Zhu, Wei
author:
- given: Ziyu
  family: Chen
- given: Markos
  family: Katsoulakis
- given: Luc
  family: Rey-Bellet
- given: Wei
  family: Zhu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/chen23p/chen23p.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
