---
title: Improving Adversarial Robustness of Deep Equilibrium Models with Explicit Regulations
  Along the Neural Dynamics
openreview: sikdq1zHiX
abstract: Deep equilibrium (DEQ) models replace the multiple-layer stacking of conventional
  deep networks with a fixed-point iteration of a single-layer transformation. Having
  been demonstrated to be competitive in a variety of real-world scenarios, the adversarial
  robustness of general DEQs becomes increasingly crucial for their reliable deployment.
  Existing works improve the robustness of general DEQ models with the widely-used
  adversarial training (AT) framework, but they fail to exploit the structural uniquenesses
  of DEQ models. To this end, we interpret DEQs through the lens of neural dynamics
  and find that AT under-regulates intermediate states. Besides, the intermediate
  states typically provide predictions with a high prediction entropy. Informed by
  the correlation between the entropy of dynamical systems and their stability properties,
  we propose reducing prediction entropy by progressively updating inputs along the
  neural dynamics. During AT, we also utilize random intermediate states to compute
  the loss function. Our methods regulate the neural dynamics of DEQ models in this
  manner. Extensive experiments demonstrate that our methods substantially increase
  the robustness of DEQ models and even outperform the strong deep network baselines.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang23i
month: 0
tex_title: Improving Adversarial Robustness of Deep Equilibrium Models with Explicit
  Regulations Along the Neural Dynamics
firstpage: 39349
lastpage: 39364
page: 39349-39364
order: 39349
cycles: false
bibtex_author: Yang, Zonghan and Li, Peng and Pang, Tianyu and Liu, Yang
author:
- given: Zonghan
  family: Yang
- given: Peng
  family: Li
- given: Tianyu
  family: Pang
- given: Yang
  family: Liu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/yang23i/yang23i.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
