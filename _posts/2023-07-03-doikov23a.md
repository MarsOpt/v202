---
title: Second-Order Optimization with Lazy Hessians
openreview: Hk2fFm7W8c
abstract: We analyze Newtonâ€™s method with lazy Hessian updates for solving general
  possibly non-convex optimization problems. We propose to reuse a previously seen
  Hessian for several iterations while computing new gradients at each step of the
  method. This significantly reduces the overall arithmetic complexity of second-order
  optimization schemes. By using the cubic regularization technique, we establish
  fast global convergence of our method to a second-order stationary point, while
  the Hessian does not need to be updated each iteration. For convex problems, we
  justify global and local superlinear rates for lazy Newton steps with quadratic
  regularization, which is easier to compute. The optimal frequency for updating the
  Hessian is once every $d$ iterations, where $d$ is the dimension of the problem.
  This provably improves the total arithmetic complexity of second-order algorithms
  by a factor $\sqrt{d}$.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: doikov23a
month: 0
tex_title: Second-Order Optimization with Lazy Hessians
firstpage: 8138
lastpage: 8161
page: 8138-8161
order: 8138
cycles: false
bibtex_author: Doikov, Nikita and Chayti, El Mahdi and Jaggi, Martin
author:
- given: Nikita
  family: Doikov
- given: El Mahdi
  family: Chayti
- given: Martin
  family: Jaggi
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/doikov23a/doikov23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
