---
title: Neural Markov Jump Processes
openreview: ZtvnhohkVk
abstract: Markov jump processes are continuous-time stochastic processes with a wide
  range of applications in both natural and social sciences. Despite their widespread
  use, inference in these models is highly non-trivial and typically proceeds via
  either Monte Carlo or expectation-maximization methods. In this work we introduce
  an alternative, variational inference algorithm for Markov jump processes which
  relies on neural ordinary differential equations, and is trainable via back-propagation.
  Our methodology learns neural, continuous-time representations of the observed data,
  that are used to approximate the initial distribution and time-dependent transition
  probability rates of the posterior Markov jump process. The time-independent rates
  of the prior process are in contrast trained akin to generative adversarial networks.
  We test our approach on synthetic data sampled from ground-truth Markov jump processes,
  experimental switching ion channel data and molecular dynamics simulations. Source
  code to reproduce our experiments is available online.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: seifner23a
month: 0
tex_title: Neural {M}arkov Jump Processes
firstpage: 30523
lastpage: 30552
page: 30523-30552
order: 30523
cycles: false
bibtex_author: Seifner, Patrick and Sanchez, Ramses J
author:
- given: Patrick
  family: Seifner
- given: Ramses J
  family: Sanchez
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/seifner23a/seifner23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
