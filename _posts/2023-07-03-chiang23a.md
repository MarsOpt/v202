---
title: Tighter Bounds on the Expressivity of Transformer Encoders
openreview: XKcogevHj8
abstract: Characterizing neural networks in terms of better-understood formal systems
  has the potential to yield new insights into the power and limitations of these
  networks. Doing so for transformers remains an active area of research. Bhattamishra
  and others have shown that transformer encoders are at least as expressive as a
  certain kind of counter machine, while Merrill and Sabharwal have shown that fixed-precision
  transformer encoders recognize only languages in uniform $TC^0$. We connect and
  strengthen these results by identifying a variant of first-order logic with counting
  quantifiers that is simultaneously an upper bound for fixed-precision transformer
  encoders and a lower bound for transformer encoders. This brings us much closer
  than before to an exact characterization of the languages that transformer encoders
  recognize.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chiang23a
month: 0
tex_title: Tighter Bounds on the Expressivity of Transformer Encoders
firstpage: 5544
lastpage: 5562
page: 5544-5562
order: 5544
cycles: false
bibtex_author: Chiang, David and Cholak, Peter and Pillay, Anand
author:
- given: David
  family: Chiang
- given: Peter
  family: Cholak
- given: Anand
  family: Pillay
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/chiang23a/chiang23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
