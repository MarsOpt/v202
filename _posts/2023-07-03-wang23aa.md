---
title: Data Efficient Neural Scaling Law via Model Reusing
openreview: iXYnIz4RRx
abstract: 'The number of parameters in large transformers has been observed to grow
  exponentially. Despite notable performance improvements, concerns have been raised
  that such a growing model size will run out of data in the near future. As manifested
  in the neural scaling law, modern learning backbones are not data-efficient. To
  maintain the utility of the model capacity, training data should be increased proportionally.
  In this paper, we study the neural scaling law under the previously overlooked data
  scarcity regime, focusing on the more challenging situation where we need to train
  a gigantic model with a disproportionately limited supply of available training
  data. We find that the existing power laws underestimate the data inefficiency of
  large transformers. Their performance will drop significantly if the training set
  is insufficient. Fortunately, we discover another blessing - such a data-inefficient
  scaling law can be restored through a model reusing approach that warm-starts the
  training of a large model by initializing it using smaller models. Our empirical
  study shows that model reusing can effectively reproduce the power law under the
  data scarcity regime. When progressively applying model reusing to expand the model
  size, we also observe consistent performance improvement in large transformers.
  We release our code at: https://github.com/VITA-Group/Data-Efficient-Scaling.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23aa
month: 0
tex_title: Data Efficient Neural Scaling Law via Model Reusing
firstpage: 36193
lastpage: 36204
page: 36193-36204
order: 36193
cycles: false
bibtex_author: Wang, Peihao and Panda, Rameswar and Wang, Zhangyang
author:
- given: Peihao
  family: Wang
- given: Rameswar
  family: Panda
- given: Zhangyang
  family: Wang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wang23aa/wang23aa.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
