---
title: Unscented Autoencoder
openreview: 7INNiSKwpN
abstract: The Variational Autoencoder (VAE) is a seminal approach in deep generative
  modeling with latent variables. Interpreting its reconstruction process as a nonlinear
  transformation of samples from the latent posterior distribution, we apply the Unscented
  Transform (UT) – a well-known distribution approximation used in the Unscented Kalman
  Filter (UKF) from the field of filtering. A finite set of statistics called sigma
  points, sampled deterministically, provides a more informative and lower-variance
  posterior representation than the ubiquitous noise-scaling of the reparameterization
  trick, while ensuring higher-quality reconstruction. We further boost the performance
  by replacing the Kullback-Leibler (KL) divergence with the Wasserstein distribution
  metric that allows for a sharper posterior. Inspired by the two components, we derive
  a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder (UAE),
  trained purely with regularization-like terms on the per-sample posterior. We empirically
  show competitive performance in Fréchet Inception Distance scores over closely-related
  models, in addition to a lower training variance than the VAE.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: janjos23a
month: 0
tex_title: Unscented Autoencoder
firstpage: 14758
lastpage: 14779
page: 14758-14779
order: 14758
cycles: false
bibtex_author: Janjos, Faris and Rosenbaum, Lars and Dolgov, Maxim and Zoellner, J.
  Marius
author:
- given: Faris
  family: Janjos
- given: Lars
  family: Rosenbaum
- given: Maxim
  family: Dolgov
- given: J. Marius
  family: Zoellner
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/janjos23a/janjos23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
