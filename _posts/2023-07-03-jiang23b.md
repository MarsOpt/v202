---
title: 'VIMA: Robot Manipulation with Multimodal Prompts'
openreview: nkDMZ8yqBt
abstract: Prompt-based learning has emerged as a successful paradigm in natural language
  processing, where a single general-purpose language model can be instructed to perform
  any task specified by input prompts. Yet task specification in robotics comes in
  various forms, such as imitating one-shot demonstrations, following language instructions,
  and reaching visual goals. They are often considered different tasks and tackled
  by specialized models. We show that a wide spectrum of robot manipulation tasks
  can be expressed with multimodal prompts, interleaving textual and visual tokens.
  Accordingly, we develop a new simulation benchmark that consists of thousands of
  procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories
  for imitation learning, and a four-level evaluation protocol for systematic generalization.
  We design a transformer-based robot agent, VIMA, that processes these prompts and
  outputs motor actions autoregressively. VIMA features a recipe that achieves strong
  model scalability and data efficiency. It outperforms alternative designs in the
  hardest zero-shot generalization setting by up to $2.9\times$ task success rate
  given the same training data. With $10\times$ less training data, VIMA still performs
  $2.7\times$ better than the best competing variant. Code and video demos are available
  at https://vimalabs.github.io
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jiang23b
month: 0
tex_title: "{VIMA}: Robot Manipulation with Multimodal Prompts"
firstpage: 14975
lastpage: 15022
page: 14975-15022
order: 14975
cycles: false
bibtex_author: Jiang, Yunfan and Gupta, Agrim and Zhang, Zichen and Wang, Guanzhi
  and Dou, Yongqiang and Chen, Yanjun and Fei-Fei, Li and Anandkumar, Anima and Zhu,
  Yuke and Fan, Linxi
author:
- given: Yunfan
  family: Jiang
- given: Agrim
  family: Gupta
- given: Zichen
  family: Zhang
- given: Guanzhi
  family: Wang
- given: Yongqiang
  family: Dou
- given: Yanjun
  family: Chen
- given: Li
  family: Fei-Fei
- given: Anima
  family: Anandkumar
- given: Yuke
  family: Zhu
- given: Linxi
  family: Fan
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/jiang23b/jiang23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
