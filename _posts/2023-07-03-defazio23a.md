---
title: Learning-Rate-Free Learning by D-Adaptation
openreview: GXZ6cT5cvY
abstract: The speed of gradient descent for convex Lipschitz functions is highly dependent
  on the choice of learning rate. Setting the learning rate to achieve the optimal
  convergence rate requires knowing the distance D from the initial point to the solution
  set. In this work, we describe a single-loop method, with no back-tracking or line
  searches, which does not require knowledge of D yet asymptotically achieves the
  optimal rate of convergence for the complexity class of convex Lipschitz functions.
  Our approach is the first parameter-free method for this class without additional
  multiplicative log factors in the convergence rate. We present extensive experiments
  for SGD and Adam variants of our method, where the method automatically matches
  hand-tuned learning rates across more than a dozen diverse machine learning problems,
  including large-scale vision and language problems. Our method is practical, efficient
  and requires no additional function value or gradient evaluations each step. An
  implementation is provided in the supplementary material.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: defazio23a
month: 0
tex_title: Learning-Rate-Free Learning by D-Adaptation
firstpage: 7449
lastpage: 7479
page: 7449-7479
order: 7449
cycles: false
bibtex_author: Defazio, Aaron and Mishchenko, Konstantin
author:
- given: Aaron
  family: Defazio
- given: Konstantin
  family: Mishchenko
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/defazio23a/defazio23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
