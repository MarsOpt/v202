---
title: Understanding and Defending Patched-based Adversarial Attacks for Vision Transformer
openreview: GR4c6Onxfw
abstract: Vision Transformer (ViT) is an attention-based model architecture that has
  demonstrated superior performance on many computer vision tasks. However, its security
  properties, in particular, the robustness against adversarial attacks, are yet to
  be thoroughly studied. Recent works have shown that ViT is vulnerable to attention-based
  adversarial patch attacks, which cover 1-3% area of the input image using adversarial
  patches and degrades the model accuracy to 0%. This work provides a generic study
  targeting the attention-based patch attack. First, we experimentally observe that
  adversarial patches only activate in a few layers and become lazy during attention
  updating. According to experiments, we study the theory of how a small adversarial
  patch perturbates the whole model. Based on understanding adversarial patch attacks,
  we propose a simple but efficient defense that correctly detects more than 95% of
  adversarial patches.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu23n
month: 0
tex_title: Understanding and Defending Patched-based Adversarial Attacks for Vision
  Transformer
firstpage: 21631
lastpage: 21657
page: 21631-21657
order: 21631
cycles: false
bibtex_author: Liu, Liang and Guo, Yanan and Zhang, Youtao and Yang, Jun
author:
- given: Liang
  family: Liu
- given: Yanan
  family: Guo
- given: Youtao
  family: Zhang
- given: Jun
  family: Yang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/liu23n/liu23n.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
