---
title: Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient
  Abstract Geometric Reasoning
openreview: tE3BMOyUl5
abstract: The Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most
  recent language-complete instantiation (LARC) has been postulated as an important
  step towards general AI. Yet, even state-of-the-art machine learning models struggle
  to achieve meaningful performance on these problems, falling behind non-learning
  based approaches. We argue that solving these tasks requires extreme generalization
  that can only be achieved by proper accounting for core knowledge priors. As a step
  towards this goal, we focus on geometry priors and introduce LatFormer, a model
  that incorporates lattice symmetry priors in attention masks. We show that, for
  any transformation of the hypercubic lattice, there exists a binary attention mask
  that implements that group action. Hence, our study motivates a modification to
  the standard attention mechanism, where attention weights are scaled using soft
  masks generated by a convolutional network. Experiments on synthetic geometric reasoning
  show that LatFormer requires 2 orders of magnitude fewer data than standard attention
  and transformers. Moreover, our results on ARC and LARC tasks that incorporate geometric
  priors provide preliminary evidence that these complex datasets do not lie out of
  the reach of deep learning models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: atzeni23a
month: 0
tex_title: Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient
  Abstract Geometric Reasoning
firstpage: 1200
lastpage: 1217
page: 1200-1217
order: 1200
cycles: false
bibtex_author: Atzeni, Mattia and Sachan, Mrinmaya and Loukas, Andreas
author:
- given: Mattia
  family: Atzeni
- given: Mrinmaya
  family: Sachan
- given: Andreas
  family: Loukas
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/atzeni23a/atzeni23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
