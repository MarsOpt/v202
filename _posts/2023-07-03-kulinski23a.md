---
title: Towards Explaining Distribution Shifts
openreview: Tig5ELxc0M
abstract: A distribution shift can have fundamental consequences such as signaling
  a change in the operating environment or significantly reducing the accuracy of
  downstream models. Thus, understanding distribution shifts is critical for examining
  and hopefully mitigating the effect of such a shift. Most prior work has focused
  on merely detecting if a shift has occurred and assumes any detected shift can be
  understood and handled appropriately by a human operator. We hope to aid in these
  manual mitigation tasks by explaining the distribution shift using interpretable
  transportation maps from the original distribution to the shifted one. We derive
  our interpretable mappings from a relaxation of the optimal transport problem, where
  the candidate mappings are restricted to a set of interpretable mappings. We then
  use a wide array of quintessential examples of distribution shift in real-world
  tabular, text, and image cases to showcase how our explanatory mappings provide
  a better balance between detail and interpretability than baseline explanations
  by both visual inspection and our PercentExplained metric.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kulinski23a
month: 0
tex_title: Towards Explaining Distribution Shifts
firstpage: 17931
lastpage: 17952
page: 17931-17952
order: 17931
cycles: false
bibtex_author: Kulinski, Sean and Inouye, David I.
author:
- given: Sean
  family: Kulinski
- given: David I.
  family: Inouye
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kulinski23a/kulinski23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
