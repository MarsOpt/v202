---
title: 'Extending Kernel PCA through Dualization: Sparsity, Robustness and Fast Algorithms'
openreview: 9xqrSeujqc
abstract: The goal of this paper is to revisit Kernel Principal Component Analysis
  (KPCA) through dualization of a difference of convex functions. This allows to naturally
  extend KPCA to multiple objective functions and leads to efficient gradient-based
  algorithms avoiding the expensive SVD of the Gram matrix. Particularly, we consider
  objective functions that can be written as Moreau envelopes, demonstrating how to
  promote robustness and sparsity within the same framework. The proposed method is
  evaluated on synthetic and realworld benchmarks, showing significant speedup in
  KPCA training time as well as highlighting the benefits in terms of robustness and
  sparsity.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tonin23a
month: 0
tex_title: 'Extending Kernel {PCA} through Dualization: Sparsity, Robustness and Fast
  Algorithms'
firstpage: 34379
lastpage: 34393
page: 34379-34393
order: 34379
cycles: false
bibtex_author: Tonin, Francesco and Lambert, Alex and Patrinos, Panagiotis and Suykens,
  Johan
author:
- given: Francesco
  family: Tonin
- given: Alex
  family: Lambert
- given: Panagiotis
  family: Patrinos
- given: Johan
  family: Suykens
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/tonin23a/tonin23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
