---
title: Tighter Information-Theoretic Generalization Bounds from Supersamples
openreview: y6gg68aGiq
abstract: In this work, we present a variety of novel information-theoretic generalization
  bounds for learning algorithms, from the supersample setting of Steinke & Zakynthinou
  (2020)—the setting of the “conditional mutual information” framework. Our development
  exploits projecting the loss pair (obtained from a training instance and a testing
  instance) down to a single number and correlating loss values with a Rademacher
  sequence (and its shifted variants). The presented bounds include square-root bounds,
  fast-rate bounds, including those based on variance and sharpness, and bounds for
  interpolating algorithms etc. We show theoretically or empirically that these bounds
  are tighter than all information-theoretic bounds known to date on the same supersample
  setting.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23w
month: 0
tex_title: Tighter Information-Theoretic Generalization Bounds from Supersamples
firstpage: 36111
lastpage: 36137
page: 36111-36137
order: 36111
cycles: false
bibtex_author: Wang, Ziqiao and Mao, Yongyi
author:
- given: Ziqiao
  family: Wang
- given: Yongyi
  family: Mao
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wang23w/wang23w.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
