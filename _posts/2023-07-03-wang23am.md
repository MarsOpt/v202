---
title: Model-Free Robust Average-Reward Reinforcement Learning
openreview: ikDXPA0BA2
abstract: Robust Markov decision processes (MDPs) address the challenge of model uncertainty
  by optimizing the worst-case performance over an uncertainty set of MDPs. In this
  paper, we focus on the robust average-reward MDPs under the model-free setting.
  We first theoretically characterize the structure of solutions to the robust average-reward
  Bellman equation, which is essential for our later convergence analysis. We then
  design two model-free algorithms, robust relative value iteration (RVI) TD and robust
  RVI Q-learning, and theoretically prove their convergence to the optimal solution.
  We provide several widely used uncertainty sets as examples, including those defined
  by the contamination model, total variation, Chi-squared divergence, Kullback-Leibler
  (KL) divergence, and Wasserstein distance.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23am
month: 0
tex_title: Model-Free Robust Average-Reward Reinforcement Learning
firstpage: 36431
lastpage: 36469
page: 36431-36469
order: 36431
cycles: false
bibtex_author: Wang, Yue and Velasquez, Alvaro and Atia, George K. and Prater-Bennette,
  Ashley and Zou, Shaofeng
author:
- given: Yue
  family: Wang
- given: Alvaro
  family: Velasquez
- given: George K.
  family: Atia
- given: Ashley
  family: Prater-Bennette
- given: Shaofeng
  family: Zou
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wang23am/wang23am.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
