---
title: On the Role of Attention in Prompt-tuning
openreview: qorOnDor89
abstract: 'Prompt-tuning is an emerging strategy to adapt large language models (LLM)
  to downstream tasks by learning a (soft-)prompt parameter from data. Despite its
  success in LLMs, there is limited theoretical understanding of the power of prompt-tuning
  and the role of the attention mechanism in prompting. In this work, we explore prompt-tuning
  for one-layer attention architectures and study contextual mixture-models where
  each input token belongs to a context-relevant or -irrelevant set. We isolate the
  role of prompt-tuning through a self-contained prompt-attention model. Our contributions
  are as follows: (1) We show that softmax-prompt-attention is provably more expressive
  than softmax-self-attention and linear-prompt-attention under our contextual data
  model. (2) We analyze the initial trajectory of gradient descent and show that it
  learns the prompt and prediction head with near-optimal sample complexity and demonstrate
  how the prompt can provably attend to sparse context-relevant tokens. (3) Assuming
  a known prompt but an unknown prediction head, we characterize the exact finite
  sample performance of prompt-attention which reveals the fundamental performance
  limits and the precise benefit of the context information. We also provide experiments
  that verify our theoretical insights on real datasets and demonstrate how prompt-tuning
  enables the model to attend to context-relevant information.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: oymak23a
month: 0
tex_title: On the Role of Attention in Prompt-tuning
firstpage: 26724
lastpage: 26768
page: 26724-26768
order: 26724
cycles: false
bibtex_author: Oymak, Samet and Rawat, Ankit Singh and Soltanolkotabi, Mahdi and Thrampoulidis,
  Christos
author:
- given: Samet
  family: Oymak
- given: Ankit Singh
  family: Rawat
- given: Mahdi
  family: Soltanolkotabi
- given: Christos
  family: Thrampoulidis
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/oymak23a/oymak23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
