---
title: Nearly-tight Bounds for Deep Kernel Learning
openreview: F5bcSnILOZ
abstract: The generalization analysis of deep kernel learning (DKL) is a crucial and
  open problem of kernel methods for deep learning. The implicit nonlinear mapping
  in DKL makes existing methods of capacity-based generalization analysis for deep
  learning invalid. In an attempt to overcome this challenge and make up for the gap
  in the generalization theory of DKL, we develop an analysis method based on the
  composite relationship of function classes and derive capacity-based bounds with
  mild dependence on the depth, which generalizes learning theory bounds to deep kernels
  and serves as theoretical guarantees for the generalization of DKL. In this paper,
  we prove novel and nearly-tight generalization bounds based on the uniform covering
  number and the Rademacher chaos complexity for deep (multiple) kernel machines.
  In addition, for some common classes, we estimate their uniform covering numbers
  and Rademacher chaos complexities by bounding their pseudo-dimensions and kernel
  pseudo-dimensions, respectively. The mild bounds without strong assumptions partially
  explain the good generalization ability of deep learning combined with kernel methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang23ax
month: 0
tex_title: Nearly-tight Bounds for Deep Kernel Learning
firstpage: 41861
lastpage: 41879
page: 41861-41879
order: 41861
cycles: false
bibtex_author: Zhang, Yifan and Zhang, Min-Ling
author:
- given: Yifan
  family: Zhang
- given: Min-Ling
  family: Zhang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhang23ax/zhang23ax.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
