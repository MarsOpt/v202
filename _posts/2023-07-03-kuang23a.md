---
title: Variance Control for Distributional Reinforcement Learning
openreview: FLTAMLAgEq
abstract: Although distributional reinforcement learning (DRL) has been widely examined
  in the past few years, very few studies investigate the validity of the obtained
  Q-function estimator in the distributional setting. To fully understand how the
  approximation errors of the Q-function affect the whole training process, we do
  some error analysis and theoretically show how to reduce both the bias and the variance
  of the error terms. With this new understanding, we construct a new estimator Quantiled
  Expansion Mean (QEM) and introduce a new DRL algorithm (QEMRL) from the statistical
  perspective. We extensively evaluate our QEMRL algorithm on a variety of Atari and
  Mujoco benchmark tasks and demonstrate that QEMRL achieves significant improvement
  over baseline algorithms in terms of sample efficiency and convergence performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kuang23a
month: 0
tex_title: Variance Control for Distributional Reinforcement Learning
firstpage: 17874
lastpage: 17895
page: 17874-17895
order: 17874
cycles: false
bibtex_author: Kuang, Qi and Zhu, Zhoufan and Zhang, Liwen and Zhou, Fan
author:
- given: Qi
  family: Kuang
- given: Zhoufan
  family: Zhu
- given: Liwen
  family: Zhang
- given: Fan
  family: Zhou
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kuang23a/kuang23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
