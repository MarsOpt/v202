---
title: Leveraging Proxy of Training Data for Test-Time Adaptation
openreview: VVGNInOAm9
abstract: We consider test-time adaptation (TTA), the task of adapting a trained model
  to an arbitrary test domain using unlabeled input data on-the-fly during testing.
  A common practice of TTA is to disregard data used in training due to large memory
  demand and privacy leakage. However, the training data are the only source of supervision.
  This motivates us to investigate a proper way of using them while minimizing the
  side effects. To this end, we propose two lightweight yet informative proxies of
  the training data and a TTA method fully exploiting them. One of the proxies is
  composed of a small number of images synthesized (hence, less privacy-sensitive)
  by data condensation which minimizes their domain-specificity to capture a general
  underlying structure over a wide spectrum of domains. Then, in TTA, they are translated
  into labeled test data by stylizing them to match styles of unlabeled test samples.
  This enables virtually supervised test-time training. The other proxy is inter-class
  relations of training data, which are transferred to target model during TTA. On
  four public benchmarks, our method outperforms the state-of-the-art ones at remarkably
  less computation and memory.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kang23a
month: 0
tex_title: Leveraging Proxy of Training Data for Test-Time Adaptation
firstpage: 15737
lastpage: 15752
page: 15737-15752
order: 15737
cycles: false
bibtex_author: Kang, Juwon and Kim, Nayeong and Kwon, Donghyeon and Ok, Jungseul and
  Kwak, Suha
author:
- given: Juwon
  family: Kang
- given: Nayeong
  family: Kim
- given: Donghyeon
  family: Kwon
- given: Jungseul
  family: Ok
- given: Suha
  family: Kwak
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kang23a/kang23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
