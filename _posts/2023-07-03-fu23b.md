---
title: 'MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without Camera
  Poses'
openreview: OTZyQCwgNL
abstract: 'We propose a generalizable neural radiance fields - MonoNeRF, that can
  be trained on large-scale monocular videos of moving in static scenes without any
  ground-truth annotations of depth and camera poses. MonoNeRF follows an Autoencoder-based
  architecture, where the encoder estimates the monocular depth and the camera pose,
  and the decoder constructs a Multiplane NeRF representation based on the depth encoder
  feature, and renders the input frames with the estimated camera. The learning is
  supervised by the reconstruction error. Once the model is learned, it can be applied
  to multiple applications including depth estimation, camera pose estimation, and
  single-image novel view synthesis. More qualitative results are available at: https://oasisyang.github.io/mononerf.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fu23b
month: 0
tex_title: "{M}ono{N}e{RF}: Learning Generalizable {N}e{RF}s from Monocular Videos
  without Camera Poses"
firstpage: 10392
lastpage: 10404
page: 10392-10404
order: 10392
cycles: false
bibtex_author: Fu, Yang and Misra, Ishan and Wang, Xiaolong
author:
- given: Yang
  family: Fu
- given: Ishan
  family: Misra
- given: Xiaolong
  family: Wang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/fu23b/fu23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
