---
title: Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning
openreview: LihAbUvtLG
abstract: Spurious correlations that degrade model generalization or lead the model
  to be right for the wrong reasons are one of the main robustness concerns for real-world
  deployments. However, mitigating these correlations during pre-training for large-scale
  models can be costly and impractical, particularly for those without access to high-performance
  computing resources. This paper proposes a novel approach to address spurious correlations
  during fine-tuning for a given domain of interest. With a focus on multi-modal models
  (e.g., CLIP), the proposed method leverages different modalities in these models
  to detect and explicitly set apart spurious attributes from the affected class,
  achieved through a multi-modal contrastive loss function that expresses spurious
  relationships through language. Our experimental results and in-depth visualizations
  on CLIP show that such an intervention can effectively i) improve the model’s accuracy
  when spurious attributes are not present, and ii) directs the model’s activation
  maps towards the actual class rather than the spurious attribute when present. In
  particular, on the Waterbirds dataset, our algorithm achieved a worst-group accuracy
  23% higher than ERM on CLIP with a ResNet-50 backbone, and 32% higher on CLIP with
  a ViT backbone, while maintaining the same average accuracy as ERM.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang23j
month: 0
tex_title: Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning
firstpage: 39365
lastpage: 39379
page: 39365-39379
order: 39365
cycles: false
bibtex_author: Yang, Yu and Nushi, Besmira and Palangi, Hamid and Mirzasoleiman, Baharan
author:
- given: Yu
  family: Yang
- given: Besmira
  family: Nushi
- given: Hamid
  family: Palangi
- given: Baharan
  family: Mirzasoleiman
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/yang23j/yang23j.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
