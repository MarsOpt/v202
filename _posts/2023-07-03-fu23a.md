---
title: Simple Hardware-Efficient Long Convolutions for Sequence Modeling
openreview: HwbKflLo6j
abstract: 'State space models (SSMs) have high performance on long sequence modeling
  but require sophisticated initialization techniques and specialized implementations
  for high quality and runtime performance. We study whether a simple alternative
  can match SSMs in performance and efficiency: directly learning long convolutions
  over the sequence. We find that a key requirement to achieving high performance
  is keeping the convolution kernels smooth. We find that simple interventions-such
  as squashing the kernel weights-result in smooth kernels and recover SSM performance
  on a range of tasks including the long range arena, image classification, language
  modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware
  algorithm to improve the runtime performance of long convolutions. FlashButterfly
  appeals to classic Butterfly decompositions of the convolution to reduce GPU memory
  IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\times$,
  and allows us to train on Path256, a challenging task with sequence length 64K,
  where we set state-of-the-art by 29.1 points while training 7.2$\times$ faster than
  prior work. Lastly, we introduce an extension to FlashButterfly that learns the
  coefficients of the Butterfly decomposition, increasing expressivity without increasing
  runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2
  PPL with 30% fewer parameters.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fu23a
month: 0
tex_title: Simple Hardware-Efficient Long Convolutions for Sequence Modeling
firstpage: 10373
lastpage: 10391
page: 10373-10391
order: 10373
cycles: false
bibtex_author: Fu, Daniel Y and Epstein, Elliot L and Nguyen, Eric and Thomas, Armin
  W and Zhang, Michael and Dao, Tri and Rudra, Atri and Re, Christopher
author:
- given: Daniel Y
  family: Fu
- given: Elliot L
  family: Epstein
- given: Eric
  family: Nguyen
- given: Armin W
  family: Thomas
- given: Michael
  family: Zhang
- given: Tri
  family: Dao
- given: Atri
  family: Rudra
- given: Christopher
  family: Re
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/fu23a/fu23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
