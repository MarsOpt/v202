---
title: Grounding Language Models to Images for Multimodal Inputs and Outputs
openreview: ElaajXDEKR
abstract: We propose an efficient method to ground pretrained text-only language models
  to the visual domain, enabling them to process arbitrarily interleaved image-and-text
  data, and generate text interleaved with retrieved images. Our method leverages
  the abilities of language models learnt from large scale text-only pretraining,
  such as in-context learning and free-form text generation. We keep the language
  model frozen, and finetune input and output linear layers to enable cross-modality
  interactions. This allows our model to process arbitrarily interleaved image-and-text
  inputs, and generate free-form text interleaved with retrieved images. We achieve
  strong zero-shot performance on grounded tasks such as contextual image retrieval
  and multimodal dialogue, and showcase compelling interactive abilities. Our approach
  works with any off-the-shelf language model and paves the way towards an effective,
  general solution for leveraging pretrained language models in visually grounded
  settings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: koh23a
month: 0
tex_title: Grounding Language Models to Images for Multimodal Inputs and Outputs
firstpage: 17283
lastpage: 17300
page: 17283-17300
order: 17283
cycles: false
bibtex_author: Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel
author:
- given: Jing Yu
  family: Koh
- given: Ruslan
  family: Salakhutdinov
- given: Daniel
  family: Fried
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/koh23a/koh23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
