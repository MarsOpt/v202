---
title: Gradient Descent Finds the Global Optima of Two-Layer Physics-Informed Neural
  Networks
openreview: DRMh8mVEav
abstract: The main aim of this paper is to conduct the convergence analysis of the
  gradient descent for two-layer physics-informed neural networks (PINNs). Here, the
  loss function involves derivatives of neural network outputs with respect to its
  inputs, so the interaction between the trainable parameters is more complicated
  compared with simple regression and classification tasks. We first develop the positive
  definiteness of Gram matrices and prove that the gradient flow finds the global
  optima of the empirical loss under over-parameterization. Then, we demonstrate that
  the standard gradient descent converges to the global optima of the loss with proper
  choices of learning rates. The framework of our analysis works for various categories
  of PDEs (e.g., linear second-order PDEs) and common types of network initialization
  (LecunUniform etc.). Our theoretical results do not need a very strict hypothesis
  for training samples and have a looser requirement on the network width compared
  with some previous works.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gao23b
month: 0
tex_title: Gradient Descent Finds the Global Optima of Two-Layer Physics-Informed
  Neural Networks
firstpage: 10676
lastpage: 10707
page: 10676-10707
order: 10676
cycles: false
bibtex_author: Gao, Yihang and Gu, Yiqi and Ng, Michael
author:
- given: Yihang
  family: Gao
- given: Yiqi
  family: Gu
- given: Michael
  family: Ng
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/gao23b/gao23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
