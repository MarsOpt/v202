---
title: Scaling of Class-wise Training Losses for Post-hoc Calibration
openreview: mzd1NfZxsX
abstract: The class-wise training losses often diverge as a result of the various
  levels of intra-class and inter-class appearance variation, and we find that the
  diverging class-wise training losses cause the uncalibrated prediction with its
  reliability. To resolve the issue, we propose a new calibration method to synchronize
  the class-wise training losses. We design a new training loss to alleviate the variance
  of class-wise training losses by using multiple class-wise scaling factors. Since
  our framework can compensate the training losses of overfitted classes with those
  of under-fitted classes, the integrated training loss is preserved, preventing the
  performance drop even after the model calibration. Furthermore, our method can be
  easily employed in the post-hoc calibration methods, allowing us to use the pre-trained
  model as an initial model and reduce the additional computation for model calibration.
  We validate the proposed framework by employing it in the various post-hoc calibration
  methods, which generally improves calibration performance while preserving accuracy,
  and discover through the investigation that our approach performs well with unbalanced
  datasets and untuned hyperparameters.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jung23a
month: 0
tex_title: Scaling of Class-wise Training Losses for Post-hoc Calibration
firstpage: 15421
lastpage: 15434
page: 15421-15434
order: 15421
cycles: false
bibtex_author: Jung, Seungjin and Seo, Seungmo and Jeong, Yonghyun and Choi, Jongwon
author:
- given: Seungjin
  family: Jung
- given: Seungmo
  family: Seo
- given: Yonghyun
  family: Jeong
- given: Jongwon
  family: Choi
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/jung23a/jung23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
