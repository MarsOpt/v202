---
title: Explaining the effects of non-convergent MCMC in the training of Energy-Based
  Models
openreview: DF9aUqGzsV
abstract: In this paper, we quantify the impact of using non-convergent Markov chains
  to train Energy-Based models (EBMs). In particular, we show analytically that EBMs
  trained with non-persistent short runs to estimate the gradient can perfectly reproduce
  a set of empirical statistics of the data, not at the level of the equilibrium measure,
  but through a precise dynamical process. Our results provide a first-principles
  explanation for the observations of recent works proposing the strategy of using
  short runs starting from random initial conditions as an efficient way to generate
  high-quality samples in EBMs, and lay the groundwork for using EBMs as diffusion
  models. After explaining this effect in generic EBMs, we analyze two solvable models
  in which the effect of the non-convergent sampling in the trained parameters can
  be described in detail. Finally, we test these predictions numerically on a ConvNet
  EBM and a Boltzmann machine.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: agoritsas23a
month: 0
tex_title: Explaining the effects of non-convergent {MCMC} in the training of Energy-Based
  Models
firstpage: 322
lastpage: 336
page: 322-336
order: 322
cycles: false
bibtex_author: Agoritsas, Elisabeth and Catania, Giovanni and Decelle, Aur\'{e}lien
  and Seoane, Beatriz
author:
- given: Elisabeth
  family: Agoritsas
- given: Giovanni
  family: Catania
- given: Aur√©lien
  family: Decelle
- given: Beatriz
  family: Seoane
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/agoritsas23a/agoritsas23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
