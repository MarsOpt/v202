---
title: 'Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates'
openreview: 4fBTKlUfTW
abstract: In recent years, particle-based variational inference (ParVI) methods such
  as Stein variational gradient descent (SVGD) have grown in popularity as scalable
  methods for Bayesian inference. Unfortunately, the properties of such methods invariably
  depend on hyperparameters such as the learning rate, which must be carefully tuned
  by the practitioner in order to ensure convergence to the target measure at a suitable
  rate. In this paper, we introduce a suite of new particle-based methods for scalable
  Bayesian inference based on coin betting, which are entirely learning-rate free.
  We illustrate the performance of our approach on a range of numerical examples,
  including several high-dimensional models and datasets, demonstrating comparable
  performance to other ParVI algorithms with no need to tune a learning rate.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sharrock23a
month: 0
tex_title: 'Coin Sampling: Gradient-Based {B}ayesian Inference without Learning Rates'
firstpage: 30850
lastpage: 30882
page: 30850-30882
order: 30850
cycles: false
bibtex_author: Sharrock, Louis and Nemeth, Christopher
author:
- given: Louis
  family: Sharrock
- given: Christopher
  family: Nemeth
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/sharrock23a/sharrock23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
