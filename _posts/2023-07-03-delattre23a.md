---
title: Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration
openreview: Z0ATKIJR8G
abstract: Since the control of the Lipschitz constant has a great impact on the training
  stability, generalization, and robustness of neural networks, the estimation of
  this value is nowadays a real scientific challenge. In this paper we introduce a
  precise, fast, and differentiable upper bound for the spectral norm of convolutional
  layers using circulant matrix theory and a new alternative to the Power iteration.
  Called the Gram iteration, our approach exhibits a superlinear convergence. First,
  we show through a comprehensive set of experiments that our approach outperforms
  other state-of-the-art methods in terms of precision, computational cost, and scalability.
  Then, it proves highly effective for the Lipschitz regularization of convolutional
  neural networks, with competitive results against concurrent approaches.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: delattre23a
month: 0
tex_title: Efficient Bound of {L}ipschitz Constant for Convolutional Layers by {G}ram
  Iteration
firstpage: 7513
lastpage: 7532
page: 7513-7532
order: 7513
cycles: false
bibtex_author: Delattre, Blaise and Barth\'{e}lemy, Quentin and Araujo, Alexandre
  and Allauzen, Alexandre
author:
- given: Blaise
  family: Delattre
- given: Quentin
  family: Barth√©lemy
- given: Alexandre
  family: Araujo
- given: Alexandre
  family: Allauzen
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/delattre23a/delattre23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
