---
title: Gradient-Free Structured Pruning with Unlabeled Data
openreview: Ga6nQOAb7A
abstract: Large Language Models (LLMs) have achieved great success in solving difficult
  tasks across many domains, but such success comes with a high computation cost,
  and inference latency. As developers and third parties customize these models, the
  need to provide efficient inference has increased. Many efforts have attempted to
  reduce inference cost through model compression techniques such as pruning and distillation.
  However, these techniques either require labeled data, or are time-consuming as
  they require the compressed model to be retrained to regain accuracy. In this paper,
  we propose a gradient-free structured pruning framework that uses only unlabeled
  data. An evaluation on the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT
  illustrates the effectiveness of the proposed approach. By only using the weights
  of the pre-trained model and unlabeled data, in a matter of a few minutes on a single
  GPU, up to 40% of the original FLOP count can be reduced with less than a $4%$ accuracy
  loss across all tasks considered.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nova23a
month: 0
tex_title: Gradient-Free Structured Pruning with Unlabeled Data
firstpage: 26326
lastpage: 26341
page: 26326-26341
order: 26326
cycles: false
bibtex_author: Nova, Azade and Dai, Hanjun and Schuurmans, Dale
author:
- given: Azade
  family: Nova
- given: Hanjun
  family: Dai
- given: Dale
  family: Schuurmans
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/nova23a/nova23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
