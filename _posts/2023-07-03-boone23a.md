---
title: The Regret of Exploration and the Control of Bad Episodes in Reinforcement
  Learning
openreview: vxyaYltes2
abstract: The first contribution of this paper is the introduction of a new performance
  measure of a RL algorithm that is more discriminating than the regret, that we call
  the <em>regret of exploration</em> that measures the asymptotic cost of exploration.
  The second contribution is a new <em>performance test</em> (PT) to end episodes
  in RL optimistic algorithms. This test is based on the performance of the current
  policy with respect to the best policy over the current confidence set. This is
  in contrast with all existing RL algorithms whose episode lengths are only based
  on the number of visits to the states. This modification does not harm the regret
  and brings an additional property. We show that while all current episodic RL algorithms
  have a linear regret of exploration, our method has a $O(\log{T})$ regret of exploration
  for non-degenerate deterministic MDPs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: boone23a
month: 0
tex_title: The Regret of Exploration and the Control of Bad Episodes in Reinforcement
  Learning
firstpage: 2824
lastpage: 2856
page: 2824-2856
order: 2824
cycles: false
bibtex_author: Boone, Victor and Gaujal, Bruno
author:
- given: Victor
  family: Boone
- given: Bruno
  family: Gaujal
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/boone23a/boone23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
