---
title: 'FlexRound: Learnable Rounding based on Element-wise Division for Post-Training
  Quantization'
openreview: EPnzNJTYsb
abstract: Post-training quantization (PTQ) has been gaining popularity for the deployment
  of deep neural networks on resource-limited devices since unlike quantization-aware
  training, neither a full training dataset nor end-to-end training is required at
  all. As PTQ schemes based on reconstructing each layer or block output turn out
  to be effective to enhance quantized model performance, recent works have developed
  algorithms to devise and learn a new weight-rounding scheme so as to better reconstruct
  each layer or block output. In this work, we propose a simple yet effective new
  weight-rounding mechanism for PTQ, coined FlexRound, based on element-wise division
  instead of typical element-wise addition such that FlexRound enables jointly learning
  a common quantization grid size as well as a different scale for each pre-trained
  weight. Thanks to the reciprocal rule of derivatives induced by element-wise division,
  FlexRound is inherently able to exploit pre-trained weights when updating their
  corresponding scales, and thus, flexibly quantize pre-trained weights depending
  on their magnitudes. We empirically validate the efficacy of FlexRound on a wide
  range of models and tasks. To the best of our knowledge, our work is the first to
  carry out comprehensive experiments on not only image classification and natural
  language understanding but also natural language generation, assuming a per-tensor
  uniform PTQ setting. Moreover, we demonstrate, for the first time, that large language
  models can be efficiently quantized, with only a negligible impact on performance
  compared to half-precision baselines, achieved by reconstructing the output in a
  block-by-block manner.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lee23h
month: 0
tex_title: "{F}lex{R}ound: Learnable Rounding based on Element-wise Division for Post-Training
  Quantization"
firstpage: 18913
lastpage: 18939
page: 18913-18939
order: 18913
cycles: false
bibtex_author: Lee, Jung Hyun and Kim, Jeonghoon and Kwon, Se Jung and Lee, Dongsoo
author:
- given: Jung Hyun
  family: Lee
- given: Jeonghoon
  family: Kim
- given: Se Jung
  family: Kwon
- given: Dongsoo
  family: Lee
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/lee23h/lee23h.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
