---
title: 'SparseProp: Efficient Sparse Backpropagation for Faster Training of Neural
  Networks at the Edge'
openreview: JSTp7NiuYi
abstract: We provide an efficient implementation of the backpropagation algorithm,
  specialized to the case where the weights of the neural network being trained are
  <em>sparse</em>. Our algorithm is general, as it applies to arbitrary (unstructured)
  sparsity and common layer types (e.g., convolutional or linear). We provide a fast
  vectorized implementation on commodity CPUs, and show that it can yield speedups
  in end-to-end runtime experiments, both in transfer learning using already-sparsified
  networks, and in training sparse networks from scratch. Thus, our results provide
  the first support for sparse training on commodity hardware.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nikdan23a
month: 0
tex_title: "{S}parse{P}rop: Efficient Sparse Backpropagation for Faster Training of
  Neural Networks at the Edge"
firstpage: 26215
lastpage: 26227
page: 26215-26227
order: 26215
cycles: false
bibtex_author: Nikdan, Mahdi and Pegolotti, Tommaso and Iofinova, Eugenia and Kurtic,
  Eldar and Alistarh, Dan
author:
- given: Mahdi
  family: Nikdan
- given: Tommaso
  family: Pegolotti
- given: Eugenia
  family: Iofinova
- given: Eldar
  family: Kurtic
- given: Dan
  family: Alistarh
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/nikdan23a/nikdan23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
