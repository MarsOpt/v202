---
title: 'Master-ASR: Achieving Multilingual Scalability and Low-Resource Adaptation
  in ASR with Modular Learning'
openreview: nrSM4XmF5k
abstract: 'Despite the impressive performance recently achieved by automatic speech
  recognition (ASR), we observe two primary challenges that hinder its broader applications:
  (1) The difficulty of introducing scalability into the model to support more languages
  with limited training, inference, and storage overhead; (2) The low-resource adaptation
  ability that enables effective low-resource adaptation while avoiding over fitting
  and catastrophic forgetting issues. Inspired by recent findings, we hypothesize
  that we can address the above challenges with modules widely shared across languages.
  To this end, we propose an ASR framework, dubbed Master-ASR, that, for the first
  time, simultaneously achieves strong multilingual scalability and low-resource adaptation
  ability thanks to its modularize-then-assemble strategy. Specifically, Master-ASR
  learns a small set of generalizable sub-modules and adaptively assembles them for
  different languages to reduce the multilingual overhead and enable effective knowledge
  transfer for low-resource adaptation. Extensive experiments and visualizations demonstrate
  that Master-ASR can effectively discover language similarity and improve multilingual
  and low-resource ASR performance over state-of-the-art (SOTA) methods, e.g., under
  multilingual-ASR, our framework achieves a 0.13âˆ¼2.41 lower character error rate
  (CER) with 30% smaller inference overhead over SOTA solutions on multilingual ASR
  and a comparable CER with nearly 100 times fewer trainable parameters over SOTA
  solutions on low-resource tuning, respectively.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yu23l
month: 0
tex_title: 'Master-{ASR}: Achieving Multilingual Scalability and Low-Resource Adaptation
  in {ASR} with Modular Learning'
firstpage: 40475
lastpage: 40487
page: 40475-40487
order: 40475
cycles: false
bibtex_author: Yu, Zhongzhi and Zhang, Yang and Qian, Kaizhi and Wan, Cheng and Fu,
  Yonggan and Zhang, Yongan and Lin, Yingyan Celine
author:
- given: Zhongzhi
  family: Yu
- given: Yang
  family: Zhang
- given: Kaizhi
  family: Qian
- given: Cheng
  family: Wan
- given: Yonggan
  family: Fu
- given: Yongan
  family: Zhang
- given: Yingyan Celine
  family: Lin
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/yu23l/yu23l.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
