---
title: 'SAM operates far from home: eigenvalue regularization as a dynamical phenomenon'
openreview: 5YAP9Ntq3L
abstract: The Sharpness Aware Minimization (SAM) optimization algorithm has been shown
  to control large eigenvalues of the loss Hessian and provide generalization benefits
  in a variety of settings. The original motivation for SAM was a modified loss function
  which penalized sharp minima; subsequent analyses have also focused on the behavior
  near minima. However, our work reveals that SAM provides a strong regularization
  of the eigenvalues throughout the learning trajectory. We show that in a simplified
  setting, SAM dynamically induces a stabilization related to the edge of stability
  (EOS) phenomenon observed in large learning rate gradient descent. Our theory predicts
  the largest eigenvalue as a function of the learning rate and SAM radius parameters.
  Finally, we show that practical models can also exhibit this EOS stabilization,
  and that understanding SAM must account for these dynamics far away from any minima.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: agarwala23a
month: 0
tex_title: "{SAM} operates far from home: eigenvalue regularization as a dynamical
  phenomenon"
firstpage: 152
lastpage: 168
page: 152-168
order: 152
cycles: false
bibtex_author: Agarwala, Atish and Dauphin, Yann
author:
- given: Atish
  family: Agarwala
- given: Yann
  family: Dauphin
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/agarwala23a/agarwala23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
