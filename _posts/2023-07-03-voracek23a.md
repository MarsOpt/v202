---
title: Improving l1-Certified Robustness via Randomized Smoothing by Leveraging Box
  Constraints
openreview: vPLIRidmYO
abstract: Randomized smoothing is a popular method to certify robustness of image
  classifiers to adversarial input perturbations. It is the only certification technique
  which scales directly to datasets of higher dimension such as ImageNet. However,
  current techniques are not able to utilize the fact that any adversarial example
  has to lie in the image space, that is $[0,1]^d$; otherwise, one can trivially detect
  it. To address this suboptimality, we derive new certification formulae which lead
  to significant improvements in the certified $\ell_1$-robustness without the need
  of adapting the classifiers or change of smoothing distributions. The code is released
  at https://github.com/vvoracek/L1-smoothing
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: voracek23a
month: 0
tex_title: Improving l1-Certified Robustness via Randomized Smoothing by Leveraging
  Box Constraints
firstpage: 35198
lastpage: 35222
page: 35198-35222
order: 35198
cycles: false
bibtex_author: Voracek, Vaclav and Hein, Matthias
author:
- given: Vaclav
  family: Voracek
- given: Matthias
  family: Hein
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/voracek23a/voracek23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
