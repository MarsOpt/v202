---
title: 'HyperTuning: Toward Adapting Large Language Models without Back-propagation'
openreview: O0iQkpQfFe
abstract: 'Fine-tuning large language models for different tasks can be costly and
  inefficient, and even methods that reduce the number of tuned parameters still require
  full gradient-based optimization. We propose HyperTuning, a novel approach to model
  adaptation that uses a hypermodel to generate task-specific parameters for a fixed
  downstream model. We demonstrate a simple setup for hypertuning with HyperT5, a
  T5-based hypermodel that produces soft prefixes or LoRA parameters for a frozen
  T5 model from few-shot examples. We train HyperT5 in two stages: first, hyperpretraining
  with a modified conditional language modeling objective that trains a hypermodel
  to generate parameters; second, multi-task fine-tuning (MTF) on a large number of
  diverse language tasks. We evaluate HyperT5 on P3, MetaICL and Super-NaturalInstructions
  datasets, and show that it can effectively generate parameters for unseen tasks.
  Moreover, we show that using hypermodel-generated parameters as initializations
  for further parameter-efficient fine-tuning improves performance. HyperTuning can
  thus be a flexible and efficient way to leverage large language models for diverse
  downstream applications.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: phang23a
month: 0
tex_title: "{H}yper{T}uning: Toward Adapting Large Language Models without Back-propagation"
firstpage: 27854
lastpage: 27875
page: 27854-27875
order: 27854
cycles: false
bibtex_author: Phang, Jason and Mao, Yi and He, Pengcheng and Chen, Weizhu
author:
- given: Jason
  family: Phang
- given: Yi
  family: Mao
- given: Pengcheng
  family: He
- given: Weizhu
  family: Chen
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/phang23a/phang23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
