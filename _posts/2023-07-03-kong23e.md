---
title: 'Masked Bayesian Neural Networks : Theoretical Guarantee and its Posterior
  Inference'
openreview: strSQ5q4ZS
abstract: Bayesian approaches for learning deep neural networks (BNN) have been received
  much attention and successfully applied to various applications. Particularly, BNNs
  have the merit of having better generalization ability as well as better uncertainty
  quantification. For the success of BNN, search an appropriate architecture of the
  neural networks is an important task, and various algorithms to find good sparse
  neural networks have been proposed. In this paper, we propose a new node-sparse
  BNN model which has good theoretical properties and is computationally feasible.
  We prove that the posterior concentration rate to the true model is near minimax
  optimal and adaptive to the smoothness of the true model. In particular the adaptiveness
  is the first of its kind for node-sparse BNNs. In addition, we develop a novel MCMC
  algorithm which makes the Bayesian inference of the node-sparse BNN model feasible
  in practice.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kong23e
month: 0
tex_title: 'Masked {B}ayesian Neural Networks : Theoretical Guarantee and its Posterior
  Inference'
firstpage: 17462
lastpage: 17491
page: 17462-17491
order: 17462
cycles: false
bibtex_author: Kong, Insung and Yang, Dongyoon and Lee, Jongjin and Ohn, Ilsang and
  Baek, Gyuseung and Kim, Yongdai
author:
- given: Insung
  family: Kong
- given: Dongyoon
  family: Yang
- given: Jongjin
  family: Lee
- given: Ilsang
  family: Ohn
- given: Gyuseung
  family: Baek
- given: Yongdai
  family: Kim
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kong23e/kong23e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
