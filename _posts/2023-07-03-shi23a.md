---
title: Large Language Models Can Be Easily Distracted by Irrelevant Context
openreview: JSZmoN03Op
abstract: Large language models have achieved impressive performance on various natural
  language processing tasks. However, so far they have been evaluated primarily on
  benchmarks where all information in the input context is relevant for solving the
  task. In this work, we investigate the <em>distractibility</em> of large language
  models, i.e., how the model prediction can be distracted by irrelevant context.
  In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC),
  an arithmetic reasoning dataset with irrelevant information in the problem description.
  We use this benchmark to measure the distractibility of different prompting techniques
  for large language models, and find that the model is easily distracted by irrelevant
  information. We also identify several approaches for mitigating this deficiency,
  such as decoding with self-consistency and adding to the prompt an instruction that
  tells the language model to ignore the irrelevant information.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shi23a
month: 0
tex_title: Large Language Models Can Be Easily Distracted by Irrelevant Context
firstpage: 31210
lastpage: 31227
page: 31210-31227
order: 31210
cycles: false
bibtex_author: Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan
  and Dohan, David and Chi, Ed H. and Sch\"{a}rli, Nathanael and Zhou, Denny
author:
- given: Freda
  family: Shi
- given: Xinyun
  family: Chen
- given: Kanishka
  family: Misra
- given: Nathan
  family: Scales
- given: David
  family: Dohan
- given: Ed H.
  family: Chi
- given: Nathanael
  family: Sch√§rli
- given: Denny
  family: Zhou
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/shi23a/shi23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
