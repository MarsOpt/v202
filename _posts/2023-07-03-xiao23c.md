---
title: 'SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language
  Models'
openreview: sHfSV8eYEp
abstract: Large language models (LLMs) show excellent performance but are compute-
  and memory-intensive. Quantization can reduce memory and accelerate inference. However,
  existing methods cannot maintain accuracy and hardware efficiency at the same time.
  We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose
  post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation
  (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize
  while activations are not, SmoothQuant smooths the activation outliers by offline
  migrating the quantization difficulty from activations to weights with a mathematically
  equivalent transformation. SmoothQuant enables an INT8 quantization of both weights
  and activations for all the matrix multiplications in LLMs, including OPT, BLOOM,
  GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56$\times$ speedup and 2$\times$
  memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables
  serving 530B LLM within a single node. Our work offers a turn-key solution that
  reduces hardware costs and democratizes LLMs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xiao23c
month: 0
tex_title: "{S}mooth{Q}uant: Accurate and Efficient Post-Training Quantization for
  Large Language Models"
firstpage: 38087
lastpage: 38099
page: 38087-38099
order: 38087
cycles: false
bibtex_author: Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth,
  Julien and Han, Song
author:
- given: Guangxuan
  family: Xiao
- given: Ji
  family: Lin
- given: Mickael
  family: Seznec
- given: Hao
  family: Wu
- given: Julien
  family: Demouth
- given: Song
  family: Han
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
