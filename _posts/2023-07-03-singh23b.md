---
title: When do Minimax-fair Learning and Empirical Risk Minimization Coincide?
openreview: bZXfHpbUFi
abstract: Minimax-fair machine learning minimizes the error for the worst-off group.
  However, empirical evidence suggests that when sophisticated models are trained
  with standard empirical risk minimization (ERM), they often have the same performance
  on the worst-off group as a minimax-trained model. Our work makes this counter-intuitive
  observation concrete. We prove that if the hypothesis class is sufficiently expressive
  and the group information is recoverable from the features, ERM and minimax-fairness
  learning formulations indeed have the same performance on the worst-off group. We
  provide additional empirical evidence of how this observation holds on a wide range
  of datasets and hypothesis classes. Since ERM is fundamentally easier than minimax
  optimization, our findings have implications on the practice of fair machine learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: singh23b
month: 0
tex_title: When do Minimax-fair Learning and Empirical Risk Minimization Coincide?
firstpage: 31969
lastpage: 31989
page: 31969-31989
order: 31969
cycles: false
bibtex_author: Singh, Harvineet and Kleindessner, Matth\"{a}us and Cevher, Volkan
  and Chunara, Rumi and Russell, Chris
author:
- given: Harvineet
  family: Singh
- given: Matth√§us
  family: Kleindessner
- given: Volkan
  family: Cevher
- given: Rumi
  family: Chunara
- given: Chris
  family: Russell
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/singh23b/singh23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
