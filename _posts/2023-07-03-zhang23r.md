---
title: 'CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling'
openreview: AXt40tAbif
abstract: Transformer has achieved remarkable success in language, image, and speech
  processing. Recently, various efficient attention architectures have been proposed
  to improve transformer’s efficiency while largely preserving its efficacy, especially
  in modeling long sequences. A widely-used benchmark to test these efficient methods’
  capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses
  on the standard bidirectional (or noncausal) self attention, and completely ignores
  cross attentions and unidirectional (or causal) attentions, which are equally important
  to downstream applications. In this paper, we propose Comprehensive Attention Benchmark
  (CAB) under a fine-grained attention taxonomy with four distinguishable attention
  patterns, namely, noncausal self, causal self, noncausal cross, and causal cross
  attentions. CAB collects seven real-world tasks from different research areas to
  evaluate efficient attentions under the four attention patterns. Among these tasks,
  CAB validates efficient attentions in eight backbone networks to show their generalization
  across neural architectures. We conduct exhaustive experiments to benchmark the
  performances of nine widely-used efficient attention architectures designed with
  different philosophies on CAB. Extensive experimental results also shed light on
  the fundamental problems of efficient attentions, such as efficiency length against
  vanilla attention, performance consistency across attention patterns, the benefit
  of attention mechanisms, and interpolation/extrapolation on long-context language
  modeling.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang23r
month: 0
tex_title: "{CAB}: Comprehensive Attention Benchmarking on Long Sequence Modeling"
firstpage: 41194
lastpage: 41218
page: 41194-41218
order: 41194
cycles: false
bibtex_author: Zhang, Jun and Jiang, Shuyang and Feng, Jiangtao and Zheng, Lin and
  Kong, Lingpeng
author:
- given: Jun
  family: Zhang
- given: Shuyang
  family: Jiang
- given: Jiangtao
  family: Feng
- given: Lin
  family: Zheng
- given: Lingpeng
  family: Kong
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhang23r/zhang23r.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
