---
title: 'MixFlows: principled variational inference via mixed flows'
openreview: HltJfwwfhX
abstract: This work presents mixed variational flows (MixFlows), a new variational
  family that consists of a mixture of repeated applications of a map to an initial
  reference distribution. First, we provide efficient algorithms for i.i.d. sampling,
  density evaluation, and unbiased ELBO estimation. We then show that MixFlows have
  MCMC-like convergence guarantees when the flow map is ergodic and measure-preserving,
  and provide bounds on the accumulation of error for practical implementations where
  the flow map is approximated. Finally, we develop an implementation of MixFlows
  based on uncorrected discretized Hamiltonian dynamics combined with deterministic
  momentum refreshment. Simulated and real data experiments show that MixFlows can
  provide more reliable posterior approximations than several black-box normalizing
  flows, as well as samples of comparable quality to those obtained from state-of-the-art
  MCMC methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu23b
month: 0
tex_title: "{M}ix{F}lows: principled variational inference via mixed flows"
firstpage: 38342
lastpage: 38376
page: 38342-38376
order: 38342
cycles: false
bibtex_author: Xu, Zuheng and Chen, Naitong and Campbell, Trevor
author:
- given: Zuheng
  family: Xu
- given: Naitong
  family: Chen
- given: Trevor
  family: Campbell
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/xu23b/xu23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
