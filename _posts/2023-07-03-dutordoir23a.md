---
title: Neural Diffusion Processes
openreview: tV7GSY5GYG
abstract: Neural network approaches for meta-learning distributions over functions
  have desirable properties such as increased flexibility and a reduced complexity
  of inference. Building on the successes of denoising diffusion models for generative
  modelling, we propose Neural Diffusion Processes (NDPs), a novel approach that learns
  to sample from a rich distribution over functions through its finite marginals.
  By introducing a custom attention block we are able to incorporate properties of
  stochastic processes, such as exchangeability, directly into the NDPâ€™s architecture.
  We empirically show that NDPs can capture functional distributions close to the
  true Bayesian posterior, demonstrating that they can successfully emulate the behaviour
  of Gaussian processes and surpass the performance of neural processes. NDPs enable
  a variety of downstream tasks, including regression, implicit hyperparameter marginalisation,
  non-Gaussian posterior prediction and global optimisation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dutordoir23a
month: 0
tex_title: Neural Diffusion Processes
firstpage: 8990
lastpage: 9012
page: 8990-9012
order: 8990
cycles: false
bibtex_author: Dutordoir, Vincent and Saul, Alan and Ghahramani, Zoubin and Simpson,
  Fergus
author:
- given: Vincent
  family: Dutordoir
- given: Alan
  family: Saul
- given: Zoubin
  family: Ghahramani
- given: Fergus
  family: Simpson
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/dutordoir23a/dutordoir23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
