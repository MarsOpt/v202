---
title: Efficient Training of Language Models using Few-Shot Learning
openreview: SpFIO5Mdso
abstract: Large deep learning models have achieved state-of-the-art performance across
  various natural language processing (NLP) tasks and demonstrated remarkable few-shot
  learning performance. However, training them is often challenging and resource-intensive.
  In this paper, we study an efficient approach to train language models using few-shot
  learners. We show that, by leveraging the fast learning nature of few-shot learners,
  one can train language models efficiently in a stagewise manner. Our main insight
  is that stacking a good few-shot learner on a good small language model provides
  a good initializer for a larger language model. Using this insight and building
  upon progressive stacking approaches, we develop novel approaches for training such
  networks in a stagewise manner. Furthermore, we also provide a theoretical framework
  and accompanying empirical studies to support our insights, thereby creating a theoretical
  foundation for progressive stacking. Finally, we provide empirical results to demonstrate
  the effectiveness of our approach in reducing the training time of few-shot learners.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: j-reddi23a
month: 0
tex_title: Efficient Training of Language Models using Few-Shot Learning
firstpage: 14553
lastpage: 14568
page: 14553-14568
order: 14553
cycles: false
bibtex_author: J. Reddi, Sashank and Miryoosefi, Sobhan and Karp, Stefani and Krishnan,
  Shankar and Kale, Satyen and Kim, Seungyeon and Kumar, Sanjiv
author:
- given: Sashank
  family: J. Reddi
- given: Sobhan
  family: Miryoosefi
- given: Stefani
  family: Karp
- given: Shankar
  family: Krishnan
- given: Satyen
  family: Kale
- given: Seungyeon
  family: Kim
- given: Sanjiv
  family: Kumar
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/j-reddi23a/j-reddi23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
