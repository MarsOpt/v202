---
title: Input Perturbation Reduces Exposure Bias in Diffusion Models
openreview: 0OcEWSMnSh
abstract: Denoising Diffusion Probabilistic Models have shown an impressive generation
  quality although their long sampling chain leads to high computational costs. In
  this paper, we observe that a long sampling chain also leads to an error accumulation
  phenomenon, which is similar to the exposure bias problem in autoregressive text
  generation. Specifically, we note that there is a discrepancy between training and
  testing, since the former is conditioned on the ground truth samples, while the
  latter is conditioned on the previously generated results. To alleviate this problem,
  we propose a very simple but effective training regularization, consisting in perturbing
  the ground truth samples to simulate the inference time prediction errors. We empirically
  show that, without affecting the recall and precision, the proposed input perturbation
  leads to a significant improvement in the sample quality while reducing both the
  training and the inference times. For instance, on CelebA 64x64, we achieve a new
  state-of-the-art FID score of 1.27, while saving 37.5% of the training time. The
  code is available at https://github.com/forever208/DDPM-IP
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ning23a
month: 0
tex_title: Input Perturbation Reduces Exposure Bias in Diffusion Models
firstpage: 26245
lastpage: 26265
page: 26245-26265
order: 26245
cycles: false
bibtex_author: Ning, Mang and Sangineto, Enver and Porrello, Angelo and Calderara,
  Simone and Cucchiara, Rita
author:
- given: Mang
  family: Ning
- given: Enver
  family: Sangineto
- given: Angelo
  family: Porrello
- given: Simone
  family: Calderara
- given: Rita
  family: Cucchiara
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/ning23a/ning23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
