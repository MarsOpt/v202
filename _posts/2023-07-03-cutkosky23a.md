---
title: Optimal Stochastic Non-smooth Non-convex Optimization through Online-to-Non-convex
  Conversion
openreview: GimajxXNc0
abstract: We present new algorithms for optimizing non-smooth, non-convex stochastic
  objectives based on a novel analysis technique. This improves the current best-known
  complexity for finding a $(\delta,\epsilon)$-stationary point from $O(\epsilon^{-4}\delta^{-1})$
  stochastic gradient queries to $O(\epsilon^{-3}\delta^{-1})$, which we also show
  to be optimal. Our primary technique is a reduction from non-smooth non-convex optimization
  to <em>online learning</em>, after which our results follow from standard regret
  bounds in online learning. For <em>deterministic and second-order smooth</em> objectives,
  applying more advanced optimistic online learning techniques enables a new complexity
  of $O(\epsilon^{-1.5}\delta^{-0.5})$. Our improved non-smooth analysis also immediately
  recovers all optimal or best-known results for finding $\epsilon$ stationary points
  of smooth or second-order smooth objectives in both stochastic and deterministic
  settings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cutkosky23a
month: 0
tex_title: Optimal Stochastic Non-smooth Non-convex Optimization through Online-to-Non-convex
  Conversion
firstpage: 6643
lastpage: 6670
page: 6643-6670
order: 6643
cycles: false
bibtex_author: Cutkosky, Ashok and Mehta, Harsh and Orabona, Francesco
author:
- given: Ashok
  family: Cutkosky
- given: Harsh
  family: Mehta
- given: Francesco
  family: Orabona
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/cutkosky23a/cutkosky23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
