---
title: Statistical Indistinguishability of Learning Algorithms
openreview: LxodbQa62n
abstract: When two different parties use the same learning rule on their own data,
  how can we test whether the distributions of the two outcomes are similar? In this
  paper, we study the similarity of outcomes of learning rules through the lens of
  the Total Variation (TV) distance of distributions. We say that a learning rule
  is TV indistinguishable if the expected TV distance between the posterior distributions
  of its outputs, executed on two training data sets drawn independently from the
  same distribution, is small. We first investigate the learnability of hypothesis
  classes using TV indistinguishable learners. Our main results are information-theoretic
  equivalences between TV indistinguishability and existing algorithmic stability
  notions such as replicability and approximate differential privacy. Then, we provide
  statistical amplification and boosting algorithms for TV indistinguishable learners.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kalavasis23a
month: 0
tex_title: Statistical Indistinguishability of Learning Algorithms
firstpage: 15586
lastpage: 15622
page: 15586-15622
order: 15586
cycles: false
bibtex_author: Kalavasis, Alkis and Karbasi, Amin and Moran, Shay and Velegkas, Grigoris
author:
- given: Alkis
  family: Kalavasis
- given: Amin
  family: Karbasi
- given: Shay
  family: Moran
- given: Grigoris
  family: Velegkas
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kalavasis23a/kalavasis23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
