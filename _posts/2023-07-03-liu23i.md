---
title: Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein
  Discrepancy
openreview: RtA4goPg7o
abstract: Kernelized Stein discrepancy (KSD) is a score-based discrepancy widely used
  in goodness-of-fit tests. It can be applied even when the target distribution has
  an unknown normalising factor, such as in Bayesian analysis. We show theoretically
  and empirically that the KSD test can suffer from low power when the target and
  the alternative distributions have the same well-separated modes but differ in mixing
  proportions. We propose to perturb the observed sample via Markov transition kernels,
  with respect to which the target distribution is invariant. This allows us to then
  employ the KSD test on the perturbed sample. We provide numerical evidence that
  with suitably chosen transition kernels the proposed approach can lead to substantially
  higher power than the KSD test.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu23i
month: 0
tex_title: Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized
  Stein Discrepancy
firstpage: 21527
lastpage: 21547
page: 21527-21547
order: 21527
cycles: false
bibtex_author: Liu, Xing and Duncan, Andrew B. and Gandy, Axel
author:
- given: Xing
  family: Liu
- given: Andrew B.
  family: Duncan
- given: Axel
  family: Gandy
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/liu23i/liu23i.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
