---
title: 'Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling'
openreview: Uj5AVsHkoX
abstract: Learning to denoise has emerged as a prominent paradigm to design state-of-the-art
  deep generative models for natural images. How to use it to model the distributions
  of both continuous real-valued data and categorical data has been well studied in
  recently proposed diffusion models. However, it is found in this paper to have limited
  ability in modeling some other types of data, such as count and non-negative continuous
  data, that are often highly sparse, skewed, heavy-tailed, and/or overdispersed.
  To this end, we propose learning to jump as a general recipe for generative modeling
  of various types of data. Using a forward count thinning process to construct learning
  objectives to train a deep neural network, it employs a reverse count thickening
  process to iteratively refine its generation through that network. We demonstrate
  when learning to jump is expected to perform comparably to learning to denoise,
  and when it is expected to perform better. For example, learning to jump is recommended
  when the training data is non-negative and exhibits strong sparsity, skewness, heavy-tailedness,
  and/or heterogeneity.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen23ap
month: 0
tex_title: 'Learning to Jump: Thinning and Thickening Latent Counts for Generative
  Modeling'
firstpage: 5367
lastpage: 5382
page: 5367-5382
order: 5367
cycles: false
bibtex_author: Chen, Tianqi and Zhou, Mingyuan
author:
- given: Tianqi
  family: Chen
- given: Mingyuan
  family: Zhou
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/chen23ap/chen23ap.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
