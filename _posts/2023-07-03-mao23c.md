---
title: Supported Trust Region Optimization for Offline Reinforcement Learning
openreview: F6gApN2CVs
abstract: Offline reinforcement learning suffers from the out-of-distribution issue
  and extrapolation error. Most policy constraint methods regularize the density of
  the trained policy towards the behavior policy, which is too restrictive in most
  cases. We propose Supported Trust Region optimization (STR) which performs trust
  region policy optimization with the policy constrained within the support of the
  behavior policy, enjoying the less restrictive support constraint. We show that,
  when assuming no approximation and sampling error, STR guarantees strict policy
  improvement until convergence to the optimal support-constrained policy in the dataset.
  Further with both errors incorporated, STR still guarantees safe policy improvement
  for each step. Empirical results validate the theory of STR and demonstrate its
  state-of-the-art performance on MuJoCo locomotion domains and much more challenging
  AntMaze domains.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mao23c
month: 0
tex_title: Supported Trust Region Optimization for Offline Reinforcement Learning
firstpage: 23829
lastpage: 23851
page: 23829-23851
order: 23829
cycles: false
bibtex_author: Mao, Yixiu and Zhang, Hongchang and Chen, Chen and Xu, Yi and Ji, Xiangyang
author:
- given: Yixiu
  family: Mao
- given: Hongchang
  family: Zhang
- given: Chen
  family: Chen
- given: Yi
  family: Xu
- given: Xiangyang
  family: Ji
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/mao23c/mao23c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
