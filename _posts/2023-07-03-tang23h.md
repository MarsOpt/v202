---
title: VA-learning as a more efficient alternative to Q-learning
openreview: 64xmfTWt7X
abstract: 'In reinforcement learning, the advantage function is critical for policy
  improvement, but is often extracted from a learned Q-function. A natural question
  is: Why not learn the advantage function directly? In this work, we introduce VA-learning,
  which directly learns advantage function and value function using bootstrapping,
  without explicit reference to Q-functions. VA-learning learns off-policy and enjoys
  similar theoretical guarantees as Q-learning. Thanks to the direct learning of advantage
  function and value function, VA-learning improves the sample efficiency over Q-learning
  both in tabular implementations and deep RL agents on Atari-57 games. We also identify
  a close connection between VA-learning and the dueling architecture, which partially
  explains why a simple architectural change to DQN agents tends to improve performance.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tang23h
month: 0
tex_title: "{VA}-learning as a more efficient alternative to Q-learning"
firstpage: 33739
lastpage: 33757
page: 33739-33757
order: 33739
cycles: false
bibtex_author: Tang, Yunhao and Munos, Remi and Rowland, Mark and Valko, Michal
author:
- given: Yunhao
  family: Tang
- given: Remi
  family: Munos
- given: Mark
  family: Rowland
- given: Michal
  family: Valko
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/tang23h/tang23h.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
