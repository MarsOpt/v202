---
title: Model-based Offline Reinforcement Learning with Count-based Conservatism
openreview: T5VlejGx7f
abstract: In this paper, we present a model-based offline reinforcement learning method
  that integrates count-based conservatism, named $\texttt{Count-MORL}$. Our method
  utilizes the count estimates of state-action pairs to quantify model estimation
  error, marking the first algorithm of demonstrating the efficacy of count-based
  conservatism in model-based offline deep RL to the best of our knowledge. For our
  proposed method, we first show that the estimation error is inversely proportional
  to the frequency of state-action pairs. Secondly, we demonstrate that the learned
  policy under the count-based conservative model offers near-optimality performance
  guarantees. Through extensive numerical experiments, we validate that $\texttt{Count-MORL}$
  with hash code implementation significantly outperforms existing offline RL algorithms
  on the D4RL benchmark datasets. The code is accessible at https://github.com/oh-lab/Count-MORL.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim23q
month: 0
tex_title: Model-based Offline Reinforcement Learning with Count-based Conservatism
firstpage: 16728
lastpage: 16746
page: 16728-16746
order: 16728
cycles: false
bibtex_author: Kim, Byeongchan and Oh, Min-Hwan
author:
- given: Byeongchan
  family: Kim
- given: Min-Hwan
  family: Oh
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kim23q/kim23q.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
