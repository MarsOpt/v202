---
title: On the Global Convergence of Fitted Q-Iteration with Two-layer Neural Network
  Parametrization
openreview: 2azoCxs1jc
abstract: Deep Q-learning based algorithms have been applied successfully in many
  decision making problems, while their theoretical foundations are not as well understood.
  In this paper, we study a Fitted Q-Iteration with two-layer ReLU neural network
  parameterization, and find the sample complexity guarantees for the algorithm. Our
  approach estimates the Q-function in each iteration using a convex optimization
  problem. We show that this approach achieves a sample complexity of $\tilde{\mathcal{O}}(1/\epsilon^{2})$,
  which is order-optimal. This result holds for a countable state-spaces and does
  not require any assumptions such as a linear or low rank structure on the MDP.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gaur23a
month: 0
tex_title: On the Global Convergence of Fitted Q-Iteration with Two-layer Neural Network
  Parametrization
firstpage: 11013
lastpage: 11049
page: 11013-11049
order: 11013
cycles: false
bibtex_author: Gaur, Mudit and Aggarwal, Vaneet and Agarwal, Mridul
author:
- given: Mudit
  family: Gaur
- given: Vaneet
  family: Aggarwal
- given: Mridul
  family: Agarwal
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/gaur23a/gaur23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
