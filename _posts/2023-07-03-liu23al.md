---
title: Global Optimization with Parametric Function Approximation
openreview: h3GBd13xVv
abstract: We consider the problem of global optimization with noisy zeroth order oracles
  â€” a well-motivated problem useful for various applications ranging from hyper-parameter
  tuning for deep learning to new material design. Existing work relies on Gaussian
  processes or other non-parametric family, which suffers from the curse of dimensionality.
  In this paper, we propose a new algorithm GO-UCB that leverages a parametric family
  of functions (e.g., neural networks) instead. Under a realizable assumption and
  a few other mild geometric conditions, we show that GO-UCB achieves a cumulative
  regret of $\tilde{O}(\sqrt{T})$ where $T$ is the time horizon. At the core of GO-UCB
  is a carefully designed uncertainty set over parameters based on gradients that
  allows optimistic exploration. Synthetic and real-world experiments illustrate GO-UCB
  works better than popular Bayesian optimization approaches, even if the model is
  misspecified.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu23al
month: 0
tex_title: Global Optimization with Parametric Function Approximation
firstpage: 22113
lastpage: 22136
page: 22113-22136
order: 22113
cycles: false
bibtex_author: Liu, Chong and Wang, Yu-Xiang
author:
- given: Chong
  family: Liu
- given: Yu-Xiang
  family: Wang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/liu23al/liu23al.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
