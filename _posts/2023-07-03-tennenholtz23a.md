---
title: Reinforcement Learning with History Dependent Dynamic Contexts
openreview: rdOuTlTUMX
abstract: We introduce <em>Dynamic Contextual Markov Decision Processes (DCMDPs)</em>,
  a novel reinforcement learning framework for history-dependent environments that
  generalizes the contextual MDP framework to handle non-Markov environments, where
  contexts change over time. We consider special cases of the model, with a focus
  on logistic DCMDPs, which break the exponential dependence on history length by
  leveraging aggregation functions to determine context transitions. This special
  structure allows us to derive an upper-confidence-bound style algorithm for which
  we establish regret bounds. Motivated by our theoretical results, we introduce a
  practical model-based algorithm for logistic DCMDPs that plans in a latent space
  and uses optimism over history-dependent features. We demonstrate the efficacy of
  our approach on a recommendation task (using MovieLens data) where user behavior
  dynamics evolve in response to recommendations.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tennenholtz23a
month: 0
tex_title: Reinforcement Learning with History Dependent Dynamic Contexts
firstpage: 34011
lastpage: 34053
page: 34011-34053
order: 34011
cycles: false
bibtex_author: Tennenholtz, Guy and Merlis, Nadav and Shani, Lior and Mladenov, Martin
  and Boutilier, Craig
author:
- given: Guy
  family: Tennenholtz
- given: Nadav
  family: Merlis
- given: Lior
  family: Shani
- given: Martin
  family: Mladenov
- given: Craig
  family: Boutilier
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/tennenholtz23a/tennenholtz23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
