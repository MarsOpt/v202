---
title: Local Optimization Achieves Global Optimality in Multi-Agent Reinforcement
  Learning
openreview: V4jD1KmnQz
abstract: Policy optimization methods with function approximation are widely used
  in multi-agent reinforcement learning. However, it remains elusive how to design
  such algorithms with statistical guarantees. Leveraging a multi-agent performance
  difference lemma that characterizes the landscape of multi-agent policy optimization,
  we find that the localized action value function serves as an ideal descent direction
  for each local policy. Motivated by the observation, we present a multi-agent PPO
  algorithm in which the local policy of each agent is updated similarly to vanilla
  PPO. We prove that with standard regularity conditions on the Markov game and problem-dependent
  quantities, our algorithm converges to the globally optimal policy at a sublinear
  rate. We extend our algorithm to the off-policy setting and introduce pessimism
  to policy evaluation, which aligns with experiments. To our knowledge, this is the
  first provably convergent multi-agent PPO algorithm in cooperative Markov games.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao23j
month: 0
tex_title: Local Optimization Achieves Global Optimality in Multi-Agent Reinforcement
  Learning
firstpage: 42200
lastpage: 42226
page: 42200-42226
order: 42200
cycles: false
bibtex_author: Zhao, Yulai and Yang, Zhuoran and Wang, Zhaoran and Lee, Jason D.
author:
- given: Yulai
  family: Zhao
- given: Zhuoran
  family: Yang
- given: Zhaoran
  family: Wang
- given: Jason D.
  family: Lee
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhao23j/zhao23j.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
