---
title: Delay-Adapted Policy Optimization and Improved Regret for Adversarial MDP with
  Delayed Bandit Feedback
openreview: jW46xHCjUi
abstract: 'Policy Optimization (PO) is one of the most popular methods in Reinforcement
  Learning (RL). Thus, theoretical guarantees for PO algorithms have become especially
  important to the RL community. In this paper, we study PO in adversarial MDPs with
  a challenge that arises in almost every real-world application â€“ <em>delayed bandit
  feedback</em>. We give the first near-optimal regret bounds for PO in tabular MDPs,
  and may even surpass state-of-the-art (which uses less efficient methods). Our novel
  Delay-Adapted PO (DAPO) is easy to implement and to generalize, allowing us to extend
  our algorithm to: (i) infinite state space under the assumption of linear $Q$-function,
  proving the first regret bounds for delayed feedback with function approximation.
  (ii) deep RL, demonstrating its effectiveness in experiments on MuJoCo domains.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lancewicki23a
month: 0
tex_title: Delay-Adapted Policy Optimization and Improved Regret for Adversarial {MDP}
  with Delayed Bandit Feedback
firstpage: 18482
lastpage: 18534
page: 18482-18534
order: 18482
cycles: false
bibtex_author: Lancewicki, Tal and Rosenberg, Aviv and Sotnikov, Dmitry
author:
- given: Tal
  family: Lancewicki
- given: Aviv
  family: Rosenberg
- given: Dmitry
  family: Sotnikov
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/lancewicki23a/lancewicki23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
