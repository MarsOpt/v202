---
title: 'OCD: Learning to Overfit with Conditional Diffusion Models'
openreview: BgRiBwPKHC
abstract: We present a dynamic model in which the weights are conditioned on an input
  sample x and are learned to match those that would be obtained by finetuning a base
  model on x and its label y. This mapping between an input sample and network weights
  is approximated by a denoising diffusion model. The diffusion model we employ focuses
  on modifying a single layer of the base model and is conditioned on the input, activations,
  and output of this layer. Since the diffusion model is stochastic in nature, multiple
  initializations generate different networks, forming an ensemble, which leads to
  further improvements. Our experiments demonstrate the wide applicability of the
  method for image classification, 3D reconstruction, tabular data, speech separation,
  and natural language processing.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lutati23a
month: 0
tex_title: "{OCD}: Learning to Overfit with Conditional Diffusion Models"
firstpage: 23157
lastpage: 23169
page: 23157-23169
order: 23157
cycles: false
bibtex_author: Lutati, Shahar and Wolf, Lior
author:
- given: Shahar
  family: Lutati
- given: Lior
  family: Wolf
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/lutati23a/lutati23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
