---
title: Effective Structured Prompting by Meta-Learning and Representative Verbalizer
openreview: BLz4inIrHl
abstract: Prompt tuning for pre-trained masked language models (MLM) has shown promising
  performance in natural language processing tasks with few labeled examples. It tunes
  a prompt for the downstream task, and a verbalizer is used to bridge the predicted
  token and label prediction. Due to the limited training data, prompt initialization
  is crucial for prompt tuning. Recently, MetaPrompting (Hou et al., 2022) uses meta-learning
  to learn a shared initialization for all task-specific prompts. However, a single
  initialization is insufficient to obtain good prompts for all tasks and samples
  when the tasks are complex. Moreover, MetaPrompting requires tuning the whole MLM,
  causing a heavy burden on computation and memory as the MLM is usually large. To
  address these issues, we use a prompt pool to extract more task knowledge and construct
  instance-dependent prompts via attention. We further propose a novel soft verbalizer
  (RepVerb) which constructs label embedding from feature embeddings directly. Combining
  meta-learning the prompt pool and RepVerb, we propose MetaPrompter for effective
  structured prompting. MetaPrompter is parameter-efficient as only the pool is required
  to be tuned. Experimental results demonstrate that MetaPrompter performs better
  than the recent state-of-the-arts and RepVerb outperforms existing soft verbalizers.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jiang23k
month: 0
tex_title: Effective Structured Prompting by Meta-Learning and Representative Verbalizer
firstpage: 15186
lastpage: 15199
page: 15186-15199
order: 15186
cycles: false
bibtex_author: Jiang, Weisen and Zhang, Yu and Kwok, James
author:
- given: Weisen
  family: Jiang
- given: Yu
  family: Zhang
- given: James
  family: Kwok
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/jiang23k/jiang23k.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
