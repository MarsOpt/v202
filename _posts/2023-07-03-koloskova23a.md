---
title: 'Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees'
openreview: C3DXiFTrve
abstract: Gradient clipping is a popular modification to standard (stochastic) gradient
  descent, at every iteration limiting the gradient norm to a certain value $c >0$.
  It is widely used for example for stabilizing the training of deep learning models
  (Goodfellow et al., 2016), or for enforcing differential privacy (Abadi et al.,
  2016). Despite popularity and simplicity of the clipping mechanism, its convergence
  guarantees often require specific values of $c$ and strong noise assumptions. In
  this paper, we give convergence guarantees that show precise dependence on arbitrary
  clipping thresholds $c$ and show that our guarantees are tight with both deterministic
  and stochastic gradients. In particular, we show that (i) for deterministic gradient
  descent, the clipping threshold only affects the higher-order terms of convergence,
  (ii) in the stochastic setting convergence to the true optimum cannot be guaranteed
  under the standard noise assumption, even under arbitrary small step-sizes. We give
  matching upper and lower bounds for convergence of the gradient norm when running
  clipped SGD, and illustrate these results with experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: koloskova23a
month: 0
tex_title: 'Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees'
firstpage: 17343
lastpage: 17363
page: 17343-17363
order: 17343
cycles: false
bibtex_author: Koloskova, Anastasia and Hendrikx, Hadrien and Stich, Sebastian U
author:
- given: Anastasia
  family: Koloskova
- given: Hadrien
  family: Hendrikx
- given: Sebastian U
  family: Stich
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/koloskova23a/koloskova23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
