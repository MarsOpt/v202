---
title: Diffusion Models are Minimax Optimal Distribution Estimators
openreview: ORyo7fxcIA
abstract: While efficient distribution learning is no doubt behind the groundbreaking
  success of diffusion modeling, its theoretical guarantees are quite limited. In
  this paper, we provide the first rigorous analysis on approximation and generalization
  abilities of diffusion modeling for well-known function spaces. The highlight of
  this paper is that when the true density function belongs to the Besov space and
  the empirical score matching loss is properly minimized, the generated data distribution
  achieves the nearly minimax optimal estimation rates in the total variation distance
  and in the Wasserstein distance of order one. Furthermore, we extend our theory
  to demonstrate how diffusion models adapt to low-dimensional data distributions.
  We expect these results advance theoretical understandings of diffusion modeling
  and its ability to generate verisimilar outputs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: oko23a
month: 0
tex_title: Diffusion Models are Minimax Optimal Distribution Estimators
firstpage: 26517
lastpage: 26582
page: 26517-26582
order: 26517
cycles: false
bibtex_author: Oko, Kazusato and Akiyama, Shunta and Suzuki, Taiji
author:
- given: Kazusato
  family: Oko
- given: Shunta
  family: Akiyama
- given: Taiji
  family: Suzuki
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/oko23a/oko23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
