---
title: 'DIVISION: Memory Efficient Training via Dual Activation Precision'
openreview: Bxfp0zWygq
abstract: 'Activation compressed training provides a solution towards reducing the
  memory cost of training deep neural networks (DNNs). However, state-of-the-art work
  combines a search of quantization bit-width with the training, which makes the procedure
  complicated and less transparent. To this end, we propose a simple and effective
  method to compress DNN training. Our method is motivated by an instructive observation:
  DNN backward propagation mainly utilizes the low-frequency component (LFC) of the
  activation maps, while the majority of memory is for caching the high-frequency
  component (HFC) during the training. This indicates the HFC of activation maps is
  highly redundant and compressible, which inspires our proposed Dual Activation Precision
  (DIVISION). During the training, DIVISION preserves a high-precision copy of LFC
  and compresses the HFC into a light-weight copy with low numerical precision. This
  can significantly reduce the memory cost while maintaining a competitive model accuracy.
  Experiment results show DIVISION has better comprehensive performance than state-of-the-art
  methods, including over 10x compression of activation maps and competitive training
  throughput, without loss of model accuracy. The source code is available at https://github.com/guanchuwang/division.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23s
month: 0
tex_title: "{DIVISION}: Memory Efficient Training via Dual Activation Precision"
firstpage: 36036
lastpage: 36057
page: 36036-36057
order: 36036
cycles: false
bibtex_author: Wang, Guanchu and Liu, Zirui and Jiang, Zhimeng and Liu, Ninghao and
  Zou, Na and Hu, Xia
author:
- given: Guanchu
  family: Wang
- given: Zirui
  family: Liu
- given: Zhimeng
  family: Jiang
- given: Ninghao
  family: Liu
- given: Na
  family: Zou
- given: Xia
  family: Hu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wang23s/wang23s.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
