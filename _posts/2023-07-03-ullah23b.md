---
title: Private Federated Learning with Autotuned Compression
openreview: y8qAZhWbNs
abstract: We propose new techniques for reducing communication in private federated
  learning without the need for setting or tuning compression rates. Our on-the-fly
  methods automatically adjust the compression rate based on the error induced during
  training, while maintaining provable privacy guarantees through the use of secure
  aggregation and differential privacy. Our techniques are provably instance-optimal
  for mean estimation, meaning that they can adapt to the “hardness of the problem”
  with minimal interactivity. We demonstrate the effectiveness of our approach on
  real-world datasets by achieving favorable compression rates without the need for
  tuning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ullah23b
month: 0
tex_title: Private Federated Learning with Autotuned Compression
firstpage: 34668
lastpage: 34708
page: 34668-34708
order: 34668
cycles: false
bibtex_author: Ullah, Enayat and Choquette-Choo, Christopher A. and Kairouz, Peter
  and Oh, Sewoong
author:
- given: Enayat
  family: Ullah
- given: Christopher A.
  family: Choquette-Choo
- given: Peter
  family: Kairouz
- given: Sewoong
  family: Oh
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/ullah23b/ullah23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
