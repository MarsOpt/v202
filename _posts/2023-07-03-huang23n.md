---
title: Policy Contrastive Imitation Learning
openreview: MFNvOPd3hE
abstract: Adversarial imitation learning (AIL) is a popular method that has recently
  achieved much success. However, the performance of AIL is still unsatisfactory on
  the more challenging tasks. We find that one of the major reasons is due to the
  low quality of AIL discriminator representation. Since the AIL discriminator is
  trained via binary classification that does not necessarily discriminate the policy
  from the expert in a meaningful way, the resulting reward might not be meaningful
  either. We propose a new method called Policy Contrastive Imitation Learning (PCIL)
  to resolve this issue. PCIL learns a contrastive representation space by anchoring
  on different policies and uses a smooth cosine-similarity-based reward to encourage
  imitation learning. Our proposed representation learning objective can be viewed
  as a stronger version of the AIL objective and provide a more meaningful comparison
  between the agent and the policy. From a theoretical perspective, we show the validity
  of our method using the apprenticeship learning framework. Furthermore, our empirical
  evaluation on the DeepMind Control suite demonstrates that PCIL can achieve state-of-the-art
  performance. Finally, qualitative results suggest that PCIL builds a smoother and
  more meaningful representation space for imitation learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huang23n
month: 0
tex_title: Policy Contrastive Imitation Learning
firstpage: 14007
lastpage: 14022
page: 14007-14022
order: 14007
cycles: false
bibtex_author: Huang, Jialei and Yin, Zhao-Heng and Hu, Yingdong and Gao, Yang
author:
- given: Jialei
  family: Huang
- given: Zhao-Heng
  family: Yin
- given: Yingdong
  family: Hu
- given: Yang
  family: Gao
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/huang23n/huang23n.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
