---
title: On Pitfalls of Test-Time Adaptation
openreview: q0K36FPtOd
abstract: 'Test-Time Adaptation (TTA) has recently gained significant attention as
  a new paradigm for tackling distribution shifts. Despite the sheer number of existing
  methods, the inconsistent experimental conditions and lack of standardization in
  prior literature make it difficult to measure their actual efficacies and progress.
  To address this issue, we present a large-scale open-sourced Test-Time Adaptation
  Benchmark, dubbed TTAB, which includes nine state-of-the-art algorithms, a diverse
  array of distribution shifts, and two comprehensive evaluation protocols. Through
  extensive experiments, we identify three common pitfalls in prior efforts: (i) choosing
  appropriate hyper-parameter, especially for model selection, is exceedingly difficult
  due to online batch dependency; (ii) the effectiveness of TTA varies greatly depending
  on the quality of the model being adapted; (iii) even under optimal algorithmic
  conditions, existing methods still systematically struggle with certain types of
  distribution shifts. Our findings suggest that future research in the field should
  be more transparent about their experimental conditions, ensure rigorous evaluations
  on a broader set of models and shifts, and re-examine the assumptions underlying
  the potential success of TTA for practical applications.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao23d
month: 0
tex_title: On Pitfalls of Test-Time Adaptation
firstpage: 42058
lastpage: 42080
page: 42058-42080
order: 42058
cycles: false
bibtex_author: Zhao, Hao and Liu, Yuejiang and Alahi, Alexandre and Lin, Tao
author:
- given: Hao
  family: Zhao
- given: Yuejiang
  family: Liu
- given: Alexandre
  family: Alahi
- given: Tao
  family: Lin
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhao23d/zhao23d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
