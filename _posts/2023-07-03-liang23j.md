---
title: 'Less is More: Task-aware Layer-wise Distillation for Language Model Compression'
openreview: B6J3ygdPJh
abstract: 'Layer-wise distillation is a powerful tool to compress large models (i.e.
  teacher models) into small ones (i.e., student models). The student distills knowledge
  from the teacher by mimicking the hidden representations of the teacher at every
  intermediate layer. However, layer-wise distillation is difficult. Since the student
  has a smaller model capacity than the teacher, it is often under-fitted. Furthermore,
  the hidden representations of the teacher contain redundant information that the
  student does not necessarily need for the target taskâ€™s learning. To address these
  challenges, we propose a novel Task-aware layEr-wise Distillation (TED). TED designs
  task-aware filters to align the hidden representations of the student and the teacher
  at each layer. The filters select the knowledge that is useful for the target task
  from the hidden representations. As such, TED reduces the knowledge gap between
  the two models and helps the student to fit better on the target task. We evaluate
  TED in two scenarios: continual pre-training and fine-tuning. TED demonstrates significant
  and consistent improvements over existing distillation methods in both scenarios.
  Code is available at https://github.com/cliang1453/task-aware-distillation.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liang23j
month: 0
tex_title: 'Less is More: Task-aware Layer-wise Distillation for Language Model Compression'
firstpage: 20852
lastpage: 20867
page: 20852-20867
order: 20852
cycles: false
bibtex_author: Liang, Chen and Zuo, Simiao and Zhang, Qingru and He, Pengcheng and
  Chen, Weizhu and Zhao, Tuo
author:
- given: Chen
  family: Liang
- given: Simiao
  family: Zuo
- given: Qingru
  family: Zhang
- given: Pengcheng
  family: He
- given: Weizhu
  family: Chen
- given: Tuo
  family: Zhao
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/liang23j/liang23j.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
