---
title: Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization
openreview: VqnEAUnfvu
abstract: We consider the optimization problem of the form $\min_{x \in \mathbb{R}^d}
  f(x) \triangleq \mathbb{E}[F(x;\xi)]$ , where the component $F(x;\xi)$ is $L$-mean-squared
  Lipschitz but possibly nonconvex and nonsmooth.The recently proposed gradient-free
  method requires at most $\mathcal{O}( L^4 d^{3/2} \epsilon^{-4} + \Delta L^3 d^{3/2}
  \delta^{-1} \epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a
  $(\delta,\epsilon)$-Goldstein stationary point of objective function, where $\Delta
  = f(x_0) - \inf_{x \in \mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the
  algorithm. This paper proposes a more efficient algorithm using stochastic recursive
  gradient estimators, which improves the complexity to $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+
  \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen23ai
month: 0
tex_title: Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization
firstpage: 5219
lastpage: 5233
page: 5219-5233
order: 5219
cycles: false
bibtex_author: Chen, Lesi and Xu, Jing and Luo, Luo
author:
- given: Lesi
  family: Chen
- given: Jing
  family: Xu
- given: Luo
  family: Luo
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/chen23ai/chen23ai.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
