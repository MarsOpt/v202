---
title: Reprogramming Pretrained Language Models for Antibody Sequence Infilling
openreview: K2gn1WiLAu
abstract: Antibodies comprise the most versatile class of binding molecules, with
  numerous applications in biomedicine. Computational design of antibodies involves
  generating novel and diverse sequences, while maintaining structural consistency.
  Unique to antibodies, designing the complementarity-determining region (CDR), which
  determines the antigen binding affinity and specificity, creates its own unique
  challenges. Recent deep learning models have shown impressive results, however the
  limited number of known antibody sequence/structure pairs frequently leads to degraded
  performance, particularly lacking diversity in the generated sequences. In our work
  we address this challenge by leveraging Model Reprogramming (MR), which repurposes
  pretrained models on a source language to adapt to the tasks that are in a different
  language and have scarce data - where it may be difficult to train a high-performing
  model from scratch or effectively fine-tune an existing pre-trained model on the
  specific task. Specifically, we introduce ReprogBert in which a pretrained English
  language model is repurposed for protein sequence infilling - thus considers cross-language
  adaptation using less data. Results on antibody design benchmarks show that our
  model on low-resourced antibody sequence dataset provides highly diverse CDR sequences,
  up to more than a two-fold increase of diversity over the baselines, without losing
  structural integrity and naturalness. The generated sequences also demonstrate enhanced
  antigen binding specificity and virus neutralization ability. Code is available
  at https://github.com/IBM/ReprogBERT
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: melnyk23a
month: 0
tex_title: Reprogramming Pretrained Language Models for Antibody Sequence Infilling
firstpage: 24398
lastpage: 24419
page: 24398-24419
order: 24398
cycles: false
bibtex_author: Melnyk, Igor and Chenthamarakshan, Vijil and Chen, Pin-Yu and Das,
  Payel and Dhurandhar, Amit and Padhi, Inkit and Das, Devleena
author:
- given: Igor
  family: Melnyk
- given: Vijil
  family: Chenthamarakshan
- given: Pin-Yu
  family: Chen
- given: Payel
  family: Das
- given: Amit
  family: Dhurandhar
- given: Inkit
  family: Padhi
- given: Devleena
  family: Das
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/melnyk23a/melnyk23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
