---
title: Resurrecting Recurrent Neural Networks for Long Sequences
openreview: M3Yd3QyRG4
abstract: Recurrent Neural Networks (RNNs) offer fast inference on long sequences
  but are hard to optimize and slow to train. Deep state-space models (SSMs) have
  recently been shown to perform remarkably well on long sequence modeling tasks,
  and have the added benefits of fast parallelizable training and RNN-like fast inference.
  However, while SSMs are superficially similar to RNNs, there are important differences
  that make it unclear where their performance boost over RNNs comes from. We show
  that careful design of deep RNNs using standard signal propagation arguments can
  recover the impressive performance of deep SSMs on long-range reasoning tasks, while
  matching their training speed. To achieve this, we analyze and ablate a series of
  changes to standard RNNs including linearizing and diagonalizing the recurrence,
  using better parameterizations and initializations, and ensuring careful normalization
  of the forward pass. Our results provide new insights on the origins of the impressive
  performance of deep SSMs, and introduce an RNN block called the Linear Recurrent
  Unit (or LRU) that matches both their performance on the Long Range Arena benchmark
  and their computational efficiency.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: orvieto23a
month: 0
tex_title: Resurrecting Recurrent Neural Networks for Long Sequences
firstpage: 26670
lastpage: 26698
page: 26670-26698
order: 26670
cycles: false
bibtex_author: Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan
  and Gulcehre, Caglar and Pascanu, Razvan and De, Soham
author:
- given: Antonio
  family: Orvieto
- given: Samuel L
  family: Smith
- given: Albert
  family: Gu
- given: Anushan
  family: Fernando
- given: Caglar
  family: Gulcehre
- given: Razvan
  family: Pascanu
- given: Soham
  family: De
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/orvieto23a/orvieto23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
