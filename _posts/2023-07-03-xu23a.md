---
title: Why do Nearest Neighbor Language Models Work?
openreview: ARDbU7beLp
abstract: 'Language models (LMs) compute the probability of a text by sequentially
  computing a representation of an already-seen context and using this representation
  to predict the next word. Currently, most LMs calculate these representations through
  a neural network consuming the immediate previous context. However recently, retrieval-augmented
  LMs have shown to improve over standard neural LMs, by accessing information retrieved
  from a large datastore, in addition to their standard, parametric, next-word prediction.
  In this paper, we set out to understand why retrieval-augmented language models,
  and specifically why k-nearest neighbor language models (kNN-LMs) perform better
  than standard parametric LMs, even when the k-nearest neighbor component retrieves
  examples from the same training set that the LM was originally trained on. To this
  end, we perform analysis of various dimensions over which kNN-LM diverges from standard
  LMs, and investigate these dimensions one by one. Empirically, we identify three
  main reasons why kNN-LM performs better than standard LMs: using a different input
  representation for predicting the next tokens, approximate kNN search, and the importance
  of softmax temperature for the kNN distribution. Further, we incorporate some insights
  into the standard parametric LM, improving performance without the need for an explicit
  retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu23a
month: 0
tex_title: Why do Nearest Neighbor Language Models Work?
firstpage: 38325
lastpage: 38341
page: 38325-38341
order: 38325
cycles: false
bibtex_author: Xu, Frank F. and Alon, Uri and Neubig, Graham
author:
- given: Frank F.
  family: Xu
- given: Uri
  family: Alon
- given: Graham
  family: Neubig
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/xu23a/xu23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
