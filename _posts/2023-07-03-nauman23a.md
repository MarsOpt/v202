---
title: On Many-Actions Policy Gradient
openreview: HKfSTYLJh7
abstract: We study the variance of stochastic policy gradients (SPGs) with many action
  samples per state. We derive a many-actions optimality condition, which determines
  when many-actions SPG yields lower variance as compared to a single-action agent
  with proportionally extended trajectory. We propose Model-Based Many-Actions (MBMA),
  an approach leveraging dynamics models for many-actions sampling in the context
  of SPG. MBMA addresses issues associated with existing implementations of many-actions
  SPG and yields lower bias and comparable variance to SPG estimated from states in
  model-simulated rollouts. We find that MBMA bias and variance structure matches
  that predicted by theory. As a result, MBMA achieves improved sample efficiency
  and higher returns on a range of continuous action environments as compared to model-free,
  many-actions, and model-based on-policy SPG baselines.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nauman23a
month: 0
tex_title: On Many-Actions Policy Gradient
firstpage: 25769
lastpage: 25789
page: 25769-25789
order: 25769
cycles: false
bibtex_author: Nauman, Michal and Cygan, Marek
author:
- given: Michal
  family: Nauman
- given: Marek
  family: Cygan
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/nauman23a/nauman23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
