---
title: Optimal Shrinkage for Distributed Second-Order Optimization
openreview: bbKEGbS7aN
abstract: In this work, we address the problem of Hessian inversion bias in distributed
  second-order optimization algorithms. We introduce a novel shrinkage-based estimator
  for the resolvent of gram matrices which is asymptotically unbiased, and characterize
  its non-asymptotic convergence rate in the isotropic case. We apply this estimator
  to bias correction of Newton steps in distributed second-order optimization algorithms,
  as well as randomized sketching based methods. We examine the bias present in the
  naive averaging-based distributed Newtonâ€™s method using analytical expressions and
  contrast it with our proposed biasfree approach. Our approach leads to significant
  improvements in convergence rate compared to standard baselines and recent proposals,
  as shown through experiments on both real and synthetic datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang23ah
month: 0
tex_title: Optimal Shrinkage for Distributed Second-Order Optimization
firstpage: 41523
lastpage: 41549
page: 41523-41549
order: 41523
cycles: false
bibtex_author: Zhang, Fangzhao and Pilanci, Mert
author:
- given: Fangzhao
  family: Zhang
- given: Mert
  family: Pilanci
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhang23ah/zhang23ah.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
