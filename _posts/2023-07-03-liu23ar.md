---
title: Revisiting Pseudo-Label for Single-Positive Multi-Label Learning
openreview: wPn3W55oiU
abstract: To deal with the challenge of high cost of annotating all relevant labels
  for each example in multi-label learning, single-positive multi-label learning (SPMLL)
  has been studied in recent years, where each example is annotated with only one
  positive label. By adopting pseudo-label generation, i.e., assigning pseudo-label
  to each example by various strategies, existing methods have empirically validated
  that SPMLL would significantly reduce the amount of supervision with a tolerable
  damage in classification performance. However, there is no existing method that
  can provide a theoretical guarantee for learning from pseudo-label on SPMLL. In
  this paper, the conditions of the effectiveness of learning from pseudo-label for
  SPMLL are shown and the learnability of pseudo-label-based methods is proven. Furthermore,
  based on the theoretical guarantee of pseudo-label for SPMLL, we propose a novel
  SPMLL method named MIME, i.e., Mutual label enhancement for sIngle-positive Multi-label
  lEarning and prove that the generated pseudo-label by MIME approximately converges
  to the fully-supervised case. Experiments on four image datasets and five MLL datasets
  show the effectiveness of our methods over several existing SPMLL approaches.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu23ar
month: 0
tex_title: Revisiting Pseudo-Label for Single-Positive Multi-Label Learning
firstpage: 22249
lastpage: 22265
page: 22249-22265
order: 22249
cycles: false
bibtex_author: Liu, Biao and Xu, Ning and Lv, Jiaqi and Geng, Xin
author:
- given: Biao
  family: Liu
- given: Ning
  family: Xu
- given: Jiaqi
  family: Lv
- given: Xin
  family: Geng
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/liu23ar/liu23ar.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
