---
title: Emergent Agentic Transformer from Chain of Hindsight Experience
openreview: 4EINAXMptc
abstract: Large transformer models powered by diverse data and model scale have dominated
  natural language modeling and computer vision and pushed the frontier of multiple
  AI areas. In reinforcement learning (RL), despite many efforts into transformer-based
  policies, a key limitation, however, is that current transformer-based policies
  cannot learn by directly combining information from multiple sub-optimal trials.
  In this work, we address this issue using recently proposed chain of hindsight to
  relabel experience, where we train a transformer on a sequence of trajectory experience
  ascending sorted according to their total rewards. Our method consists of relabelling
  target return of each trajectory to the maximum total reward among in sequence of
  trajectories and training an autoregressive model to predict actions conditioning
  on past states, actions, rewards, target returns, and task completion tokens, the
  resulting model, Agentic Transformer (AT), can learn to improve upon itself both
  at training and test time. As we show on D4RL and ExoRL benchmarks, to the best
  our knowledge, this is the first time that a simple transformer-based model performs
  competitively with both temporal-difference and imitation-learning-based approaches,
  even from sub-optimal data. Our Agentic Transformer also shows a promising scaling
  trend that bigger models consistently improve results.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu23a
month: 0
tex_title: Emergent Agentic Transformer from Chain of Hindsight Experience
firstpage: 21362
lastpage: 21374
page: 21362-21374
order: 21362
cycles: false
bibtex_author: Liu, Hao and Abbeel, Pieter
author:
- given: Hao
  family: Liu
- given: Pieter
  family: Abbeel
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/liu23a/liu23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
