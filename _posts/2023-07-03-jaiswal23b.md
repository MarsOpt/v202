---
title: 'Instant Soup: Cheap Pruning Ensembles in A Single Pass Can Draw Lottery Tickets
  from Large Models'
openreview: CuWORvLAnp
abstract: 'Large pre-trained transformers have been receiving explosive attention
  in the past few years, due to their acculturation for numerous downstream applications
  via fine-tuning, but their exponentially increasing parameter counts are becoming
  a primary hurdle to even just fine-tune them without industry-standard hardware.
  Recently, Lottery Ticket Hypothesis (LTH) and its variants, have been exploited
  to prune these large pre-trained models generating subnetworks which can achieve
  similar performance as their dense counterparts, but LTH pragmatism is enormously
  inhibited by repetitive full training and pruning routine of iterative magnitude
  pruning (IMP) which worsens with increasing model size. Motivated by the recent
  observations of model soups, which suggest that fine-tuned weights of multiple models
  can be merged to a better minima, we propose <b>Instant Soup Pruning (ISP)</b> to
  generate lottery ticket quality subnetworks, using a fraction of the original IMP
  cost by replacing the expensive intermediate pruning stages of IMP with computationally
  efficient weak mask generation and aggregation routine. More specifically, during
  the mask generation stage, ISP takes a small handful of iterations using varying
  training protocols and data subsets to generate many weak and noisy subnetworks,
  and superpose them to average out the noise creating a high-quality denoised subnetwork.
  Our extensive experiments and ablation on two popular large-scale pre-trained models:
  $\texttt{CLIP} (unexplored in pruning till date)$ and $\texttt{BERT}$ across multiple
  benchmark vision $\texttt{\{MNIST, SVHN, Cars, GTSRB, CIFAR-10, CIFAR-100\}}$ and
  language datasets $\texttt{\{MNLI, QNLI, QQP, SST, ...\}}$ validate the effectiveness
  of ISP compared to several state-of-the-art pruning methods. Additionally, we show
  that ISP can be easily modified with minimal overhead to produce benefits comparable
  to model soups, without the prerequisite to generate multiple candidates fine-tuned
  models. Codes are available at: https://github.com/VITA-Group/instant_soup.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jaiswal23b
month: 0
tex_title: 'Instant Soup: Cheap Pruning Ensembles in A Single Pass Can Draw Lottery
  Tickets from Large Models'
firstpage: 14691
lastpage: 14701
page: 14691-14701
order: 14691
cycles: false
bibtex_author: Jaiswal, Ajay Kumar and Liu, Shiwei and Chen, Tianlong and Ding, Ying
  and Wang, Zhangyang
author:
- given: Ajay Kumar
  family: Jaiswal
- given: Shiwei
  family: Liu
- given: Tianlong
  family: Chen
- given: Ying
  family: Ding
- given: Zhangyang
  family: Wang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/jaiswal23b/jaiswal23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
