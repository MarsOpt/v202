---
title: Width and Depth Limits Commute in Residual Networks
openreview: Hgympy5bR0
abstract: We show that taking the width and depth to infinity in a deep neural network
  with skip connections, when branches are scaled by $1/\sqrt{depth}$, result in the
  same covariance structure no matter how that limit is taken. This explains why the
  standard infinite-width-then-depth approach provides practical insights even for
  networks with depth of the same order as width. We also demonstrate that the pre-activations,
  in this case, have Gaussian distributions which has direct applications in Bayesian
  deep learning. We conduct extensive simulations that show an excellent match with
  our theoretical findings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hayou23a
month: 0
tex_title: Width and Depth Limits Commute in Residual Networks
firstpage: 12700
lastpage: 12723
page: 12700-12723
order: 12700
cycles: false
bibtex_author: Hayou, Soufiane and Yang, Greg
author:
- given: Soufiane
  family: Hayou
- given: Greg
  family: Yang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/hayou23a/hayou23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
