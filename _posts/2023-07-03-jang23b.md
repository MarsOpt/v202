---
title: Learning to Boost Training by Periodic Nowcasting Near Future Weights
openreview: zHDdkb8LRQ
abstract: Recent complicated problems require large-scale datasets and complex model
  architectures, however, it is difficult to train such large networks due to high
  computational issues. Significant efforts have been made to make the training more
  efficient such as momentum, learning rate scheduling, weight regularization, and
  meta-learning. Based on our observations on 1) high correlation between past eights
  and future weights, 2) conditions for beneficial weight prediction, and 3) feasibility
  of weight prediction, we propose a more general framework by intermittently skipping
  a handful of epochs by periodically forecasting near future weights, i.e., a Weight
  Nowcaster Network (WNN). As an add-on module, WNN predicts the future weights to
  make the learning process faster regardless of tasks and architectures. Experimental
  results show that WNN can significantly save actual time cost for training with
  an additional marginal time to train WNN. We validate the generalization capability
  of WNN under various tasks, and demonstrate that it works well even for unseen tasks.
  The code and pre-trained model are available at https://github.com/jjh6297/WNN.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jang23b
month: 0
tex_title: Learning to Boost Training by Periodic Nowcasting Near Future Weights
firstpage: 14730
lastpage: 14757
page: 14730-14757
order: 14730
cycles: false
bibtex_author: Jang, Jinhyeok and Yun, Woo-Han and Kim, Won Hwa and Yoon, Youngwoo
  and Kim, Jaehong and Lee, Jaeyeon and Han, Byungok
author:
- given: Jinhyeok
  family: Jang
- given: Woo-Han
  family: Yun
- given: Won Hwa
  family: Kim
- given: Youngwoo
  family: Yoon
- given: Jaehong
  family: Kim
- given: Jaeyeon
  family: Lee
- given: Byungok
  family: Han
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/jang23b/jang23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
