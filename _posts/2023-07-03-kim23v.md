---
title: An Adaptive Entropy-Regularization Framework for Multi-Agent Reinforcement
  Learning
openreview: MP7HOGfLf3
abstract: 'In this paper, we propose an adaptive entropy-regularization framework
  (ADER) for multi-agent reinforcement learning (RL) to learn the adequate amount
  of exploration of each agent for entropy-based exploration. In order to derive a
  metric for the proper level of exploration entropy for each agent, we disentangle
  the soft value function into two types: one for pure return and the other for entropy.
  By applying multi-agent value factorization to the disentangled value function of
  pure return, we obtain a metric to determine the relevant level of exploration entropy
  for each agent, given by the partial derivative of the pure-return value function
  with respect to (w.r.t.) the policy entropy of each agent. Based on this metric,
  we propose the ADER algorithm based on maximum entropy RL, which controls the necessary
  level of exploration across agents over time by learning the proper target entropy
  for each agent. Experimental results show that the proposed scheme significantly
  outperforms current state-of-the-art multi-agent RL algorithms.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim23v
month: 0
tex_title: An Adaptive Entropy-Regularization Framework for Multi-Agent Reinforcement
  Learning
firstpage: 16829
lastpage: 16852
page: 16829-16852
order: 16829
cycles: false
bibtex_author: Kim, Woojun and Sung, Youngchul
author:
- given: Woojun
  family: Kim
- given: Youngchul
  family: Sung
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kim23v/kim23v.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
