---
title: Neural Status Registers
openreview: mZUEThXS1s
abstract: We study the problem of learning comparisons between numbers with neural
  networks. Despite comparisons being a seemingly simple problem, we find that both
  general-purpose models such as multilayer perceptrons (MLPs) as well as arithmetic
  architectures such as the Neural Arithmetic Logic Unit (NALU) struggle with learning
  comparisons. Neither architecture can extrapolate to much larger numbers than those
  seen in the training set. We propose a novel differentiable architecture, the Neural
  Status Register (NSR) to solve this problem. We experimentally validate the NSR
  in various settings. We can combine the NSR with other neural models to solve interesting
  problems such as piecewise-defined arithmetic, comparison of digit images, recurrent
  problems, or finding shortest paths in graphs. The NSR outperforms all baseline
  architectures, especially when it comes to extrapolating to larger numbers.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: faber23a
month: 0
tex_title: Neural Status Registers
firstpage: 9508
lastpage: 9522
page: 9508-9522
order: 9508
cycles: false
bibtex_author: Faber, Lukas and Wattenhofer, Roger
author:
- given: Lukas
  family: Faber
- given: Roger
  family: Wattenhofer
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/faber23a/faber23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
