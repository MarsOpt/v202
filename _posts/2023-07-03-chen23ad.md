---
title: Semi-Offline Reinforcement Learning for Optimized Text Generation
openreview: fscQU9Wufk
abstract: Existing reinforcement learning (RL) mainly utilize online or offline settings.
  The online methods explore the environment with expensive time cost, and the offline
  methods efficiently obtain reward signals by sacrificing the exploration capability.
  We propose semi-offline RL, a novel paradigm that can smoothly transit from the
  offline setting to the online setting, balances the exploration capability and training
  cost, and provides a theoretical foundation for comparing different RL settings.
  Based on the semi-offline MDP formulation, we present the RL setting that is optimal
  in terms of optimization cost, asymptotic error, and overfitting error bound. Extensive
  experiments show that our semi-offline RL approach is effective in various text
  generation tasks and datasets, and yields comparable or usually better performance
  compared with the state-of-the-art methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen23ad
month: 0
tex_title: Semi-Offline Reinforcement Learning for Optimized Text Generation
firstpage: 5087
lastpage: 5103
page: 5087-5103
order: 5087
cycles: false
bibtex_author: Chen, Changyu and Wang, Xiting and Jin, Yiqiao and Dong, Victor Ye
  and Dong, Li and Cao, Jie and Liu, Yi and Yan, Rui
author:
- given: Changyu
  family: Chen
- given: Xiting
  family: Wang
- given: Yiqiao
  family: Jin
- given: Victor Ye
  family: Dong
- given: Li
  family: Dong
- given: Jie
  family: Cao
- given: Yi
  family: Liu
- given: Rui
  family: Yan
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/chen23ad/chen23ad.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
