---
title: Gradient Descent Monotonically Decreases the Sharpness of Gradient Flow Solutions
  in Scalar Networks and Beyond
openreview: fsFYFyOFb4
abstract: 'Recent research shows that when Gradient Descent (GD) is applied to neural
  networks, the loss almost never decreases monotonically. Instead, the loss oscillates
  as gradient descent converges to its “Edge of Stability” (EoS). Here, we find a
  quantity that does decrease monotonically throughout GD training: the sharpness
  attained by the gradient flow solution (GFS)—the solution that would be obtained
  if, from now until convergence, we train with an infinitesimal step size. Theoretically,
  we analyze scalar neural networks with the squared loss, perhaps the simplest setting
  where the EoS phenomena still occur. In this model, we prove that the GFS sharpness
  decreases monotonically. Using this result, we characterize settings where GD provably
  converges to the EoS in scalar networks. Empirically, we show that GD monotonically
  decreases the GFS sharpness in a squared regression model as well as practical neural
  network architectures.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kreisler23a
month: 0
tex_title: Gradient Descent Monotonically Decreases the Sharpness of Gradient Flow
  Solutions in Scalar Networks and Beyond
firstpage: 17684
lastpage: 17744
page: 17684-17744
order: 17684
cycles: false
bibtex_author: Kreisler, Itai and Nacson, Mor Shpigel and Soudry, Daniel and Carmon,
  Yair
author:
- given: Itai
  family: Kreisler
- given: Mor Shpigel
  family: Nacson
- given: Daniel
  family: Soudry
- given: Yair
  family: Carmon
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kreisler23a/kreisler23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
