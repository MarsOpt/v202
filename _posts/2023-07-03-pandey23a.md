---
title: 'Learn to Accumulate Evidence from All Training Samples: Theory and Practice'
openreview: 2MaUpKBSju
abstract: 'Evidential deep learning, built upon belief theory and subjective logic,
  offers a principled and computationally efficient way to turn a deterministic neural
  network uncertainty-aware. The resultant evidential models can quantify fine-grained
  uncertainty using the learned evidence. To ensure theoretically sound evidential
  models, the evidence needs to be non-negative, which requires special activation
  functions for model training and inference. This constraint often leads to inferior
  predictive performance compared to standard softmax models, making it challenging
  to extend them to many large-scale datasets. To unveil the real cause of this undesired
  behavior, we theoretically investigate evidential models and identify a fundamental
  limitation that explains the inferior performance: existing evidential activation
  functions create <em>zero evidence regions</em>, which prevent the model to learn
  from training samples falling into such regions. A deeper analysis of evidential
  activation functions based on our theoretical underpinning inspires the design of
  a novel regularizer that effectively alleviates this fundamental limitation. Extensive
  experiments over many challenging real-world datasets and settings confirm our theoretical
  findings and demonstrate the effectiveness of our proposed approach.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pandey23a
month: 0
tex_title: 'Learn to Accumulate Evidence from All Training Samples: Theory and Practice'
firstpage: 26963
lastpage: 26989
page: 26963-26989
order: 26963
cycles: false
bibtex_author: Pandey, Deep Shankar and Yu, Qi
author:
- given: Deep Shankar
  family: Pandey
- given: Qi
  family: Yu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/pandey23a/pandey23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
