---
title: 'Brainformers: Trading Simplicity for Efficiency'
openreview: eEbk8eEpjU
abstract: Transformers are central to recent successes in natural language processing
  and computer vision. Transformers have a mostly uniform backbone where layers alternate
  between feed-forward and self-attention in order to build a deep network. Here we
  investigate this design choice and find that more complex blocks that have different
  permutations of layer primitives can be more efficient. Using this insight, we develop
  a complex block, named Brainformer, that consists of a diverse sets of layers such
  as sparsely gated feed-forward layers, dense feed-forward layers, attention layers,
  and various forms of layer normalization and activation functions. Brainformer consistently
  outperforms the state-of-the-art dense and sparse Transformers, in terms of both
  quality and efficiency. A Brainformer model with 8 billion activated parameters
  per token demonstrates 2x faster training convergence and 5x faster step time compared
  to its GLaM counterpart. In downstream task evaluation, Brainformer also demonstrates
  a 3% higher SuperGLUE score with fine-tuning compared to GLaM with a similar number
  of activated parameters. Finally, Brainformer largely outperforms a Primer dense
  model derived with NAS with similar computation per token on fewshot evaluations.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhou23c
month: 0
tex_title: 'Brainformers: Trading Simplicity for Efficiency'
firstpage: 42531
lastpage: 42542
page: 42531-42542
order: 42531
cycles: false
bibtex_author: Zhou, Yanqi and Du, Nan and Huang, Yanping and Peng, Daiyi and Lan,
  Chang and Huang, Da and Shakeri, Siamak and So, David and Dai, Andrew M. and Lu,
  Yifeng and Chen, Zhifeng and Le, Quoc V and Cui, Claire and Laudon, James and Dean,
  Jeff
author:
- given: Yanqi
  family: Zhou
- given: Nan
  family: Du
- given: Yanping
  family: Huang
- given: Daiyi
  family: Peng
- given: Chang
  family: Lan
- given: Da
  family: Huang
- given: Siamak
  family: Shakeri
- given: David
  family: So
- given: Andrew M.
  family: Dai
- given: Yifeng
  family: Lu
- given: Zhifeng
  family: Chen
- given: Quoc V
  family: Le
- given: Claire
  family: Cui
- given: James
  family: Laudon
- given: Jeff
  family: Dean
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhou23c/zhou23c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
