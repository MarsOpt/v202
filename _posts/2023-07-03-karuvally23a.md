---
title: General Sequential Episodic Memory Model
openreview: wmz7xLOzAi
abstract: The state-of-the-art memory model is the General Associative Memory Model,
  a generalization of the classical Hopfield network. Like its ancestor, the general
  associative memory has a well-defined state-dependant energy surface, and its memories
  correlate with its fixed points. This is unlike human memories, which are commonly
  sequential rather than separated fixed points. In this paper, we introduce a class
  of General Sequential Episodic Memory Models (GSEMM) that, in the adiabatic limit,
  exhibit a dynamic energy surface, leading to a series of meta-stable states capable
  of encoding memory sequences. A multiple-timescale architecture enables the dynamic
  nature of the energy surface with newly introduced asymmetric synapses and signal
  propagation delays. We demonstrate its dense capacity under polynomial activation
  functions. GSEMM combines separate memories, short and long sequential episodic
  memories, under a unified theoretical framework, demonstrating how energy-based
  memory modeling can provide richer, human-like episodes.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: karuvally23a
month: 0
tex_title: General Sequential Episodic Memory Model
firstpage: 15900
lastpage: 15910
page: 15900-15910
order: 15900
cycles: false
bibtex_author: Karuvally, Arjun and Sejnowski, Terrence and Siegelmann, Hava T
author:
- given: Arjun
  family: Karuvally
- given: Terrence
  family: Sejnowski
- given: Hava T
  family: Siegelmann
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/karuvally23a/karuvally23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
