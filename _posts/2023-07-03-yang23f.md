---
title: Data Poisoning Attacks Against Multimodal Encoders
openreview: K53zoOWF8g
abstract: 'Recently, the newly emerged multimodal models, which leverage both visual
  and linguistic modalities to train powerful encoders, have gained increasing attention.
  However, learning from a large-scale unlabeled dataset also exposes the model to
  the risk of potential poisoning attacks, whereby the adversary aims to perturb the
  model’s training data to trigger malicious behaviors in it. In contrast to previous
  work, only poisoning visual modality, in this work, we take the first step to studying
  poisoning attacks against multimodal models in both visual and linguistic modalities.
  Specially, we focus on answering two questions: (1) Is the linguistic modality also
  vulnerable to poisoning attacks? and (2) Which modality is most vulnerable? To answer
  the two questions, we propose three types of poisoning attacks against multimodal
  models. Extensive evaluations on different datasets and model architectures show
  that all three attacks can achieve significant attack performance while maintaining
  model utility in both visual and linguistic modalities. Furthermore, we observe
  that the poisoning effect differs between different modalities. To mitigate the
  attacks, we propose both pre-training and post-training defenses. We empirically
  show that both defenses can significantly reduce the attack performance while preserving
  the model’s utility. Our code is available at https://github.com/zqypku/mm_poison/.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang23f
month: 0
tex_title: Data Poisoning Attacks Against Multimodal Encoders
firstpage: 39299
lastpage: 39313
page: 39299-39313
order: 39299
cycles: false
bibtex_author: Yang, Ziqing and He, Xinlei and Li, Zheng and Backes, Michael and Humbert,
  Mathias and Berrang, Pascal and Zhang, Yang
author:
- given: Ziqing
  family: Yang
- given: Xinlei
  family: He
- given: Zheng
  family: Li
- given: Michael
  family: Backes
- given: Mathias
  family: Humbert
- given: Pascal
  family: Berrang
- given: Yang
  family: Zhang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/yang23f/yang23f.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
