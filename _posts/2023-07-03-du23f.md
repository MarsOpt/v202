---
title: Guiding Pretraining in Reinforcement Learning with Large Language Models
openreview: 63704LH4v5
abstract: Reinforcement learning algorithms typically struggle in the absence of a
  dense, well-shaped reward function. Intrinsically motivated exploration methods
  address this limitation by rewarding agents for visiting novel states or transitions,
  but these methods offer limited benefits in large environments where most discovered
  novelty is irrelevant for downstream tasks. We describe a method that uses background
  knowledge from text corpora to shape exploration. This method, called ELLM (Exploring
  with LLMs) rewards an agent for achieving goals suggested by a language model prompted
  with a description of the agent’s current state. By leveraging large-scale language
  model pretraining, ELLM guides agents toward human-meaningful and plausibly useful
  behaviors without requiring a human in the loop. We evaluate ELLM in the Crafter
  game environment and the Housekeep robotic simulator, showing that ELLM-trained
  agents have better coverage of common-sense behaviors during pretraining and usually
  match or improve performance on a range of downstream tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: du23f
month: 0
tex_title: Guiding Pretraining in Reinforcement Learning with Large Language Models
firstpage: 8657
lastpage: 8677
page: 8657-8677
order: 8657
cycles: false
bibtex_author: Du, Yuqing and Watkins, Olivia and Wang, Zihan and Colas, C\'{e}dric
  and Darrell, Trevor and Abbeel, Pieter and Gupta, Abhishek and Andreas, Jacob
author:
- given: Yuqing
  family: Du
- given: Olivia
  family: Watkins
- given: Zihan
  family: Wang
- given: Cédric
  family: Colas
- given: Trevor
  family: Darrell
- given: Pieter
  family: Abbeel
- given: Abhishek
  family: Gupta
- given: Jacob
  family: Andreas
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/du23f/du23f.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
