---
title: On the Training Instability of Shuffling SGD with Batch Normalization
openreview: sEP5oUajXh
abstract: 'We uncover how SGD interacts with batch normalization and can exhibit undesirable
  training dynamics such as divergence. More precisely, we study how Single Shuffle
  (SS) and Random Reshuffle (RR)—two widely used variants of SGD—interact surprisingly
  differently in the presence of batch normalization: RR leads to much more stable
  evolution of training loss than SS. As a concrete example, for regression using
  a linear network with batch normalized inputs, we prove that SS and RR converge
  to distinct global optima that are “distorted” away from gradient descent. Thereafter,
  for classification we characterize conditions under which training divergence for
  SS and RR can, and cannot occur. We present explicit constructions to show how SS
  leads to distorted optima in regression and divergence for classification, whereas
  RR avoids both distortion and divergence. We validate our results empirically in
  realistic settings, and conclude that the separation between SS and RR used with
  batch normalization is relevant in practice.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu23x
month: 0
tex_title: On the Training Instability of Shuffling {SGD} with Batch Normalization
firstpage: 37787
lastpage: 37845
page: 37787-37845
order: 37787
cycles: false
bibtex_author: Wu, David Xing and Yun, Chulhee and Sra, Suvrit
author:
- given: David Xing
  family: Wu
- given: Chulhee
  family: Yun
- given: Suvrit
  family: Sra
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wu23x/wu23x.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
