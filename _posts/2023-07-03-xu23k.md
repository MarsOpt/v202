---
title: Quantifying the Variability Collapse of Neural Networks
openreview: PSeePcY7WR
abstract: Recent studies empirically demonstrate the positive relationship between
  the transferability of neural networks and the in-class variation of the last layer
  features. The recently discovered Neural Collapse (NC) phenomenon provides a new
  perspective of understanding such last layer geometry of neural networks. In this
  paper, we propose a novel metric, named Variability Collapse Index (VCI), to quantify
  the variability collapse phenomenon in the NC paradigm. The VCI metric is well-motivated
  and intrinsically related to the linear probing loss on the last layer features.
  Moreover, it enjoys desired theoretical and empirical properties, including invariance
  under invertible linear transformations and numerical stability, that distinguishes
  it from previous metrics. Our experiments verify that VCI is indicative of the variability
  collapse and the transferability of pretrained neural networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu23k
month: 0
tex_title: Quantifying the Variability Collapse of Neural Networks
firstpage: 38535
lastpage: 38550
page: 38535-38550
order: 38535
cycles: false
bibtex_author: Xu, Jing and Liu, Haoxiong
author:
- given: Jing
  family: Xu
- given: Haoxiong
  family: Liu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/xu23k/xu23k.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
