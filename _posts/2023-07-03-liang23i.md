---
title: Conformal Inference is (almost) Free for Neural Networks Trained with Early
  Stopping
openreview: 65B1EleIre
abstract: 'Early stopping based on hold-out data is a popular regularization technique
  designed to mitigate overfitting and increase the predictive accuracy of neural
  networks. Models trained with early stopping often provide relatively accurate predictions,
  but they generally still lack precise statistical guarantees unless they are further
  calibrated using independent hold-out data. This paper addresses the above limitation
  with conformalized early stopping: a novel method that combines early stopping with
  conformal calibration while efficiently recycling the same hold-out data. This leads
  to models that are both accurate and able to provide exact predictive inferences
  without multiple data splits nor overly conservative adjustments. Practical implementations
  are developed for different learning tasks—outlier detection, multi-class classification,
  regression—and their competitive performance is demonstrated on real data.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liang23i
month: 0
tex_title: Conformal Inference is (almost) Free for Neural Networks Trained with Early
  Stopping
firstpage: 20810
lastpage: 20851
page: 20810-20851
order: 20810
cycles: false
bibtex_author: Liang, Ziyi and Zhou, Yanfei and Sesia, Matteo
author:
- given: Ziyi
  family: Liang
- given: Yanfei
  family: Zhou
- given: Matteo
  family: Sesia
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/liang23i/liang23i.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
