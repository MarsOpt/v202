---
title: Can Large Language Models Reason about Program Invariants?
openreview: mXv2aVqUGG
abstract: Identifying invariants is an important program analysis task with applications
  towards program understanding, bug finding, vulnerability analysis, and formal verification.
  Existing tools for identifying program invariants rely on dynamic analysis, requiring
  traces collected from multiple executions in order to produce reliable invariants.
  We study the application of large language models to invariant prediction, finding
  that models trained on source code and fine-tuned for invariant generation can perform
  invariant prediction as static rather than dynamic analysis. Using a scratchpad
  approach where invariants are predicted sequentially through a program gives the
  best performance, finding invariants statically of quality comparable to those obtained
  by a dynamic analysis tool with access to five program traces.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pei23a
month: 0
tex_title: Can Large Language Models Reason about Program Invariants?
firstpage: 27496
lastpage: 27520
page: 27496-27520
order: 27496
cycles: false
bibtex_author: Pei, Kexin and Bieber, David and Shi, Kensen and Sutton, Charles and
  Yin, Pengcheng
author:
- given: Kexin
  family: Pei
- given: David
  family: Bieber
- given: Kensen
  family: Shi
- given: Charles
  family: Sutton
- given: Pengcheng
  family: Yin
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/pei23a/pei23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
