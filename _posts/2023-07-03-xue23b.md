---
title: A Study on Transformer Configuration and Training Objective
openreview: qaWSjkLPuw
abstract: Transformer-based models have delivered impressive results on many tasks,
  particularly vision and language tasks. In many model training situations, conventional
  configurations are often adopted. For example, we usually set the base model with
  hidden size (i.e. model width) to be 768 and the number of transformer layers (i.e.
  model depth) to be 12. In this paper, we revisit these conventional configurations
  by studying the the relationship between transformer configuration and training
  objective. We show that the optimal transformer configuration is closely related
  to the training objective. Specifically, compared with the simple classification
  objective, the masked autoencoder is effective in alleviating the over-smoothing
  issue in deep transformer training. Based on this finding, we propose “Bamboo”,
  a notion of using deeper and narrower transformer configurations, for masked autoencoder
  training. On ImageNet, with such a simple change in configuration, the re-designed
  Base-level transformer achieves 84.2% top-1 accuracy and outperforms SoTA models
  like MAE by $0.9%$. On language tasks, re-designed model outperforms BERT with the
  default setting by 1.1 points on average, on GLUE benchmark with 8 datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xue23b
month: 0
tex_title: A Study on Transformer Configuration and Training Objective
firstpage: 38913
lastpage: 38925
page: 38913-38925
order: 38913
cycles: false
bibtex_author: Xue, Fuzhao and Chen, Jianghai and Sun, Aixin and Ren, Xiaozhe and
  Zheng, Zangwei and He, Xiaoxin and Chen, Yongming and Jiang, Xin and You, Yang
author:
- given: Fuzhao
  family: Xue
- given: Jianghai
  family: Chen
- given: Aixin
  family: Sun
- given: Xiaozhe
  family: Ren
- given: Zangwei
  family: Zheng
- given: Xiaoxin
  family: He
- given: Yongming
  family: Chen
- given: Xin
  family: Jiang
- given: Yang
  family: You
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/xue23b/xue23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
