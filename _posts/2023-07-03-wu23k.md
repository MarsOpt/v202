---
title: 'Understanding Int4 Quantization for Language Models: Latency Speedup, Composability,
  and Failure Cases'
openreview: q1WGm3hItW
abstract: Improving the deployment efficiency of transformer-based language models
  has been challenging given their high computation and memory cost. While INT8 quantization
  has recently been shown to be effective in reducing both the memory cost and latency
  while preserving model accuracy, it remains unclear whether we can leverage INT4
  (which doubles peak hardware throughput) to achieve further latency improvement.
  In this study, we explore the feasibility of employing INT4 weight and activation
  (W4A4) quantization for language models. Our findings indicate that W4A4 quantization
  introduces no to negligible accuracy degradation for encoder-only and encoder-decoder
  models, but causes a significant accuracy drop for decoder-only models. To materialize
  the performance gain using W4A4, we develop a highly-optimized end-to-end W4A4 encoder
  inference pipeline supporting different quantization strategies. Our INT4 pipeline
  is $8.5\times$ faster for latency-oriented scenarios and up to $3\times$ for throughput-oriented
  scenarios compared to the inference of FP16, and improves the SOTA BERT INT8 performance
  from FasterTransformer by up to $1.7\times$. We provide insights into the failure
  cases when applying W4A4 to decoder-only models, and further explore the compatibility
  of INT4 quantization with other compression methods, like pruning and layer reduction.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu23k
month: 0
tex_title: 'Understanding Int4 Quantization for Language Models: Latency Speedup,
  Composability, and Failure Cases'
firstpage: 37524
lastpage: 37539
page: 37524-37539
order: 37524
cycles: false
bibtex_author: Wu, Xiaoxia and Li, Cheng and Yazdani Aminabadi, Reza and Yao, Zhewei
  and He, Yuxiong
author:
- given: Xiaoxia
  family: Wu
- given: Cheng
  family: Li
- given: Reza
  family: Yazdani Aminabadi
- given: Zhewei
  family: Yao
- given: Yuxiong
  family: He
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wu23k/wu23k.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
