---
title: 'Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate
  Communication'
openreview: Ll1oayYdUC
abstract: 'Graphs are omnipresent and GNNs are a powerful family of neural networks
  for learning over graphs. Despite their popularity, scaling GNNs either by deepening
  or widening suffers from prevalent issues of $\textit{unhealthy gradients, over-smoothening,
  information squashing}$, which often lead to sub-standard performance. In this work,
  we are interested in exploring a principled way to scale GNNs capacity without deepening
  or widening, which can improve its performance across multiple small and large graphs.
  Motivated by the recent intriguing phenomenon of model soups, which suggest that
  fine-tuned weights of multiple large-language pre-trained models can be merged to
  a better minima, we argue to exploit the fundamentals of model soups to mitigate
  the aforementioned issues of memory bottleneck and trainability during GNNs scaling.
  More specifically, we propose not to deepen or widen current GNNs, but instead present
  $\textbf{first data-centric perspective}$ of model soups to build powerful GNNs
  by dividing giant graph data to build independently and parallelly trained multiple
  comparatively weaker GNNs without any intermediate communication, and $\textit{combining
  their strength}$ using a greedy interpolation soup procedure to achieve state-of-the-art
  performance. Moreover, we provide a wide variety of model soup preparation techniques
  by leveraging state-of-the-art graph sampling and graph partitioning approaches
  that can handle large graph data structures. Our extensive experiments across many
  real-world small and large graphs, illustrate the effectiveness of our approach
  and point towards a promising orthogonal direction for GNN scaling. Codes are available
  at: https://github.com/VITA-Group/graph_ladling'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jaiswal23a
month: 0
tex_title: 'Graph Ladling: Shockingly Simple Parallel {GNN} Training without Intermediate
  Communication'
firstpage: 14679
lastpage: 14690
page: 14679-14690
order: 14679
cycles: false
bibtex_author: Jaiswal, Ajay Kumar and Liu, Shiwei and Chen, Tianlong and Ding, Ying
  and Wang, Zhangyang
author:
- given: Ajay Kumar
  family: Jaiswal
- given: Shiwei
  family: Liu
- given: Tianlong
  family: Chen
- given: Ying
  family: Ding
- given: Zhangyang
  family: Wang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/jaiswal23a/jaiswal23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
