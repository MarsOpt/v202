---
title: 'The case for 4-bit precision: k-bit Inference Scaling Laws'
openreview: i8tGb1ab1j
abstract: Quantization methods reduce the number of bits required to represent each
  parameter in a model, trading accuracy for smaller memory footprints and inference
  latencies. However, the final model size depends on both the number of parameters
  of the original model and the rate of compression. For example, a 30B 8-bit model
  and a 60B 4-bit model have the same number of bits but may have very different zero-shot
  accuracies. In this work, we study this trade-off by developing inference scaling
  laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision
  and model size that maximizes zero-shot performance. We run more than 35,000 experiments
  with 16-bit inputs and k-bit parameters to examine which zero-shot quantization
  methods improve scaling for 3 to 8-bit precision at scales of 19M to 176B parameters
  across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging
  to improve the bit-level scaling trade-off, with the only improvements being the
  use of a small block size – splitting the parameters into small independently quantized
  blocks – and the quantization data type being used (e.g., Int vs Float). Overall,
  our findings show that 4-bit precision is almost universally optimal for total model
  bits and zero-shot accuracy.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dettmers23a
month: 0
tex_title: 'The case for 4-bit precision: k-bit Inference Scaling Laws'
firstpage: 7750
lastpage: 7774
page: 7750-7774
order: 7750
cycles: false
bibtex_author: Dettmers, Tim and Zettlemoyer, Luke
author:
- given: Tim
  family: Dettmers
- given: Luke
  family: Zettlemoyer
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/dettmers23a/dettmers23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
