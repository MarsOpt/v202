---
title: Differentially Private Sharpness-Aware Training
openreview: zzPMd0Ue4i
abstract: Training deep learning models with differential privacy (DP) results in
  a degradation of performance. The training dynamics of models with DP show a significant
  difference from standard training, whereas understanding the geometric properties
  of private learning remains largely unexplored. In this paper, we investigate sharpness,
  a key factor in achieving better generalization, in private learning. We show that
  flat minima can help reduce the negative effects of per-example gradient clipping
  and the addition of Gaussian noise. We then verify the effectiveness of Sharpness-Aware
  Minimization (SAM) for seeking flat minima in private learning. However, we also
  discover that SAM is detrimental to the privacy budget and computational time due
  to its two-step optimization. Thus, we propose a new sharpness-aware training method
  that mitigates the privacy-optimization trade-off. Our experimental results demonstrate
  that the proposed method improves the performance of deep learning models with DP
  from both scratch and fine-tuning. Code is available at https://github.com/jinseongP/DPSAT.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: park23g
month: 0
tex_title: Differentially Private Sharpness-Aware Training
firstpage: 27204
lastpage: 27224
page: 27204-27224
order: 27204
cycles: false
bibtex_author: Park, Jinseong and Kim, Hoki and Choi, Yujin and Lee, Jaewook
author:
- given: Jinseong
  family: Park
- given: Hoki
  family: Kim
- given: Yujin
  family: Choi
- given: Jaewook
  family: Lee
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/park23g/park23g.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
