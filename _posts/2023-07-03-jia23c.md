---
title: Smooth Non-stationary Bandits
openreview: JeY4dqt0J9
abstract: In many applications of online decision making, the environment is non-stationary
  and it is therefore crucial to use bandit algorithms that handle changes. Most existing
  approaches are designed to protect against non-smooth changes, constrained only
  by total variation or Lipschitzness over time, where they guarantee $T^{2/3}$ regret.
  However, in practice environments are often changing <em>smoothly</em>, so such
  algorithms may incur higher-than-necessary regret in these settings and do not leverage
  information on the <em>rate of change</em>. In this paper, we study a non-stationary
  two-arm bandit problem where we assume an arm’s mean reward is a $\beta$-Hölder
  function over (normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuously
  differentiable. We show the first <em>separation</em> between the smooth and non-smooth
  regimes by presenting a policy with $T^{3/5}$ regret for $\beta=2$. We complement
  this result by a $T^{\frac{\beta+1}{2\beta+1}}$ lower bound for any integer $\beta\ge
  1$, which matches our upper bound for $\beta=2$.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jia23c
month: 0
tex_title: Smooth Non-stationary Bandits
firstpage: 14930
lastpage: 14944
page: 14930-14944
order: 14930
cycles: false
bibtex_author: Jia, Su and Xie, Qian and Kallus, Nathan and Frazier, Peter I.
author:
- given: Su
  family: Jia
- given: Qian
  family: Xie
- given: Nathan
  family: Kallus
- given: Peter I.
  family: Frazier
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/jia23c/jia23c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
