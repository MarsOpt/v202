---
title: Random Shuffle Transformer for Image Restoration
openreview: 7RIjvZfceF
abstract: 'Non-local interactions play a vital role in boosting performance for image
  restoration. However, local window Transformer has been preferred due to its efficiency
  for processing high-resolution images. The superiority in efficiency comes at the
  cost of sacrificing the ability to model non-local interactions. In this paper,
  we present that local window Transformer can also function as modeling non-local
  interactions. The counterintuitive function is based on the permutation-equivariance
  of self-attention. The basic principle is quite simple: by <em>randomly shuffling</em>
  the input, local self-attention also has the potential to model non-local interactions
  without introducing extra parameters. Our random shuffle strategy enjoys elegant
  theoretical guarantees in extending the local scope. The resulting Transformer dubbed
  <em>ShuffleFormer</em> is capable of processing high-resolution images efficiently
  while modeling non-local interactions. Extensive experiments demonstrate the effectiveness
  of ShuffleFormer across a variety of image restoration tasks, including image denoising,
  deraining, and deblurring. Code is available at https://github.com/jiexiaou/ShuffleFormer.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xiao23a
month: 0
tex_title: Random Shuffle Transformer for Image Restoration
firstpage: 38039
lastpage: 38058
page: 38039-38058
order: 38039
cycles: false
bibtex_author: Xiao, Jie and Fu, Xueyang and Zhou, Man and Liu, Hongjian and Zha,
  Zheng-Jun
author:
- given: Jie
  family: Xiao
- given: Xueyang
  family: Fu
- given: Man
  family: Zhou
- given: Hongjian
  family: Liu
- given: Zheng-Jun
  family: Zha
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/xiao23a/xiao23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
