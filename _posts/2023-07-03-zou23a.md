---
title: The Benefits of Mixup for Feature Learning
openreview: hgc9pjjDOo
abstract: Mixup, a simple data augmentation method that randomly mixes two data points
  via linear interpolation, has been extensively applied in various deep learning
  applications to gain better generalization. However, its theoretical explanation
  remains largely unclear. In this work, we aim to seek a fundamental understanding
  of the benefits of Mixup. We first show that Mixup using different linear interpolation
  parameters for features and labels can still achieve similar performance as standard
  Mixup. This indicates that the intuitive linearity explanation in Zhang et al.,
  (2018) may not fully explain the success of Mixup. Then, we perform a theoretical
  study of Mixup from the feature learning perspective. We consider a feature-noise
  data model and show that Mixup training can effectively learn the rare features
  (appearing in a small fraction of data) from its mixture with the common features
  (appearing in a large fraction of data). In contrast, standard training can only
  learn the common features but fails to learn the rare features, thus suffering from
  bad generalization performance. Moreover, our theoretical analysis also shows that
  the benefits of Mixup for feature learning are mostly gained in the early training
  phase, based on which we propose to apply early stopping in Mixup. Experimental
  results verify our theoretical findings and demonstrate the effectiveness of the
  early-stopped Mixup training.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zou23a
month: 0
tex_title: The Benefits of Mixup for Feature Learning
firstpage: 43423
lastpage: 43479
page: 43423-43479
order: 43423
cycles: false
bibtex_author: Zou, Difan and Cao, Yuan and Li, Yuanzhi and Gu, Quanquan
author:
- given: Difan
  family: Zou
- given: Yuan
  family: Cao
- given: Yuanzhi
  family: Li
- given: Quanquan
  family: Gu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zou23a/zou23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
