---
title: 'Hyena Hierarchy: Towards Larger Convolutional Language Models'
openreview: 1sxiBaGEtg
abstract: Recent advances in deep learning have relied heavily on the use of large
  Transformers due to their ability to learn at scale. However, the core building
  block of Transformers, the attention operator, exhibits quadratic cost in sequence
  length, limiting the amount of context accessible. Existing subquadratic methods
  based on low-rank and sparse approximations need to be combined with dense attention
  layers to match Transformers at scale, indicating a gap in capability. In this work,
  we propose Hyena, a subquadratic drop-in replacement for attention constructed by
  interleaving implicitly parametrized long convolutions and data-controlled gating.
  In challenging reasoning tasks on sequences of thousands to hundreds of thousands
  of tokens, Hyena improves accuracy by more than 50 points over operators relying
  on state-space models, transfer functions, and other implicit and explicit methods,
  matching attention-based models. We set a new state-of-the-art for dense-attention-free
  architectures on language modeling in standard datasets WikiText103 and The Pile,
  reaching Transformer quality with a 20% reduction in training compute required at
  sequence length 2k. Hyena operators are 2x faster than highly optimized attention
  at sequence length 8k, with speedups of 100x at 64k.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: poli23a
month: 0
tex_title: 'Hyena Hierarchy: Towards Larger Convolutional Language Models'
firstpage: 28043
lastpage: 28078
page: 28043-28078
order: 28043
cycles: false
bibtex_author: Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel
  Y and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and Re,
  Christopher
author:
- given: Michael
  family: Poli
- given: Stefano
  family: Massaroli
- given: Eric
  family: Nguyen
- given: Daniel Y
  family: Fu
- given: Tri
  family: Dao
- given: Stephen
  family: Baccus
- given: Yoshua
  family: Bengio
- given: Stefano
  family: Ermon
- given: Christopher
  family: Re
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/poli23a/poli23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
