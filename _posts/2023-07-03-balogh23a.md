---
title: On the Functional Similarity of Robust and Non-Robust Neural Representations
openreview: sFqfXphJh5
abstract: Model stitching—where the internal representations of two neural networks
  are aligned linearly—helped demonstrate that the representations of different neural
  networks for the same task are surprisingly similar in a functional sense. At the
  same time, the representations of adversarially robust networks are considered to
  be different from non-robust representations. For example, robust image classifiers
  are invertible, while non-robust networks are not. Here, we investigate the functional
  similarity of robust and non-robust representations for image classification with
  the help of model stitching. We find that robust and non-robust networks indeed
  have different representations. However, these representations are compatible regarding
  accuracy. From the point of view of robust accuracy, compatibility decreases quickly
  after the first few layers but the representations become compatible again in the
  last layers, in the sense that the properties of the front model can be recovered.
  Moreover, this is true even in the case of cross-task stitching. Our results suggest
  that stitching in the initial, preprocessing layers and the final, abstract layers
  test different kinds of compatibilities. In particular, the final layers are easy
  to match, because their representations depend mostly on the same abstract task
  specification, in our case, the classification of the input into $n$ classes.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: balogh23a
month: 0
tex_title: On the Functional Similarity of Robust and Non-Robust Neural Representations
firstpage: 1614
lastpage: 1635
page: 1614-1635
order: 1614
cycles: false
bibtex_author: Balogh, Andr\'{a}s and Jelasity, M\'{a}rk
author:
- given: András
  family: Balogh
- given: Márk
  family: Jelasity
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/balogh23a/balogh23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
