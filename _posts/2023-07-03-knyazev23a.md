---
title: Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?
openreview: 7UXf8dAz5T
abstract: Pretraining a neural network on a large dataset is becoming a cornerstone
  in machine learning that is within the reach of only a few communities with large-resources.
  We aim at an ambitious goal of democratizing pretraining. Towards that goal, we
  train and release a single neural network that can predict high quality ImageNet
  parameters of other neural networks. By using predicted parameters for initialization
  we are able to boost training of diverse ImageNet models available in PyTorch. When
  transferred to other datasets, models initialized with predicted parameters also
  converge faster and reach competitive final performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: knyazev23a
month: 0
tex_title: Can We Scale Transformers to Predict Parameters of Diverse {I}mage{N}et
  Models?
firstpage: 17243
lastpage: 17259
page: 17243-17259
order: 17243
cycles: false
bibtex_author: Knyazev, Boris and Hwang, Doha and Lacoste-Julien, Simon
author:
- given: Boris
  family: Knyazev
- given: Doha
  family: Hwang
- given: Simon
  family: Lacoste-Julien
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/knyazev23a/knyazev23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
