---
title: Transformer-based Stagewise Decomposition for Large-Scale Multistage Stochastic
  Optimization
openreview: lKoEeUpkVm
abstract: Solving large-scale multistage stochastic programming (MSP) problems poses
  a significant challenge as commonly used stagewise decomposition algorithms, including
  stochastic dual dynamic programming (SDDP), face growing time complexity as the
  subproblem size and problem count increase. Traditional approaches approximate the
  value functions as piecewise linear convex functions by incrementally accumulating
  subgradient cutting planes from the primal and dual solutions of stagewise subproblems.
  Recognizing these limitations, we introduce TranSDDP, a novel Transformer-based
  stagewise decomposition algorithm. This innovative approach leverages the structural
  advantages of the Transformer model, implementing a sequential method for integrating
  subgradient cutting planes to approximate the value function. Through our numerical
  experiments, we affirm TranSDDPâ€™s effectiveness in addressing MSP problems. It efficiently
  generates a piecewise linear approximation for the value function, significantly
  reducing computation time while preserving solution quality, thus marking a promising
  progression in the treatment of large-scale multistage stochastic programming problems.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim23r
month: 0
tex_title: Transformer-based Stagewise Decomposition for Large-Scale Multistage Stochastic
  Optimization
firstpage: 16747
lastpage: 16770
page: 16747-16770
order: 16747
cycles: false
bibtex_author: Kim, Chanyeong and Park, Jongwoong and Bae, Hyunglip and Kim, Woo Chang
author:
- given: Chanyeong
  family: Kim
- given: Jongwoong
  family: Park
- given: Hyunglip
  family: Bae
- given: Woo Chang
  family: Kim
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kim23r/kim23r.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
