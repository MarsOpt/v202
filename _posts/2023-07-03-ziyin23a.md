---
title: 'spred: Solving L1 Penalty with SGD'
openreview: 880tEHqxzg
abstract: We propose to minimize a generic differentiable objective with $L_1$ constraint
  using a simple reparametrization and straightforward stochastic gradient descent.
  Our proposal is the direct generalization of previous ideas that the $L_1$ penalty
  may be equivalent to a differentiable reparametrization with weight decay. We prove
  that the proposed method, spred, is an exact differentiable solver of $L_1$ and
  that the reparametrization trick is completely â€œbenign" for a generic nonconvex
  function. Practically, we demonstrate the usefulness of the method in (1) training
  sparse neural networks to perform gene selection tasks, which involves finding relevant
  features in a very high dimensional space, and (2) neural network compression task,
  to which previous attempts at applying the $L_1$-penalty have been unsuccessful.
  Conceptually, our result bridges the gap between the sparsity in deep learning and
  conventional statistical learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ziyin23a
month: 0
tex_title: 'spred: Solving L1 Penalty with {SGD}'
firstpage: 43407
lastpage: 43422
page: 43407-43422
order: 43407
cycles: false
bibtex_author: Ziyin, Liu and Wang, Zihao
author:
- given: Liu
  family: Ziyin
- given: Zihao
  family: Wang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/ziyin23a/ziyin23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
