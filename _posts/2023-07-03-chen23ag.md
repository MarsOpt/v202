---
title: 'BEATs: Audio Pre-Training with Acoustic Tokenizers'
openreview: Fj0PRtd4e6
abstract: We introduce a self-supervised learning (SSL) framework BEATs for general
  audio representation pre-training, where we optimize an acoustic tokenizer and an
  audio SSL model by iterations. Unlike the previous audio SSL models that employ
  reconstruction loss for pre-training, our audio SSL model is trained with the discrete
  label prediction task, where the labels are generated by a semantic-rich acoustic
  tokenizer. We propose an iterative pipeline to jointly optimize the tokenizer and
  the pre-trained model, aiming to abstract high-level semantics and discard the redundant
  details for audio. The experimental results demonstrate our acoustic tokenizers
  can generate discrete labels with rich audio semantics and our audio SSL models
  achieve state-of-the-art (SOTA) results across various audio classification benchmarks,
  even outperforming previous models that use more training data and model parameters
  significantly. Specifically, we set a new SOTA mAP 50.6% on AudioSet-2M without
  using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained
  models are available at https://aka.ms/beats.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen23ag
month: 0
tex_title: "{BEAT}s: Audio Pre-Training with Acoustic Tokenizers"
firstpage: 5178
lastpage: 5193
page: 5178-5193
order: 5178
cycles: false
bibtex_author: Chen, Sanyuan and Wu, Yu and Wang, Chengyi and Liu, Shujie and Tompkins,
  Daniel and Chen, Zhuo and Che, Wanxiang and Yu, Xiangzhan and Wei, Furu
author:
- given: Sanyuan
  family: Chen
- given: Yu
  family: Wu
- given: Chengyi
  family: Wang
- given: Shujie
  family: Liu
- given: Daniel
  family: Tompkins
- given: Zhuo
  family: Chen
- given: Wanxiang
  family: Che
- given: Xiangzhan
  family: Yu
- given: Furu
  family: Wei
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/chen23ag/chen23ag.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
