---
title: 'Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time'
openreview: wIPIhHd00i
abstract: Large language models (LLMs) with hundreds of billions of parameters have
  sparked a new wave of exciting AI applications. However, they are computationally
  expensive at inference time. Sparsity is a natural approach to reduce this cost,
  but existing methods either require costly retraining, have to forgo LLM’s in-context
  learning ability, or do not yield wall-clock time speedup on modern hardware. We
  hypothesize that contextual sparsity, which are small, input-dependent sets of attention
  heads and MLP parameters that yield approximately the same output as the dense model
  for a given input, can address these issues. We show that contextual sparsity exists,
  that it can be accurately predicted, and that we can exploit it to speed up LLM
  inference in wall-clock time without compromising LLM’s quality or in-context learning
  ability. Based on these insights, we propose DejaVu, a system that uses a low-cost
  algorithm to predict contextual sparsity on the fly given inputs to each layer,
  along with an asynchronous and hardware-aware implementation that speeds up LLM
  inference. We validate that DejaVu can reduce the inference latency of OPT-175B
  by over 2$\times$ compared to the state-of-the-art FasterTransformer, and over 6$\times$
  compared to the widely used Hugging Face implementation, without compromising model
  quality. The code is available at https://github.com/FMInference/DejaVu.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu23am
month: 0
tex_title: 'Deja Vu: Contextual Sparsity for Efficient {LLM}s at Inference Time'
firstpage: 22137
lastpage: 22176
page: 22137-22176
order: 22137
cycles: false
bibtex_author: Liu, Zichang and Wang, Jue and Dao, Tri and Zhou, Tianyi and Yuan,
  Binhang and Song, Zhao and Shrivastava, Anshumali and Zhang, Ce and Tian, Yuandong
  and Re, Christopher and Chen, Beidi
author:
- given: Zichang
  family: Liu
- given: Jue
  family: Wang
- given: Tri
  family: Dao
- given: Tianyi
  family: Zhou
- given: Binhang
  family: Yuan
- given: Zhao
  family: Song
- given: Anshumali
  family: Shrivastava
- given: Ce
  family: Zhang
- given: Yuandong
  family: Tian
- given: Christopher
  family: Re
- given: Beidi
  family: Chen
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/liu23am/liu23am.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
