---
title: A Fast Optimistic Method for Monotone Variational Inequalities
openreview: 2djCNo5yXQ
abstract: We study monotone variational inequalities that can arise as optimality
  conditions for constrained convex optimization or convex-concave minimax problems
  and propose a novel algorithm that uses only one gradient/operator evaluation and
  one projection onto the constraint set per iteration. The algorithm, which we call
  fOGDA-VI, achieves a $o(\frac{1}{k})$ rate of convergence in terms of the restricted
  gap function as well as the natural residual for the last iterate. Moreover, we
  provide a convergence guarantee for the sequence of iterates to a solution of the
  variational inequality. These are the best theoretical convergence results for numerical
  methods for (only) monotone variational inequalities reported in the literature.
  To empirically validate our algorithm we investigate a two-player matrix game with
  mixed strategies of the two players. Concluding, we show promising results regarding
  the application of fOGDA-VI to the training of generative adversarial nets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sedlmayer23a
month: 0
tex_title: A Fast Optimistic Method for Monotone Variational Inequalities
firstpage: 30406
lastpage: 30438
page: 30406-30438
order: 30406
cycles: false
bibtex_author: Sedlmayer, Michael and Nguyen, Dang-Khoa and Bot, Radu Ioan
author:
- given: Michael
  family: Sedlmayer
- given: Dang-Khoa
  family: Nguyen
- given: Radu Ioan
  family: Bot
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/sedlmayer23a/sedlmayer23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
