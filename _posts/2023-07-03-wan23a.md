---
title: 'UPSCALE: Unconstrained Channel Pruning'
openreview: 25fe54GXLo
abstract: 'As neural networks grow in size and complexity, inference speeds decline.
  To combat this, one of the most effective compression techniques – channel pruning
  – removes channels from weights. However, for multi-branch segments of a model,
  channel removal can introduce inference-time memory copies. In turn, these copies
  increase inference latency – so much so that the pruned model can be slower than
  the unpruned model. As a workaround, pruners conventionally constrain certain channels
  to be pruned together. This fully eliminates memory copies but, as we show, significantly
  impairs accuracy. We now have a dilemma: Remove constraints but increase latency,
  or add constraints and impair accuracy. In response, our insight is to reorder channels
  at export time, (1) reducing latency by reducing memory copies and (2) improving
  accuracy by removing constraints. Using this insight, we design a generic algorithm
  UPSCALE to prune models with any pruning pattern. By removing constraints from existing
  pruners, we improve ImageNet accuracy for post-training pruned models by 2.1 points
  on average – benefiting DenseNet (+16.9), EfficientNetV2 (+7.9), and ResNet (+6.2).
  Furthermore, by reordering channels, UPSCALE improves inference speeds by up to
  2x over a baseline export.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wan23a
month: 0
tex_title: "{UPSCALE}: Unconstrained Channel Pruning"
firstpage: 35384
lastpage: 35412
page: 35384-35412
order: 35384
cycles: false
bibtex_author: Wan, Alvin and Hao, Hanxiang and Patnaik, Kaushik and Xu, Yueyang and
  Hadad, Omer and G\"{u}era, David and Ren, Zhile and Shan, Qi
author:
- given: Alvin
  family: Wan
- given: Hanxiang
  family: Hao
- given: Kaushik
  family: Patnaik
- given: Yueyang
  family: Xu
- given: Omer
  family: Hadad
- given: David
  family: Güera
- given: Zhile
  family: Ren
- given: Qi
  family: Shan
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wan23a/wan23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
