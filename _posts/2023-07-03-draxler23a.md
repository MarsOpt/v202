---
title: On the Convergence Rate of Gaussianization with Random Rotations
openreview: xqYFvRanEW
abstract: Gaussianization is a simple generative model that can be trained without
  backpropagation. It has shown compelling performance on low dimensional data. As
  the dimension increases, however, it has been observed that the convergence speed
  slows down. We show analytically that the number of required layers scales linearly
  with the dimension for Gaussian input. We argue that this is because the model is
  unable to capture dependencies between dimensions. Empirically, we find the same
  linear increase in cost for arbitrary input $p(x)$, but observe favorable scaling
  for some distributions. We explore potential speed-ups and formulate challenges
  for further research.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: draxler23a
month: 0
tex_title: On the Convergence Rate of Gaussianization with Random Rotations
firstpage: 8449
lastpage: 8468
page: 8449-8468
order: 8449
cycles: false
bibtex_author: Draxler, Felix and K\"{u}hmichel, Lars and Rousselot, Armand and M\"{u}ller,
  Jens and Schnoerr, Christoph and Koethe, Ullrich
author:
- given: Felix
  family: Draxler
- given: Lars
  family: Kühmichel
- given: Armand
  family: Rousselot
- given: Jens
  family: Müller
- given: Christoph
  family: Schnoerr
- given: Ullrich
  family: Koethe
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/draxler23a/draxler23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
