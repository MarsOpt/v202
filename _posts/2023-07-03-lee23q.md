---
title: Implicit Jacobian regularization weighted with impurity of probability output
openreview: VPiB4Eq4Sx
abstract: The success of deep learning is greatly attributed to stochastic gradient
  descent (SGD), yet it remains unclear how SGD finds well-generalized models. We
  demonstrate that SGD has an implicit regularization effect on the logit-weight Jacobian
  norm of neural networks. This regularization effect is weighted with the <em>impurity</em>
  of the probability output, and thus it is active in a certain phase of training.
  Moreover, based on these findings, we propose a novel optimization method that explicitly
  regularizes the Jacobian norm, which leads to similar performance as other state-of-the-art
  sharpness-aware optimization methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lee23q
month: 0
tex_title: Implicit {J}acobian regularization weighted with impurity of probability
  output
firstpage: 19141
lastpage: 19184
page: 19141-19184
order: 19141
cycles: false
bibtex_author: Lee, Sungyoon and Park, Jinseong and Lee, Jaewook
author:
- given: Sungyoon
  family: Lee
- given: Jinseong
  family: Park
- given: Jaewook
  family: Lee
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/lee23q/lee23q.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
