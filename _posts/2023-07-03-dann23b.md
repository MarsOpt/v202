---
title: Best of Both Worlds Policy Optimization
openreview: bUFUaawOTk
abstract: Policy optimization methods are popular reinforcement learning algorithms
  in practice and recent works have build theoretical foundation for them by proving
  $\sqrt{T}$ regret bounds even when the losses are adversarial. Such bounds are tight
  in the worst case but often overly pessimistic. In this work, we show that by carefully
  designing the regularizer, bonus terms, and learning rates, one can achieve a more
  favorable $\text{polylog}(T)$ regret bound when the losses are stochastic, without
  sacrificing the worst-case guarantee in the adversarial regime. Specifically, we
  show the first best of both worlds guarantee for policy optimization in tabular
  MDPs by leveraging either a Tsallis entropy or a Shannon entropy regularizer. Then
  we show that under known transitions, we can further obtain a first-order regret
  bound in the adversarial regime by leveraging the log barrier regularizer.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dann23b
month: 0
tex_title: Best of Both Worlds Policy Optimization
firstpage: 6968
lastpage: 7008
page: 6968-7008
order: 6968
cycles: false
bibtex_author: Dann, Christoph and Wei, Chen-Yu and Zimmert, Julian
author:
- given: Christoph
  family: Dann
- given: Chen-Yu
  family: Wei
- given: Julian
  family: Zimmert
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/dann23b/dann23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
