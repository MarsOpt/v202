---
title: Expected Gradients of Maxout Networks and Consequences to Parameter Initialization
openreview: mJRMHkaTcp
abstract: We study the gradients of a maxout network with respect to inputs and parameters
  and obtain bounds for the moments depending on the architecture and the parameter
  distribution. We observe that the distribution of the input-output Jacobian depends
  on the input, which complicates a stable parameter initialization. Based on the
  moments of the gradients, we formulate parameter initialization strategies that
  avoid vanishing and exploding gradients in wide networks. Experiments with deep
  fully-connected and convolutional networks show that this strategy improves SGD
  and Adam training of deep maxout networks. In addition, we obtain refined bounds
  on the expected number of linear regions, results on the expected curve length distortion,
  and results on the NTK.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tseran23a
month: 0
tex_title: Expected Gradients of Maxout Networks and Consequences to Parameter Initialization
firstpage: 34491
lastpage: 34532
page: 34491-34532
order: 34491
cycles: false
bibtex_author: Tseran, Hanna and Montufar, Guido
author:
- given: Hanna
  family: Tseran
- given: Guido
  family: Montufar
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/tseran23a/tseran23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
