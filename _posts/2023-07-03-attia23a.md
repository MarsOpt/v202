---
title: 'SGD with AdaGrad Stepsizes: Full Adaptivity with High Probability to Unknown
  Parameters, Unbounded Gradients and Affine Variance'
openreview: X7jMTrwuCz
abstract: 'We study Stochastic Gradient Descent with AdaGrad stepsizes: a popular
  adaptive (self-tuning) method for first-order stochastic optimization. Despite being
  well studied, existing analyses of this method suffer from various shortcomings:
  they either assume some knowledge of the problem parameters, impose strong global
  Lipschitz conditions, or fail to give bounds that hold with high probability. We
  provide a comprehensive analysis of this basic method without any of these limitations,
  in both the convex and non-convex (smooth) cases, that additionally supports a general
  “affine variance” noise model and provides sharp rates of convergence in both the
  low-noise and high-noise regimes.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: attia23a
month: 0
tex_title: "{SGD} with {A}da{G}rad Stepsizes: Full Adaptivity with High Probability
  to Unknown Parameters, Unbounded Gradients and Affine Variance"
firstpage: 1147
lastpage: 1171
page: 1147-1171
order: 1147
cycles: false
bibtex_author: Attia, Amit and Koren, Tomer
author:
- given: Amit
  family: Attia
- given: Tomer
  family: Koren
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/attia23a/attia23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
