---
title: Do Perceptually Aligned Gradients Imply Robustness?
openreview: 9TbDVDX7de
abstract: Adversarially robust classifiers possess a trait that non-robust models
  do not - Perceptually Aligned Gradients (PAG). Their gradients with respect to the
  input align well with human perception. Several works have identified PAG as a byproduct
  of robust training, but none have considered it as a standalone phenomenon nor studied
  its own implications. In this work, we focus on this trait and test whether Perceptually
  Aligned Gradients imply Robustness. To this end, we develop a novel objective to
  directly promote PAG in training classifiers and examine whether models with such
  gradients are more robust to adversarial attacks. Extensive experiments on multiple
  datasets and architectures validate that models with aligned gradients exhibit significant
  robustness, exposing the surprising bidirectional connection between PAG and robustness.
  Lastly, we show that better gradient alignment leads to increased robustness and
  harness this observation to boost the robustness of existing adversarial training
  techniques.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ganz23a
month: 0
tex_title: Do Perceptually Aligned Gradients Imply Robustness?
firstpage: 10628
lastpage: 10648
page: 10628-10648
order: 10628
cycles: false
bibtex_author: Ganz, Roy and Kawar, Bahjat and Elad, Michael
author:
- given: Roy
  family: Ganz
- given: Bahjat
  family: Kawar
- given: Michael
  family: Elad
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/ganz23a/ganz23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
