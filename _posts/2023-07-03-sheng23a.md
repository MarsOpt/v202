---
title: 'FlexGen: High-Throughput Generative Inference of Large Language Models with
  a Single GPU'
openreview: RRntzKrBTp
abstract: The high computational and memory requirements of large language model (LLM)
  inference make it feasible only with multiple high-end accelerators. Motivated by
  the emerging demand for latency-insensitive tasks with batched processing, this
  paper initiates the study of high-throughput LLM inference using limited resources,
  such as a single commodity GPU. We present FlexGen, a high-throughput generation
  engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured
  under various hardware resource constraints by aggregating memory and computation
  from the GPU, CPU, and disk. By solving a linear programming problem, it searches
  for efficient patterns to store and access tensors. FlexGen further compresses the
  weights and the attention cache to 4 bits with negligible accuracy loss. These techniques
  enable FlexGen to have a larger space of batch size choices and thus significantly
  increase maximum throughput. As a result, when running OPT-175B on a single 16GB
  GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art
  offloading systems, reaching a generation throughput of 1 token/s for the first
  time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark
  a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code
  is available at https://github.com/FMInference/FlexGen.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sheng23a
month: 0
tex_title: "{F}lex{G}en: High-Throughput Generative Inference of Large Language Models
  with a Single {GPU}"
firstpage: 31094
lastpage: 31116
page: 31094-31116
order: 31094
cycles: false
bibtex_author: Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and
  Ryabinin, Max and Chen, Beidi and Liang, Percy and Re, Christopher and Stoica, Ion
  and Zhang, Ce
author:
- given: Ying
  family: Sheng
- given: Lianmin
  family: Zheng
- given: Binhang
  family: Yuan
- given: Zhuohan
  family: Li
- given: Max
  family: Ryabinin
- given: Beidi
  family: Chen
- given: Percy
  family: Liang
- given: Christopher
  family: Re
- given: Ion
  family: Stoica
- given: Ce
  family: Zhang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/sheng23a/sheng23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
