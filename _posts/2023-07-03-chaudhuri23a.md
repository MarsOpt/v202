---
title: Why does Throwing Away Data Improve Worst-Group Error?
openreview: b2GYLlhH4a
abstract: When facing data with imbalanced classes or groups, practitioners follow
  an intriguing strategy to achieve best results. They throw away examples until the
  classes or groups are balanced in size, and then perform empirical risk minimization
  on the reduced training set. This opposes common wisdom in learning theory, where
  the expected error is supposed to decrease as the dataset grows in size. In this
  work, we leverage extreme value theory to address this apparent contradiction. Our
  results show that the tails of the data distribution play an important role in determining
  the worst-group-accuracy of linear classifiers. When learning on data with heavy
  tails, throwing away data restores the geometric symmetry of the resulting classifier,
  and therefore improves its worst-group generalization.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chaudhuri23a
month: 0
tex_title: Why does Throwing Away Data Improve Worst-Group Error?
firstpage: 4144
lastpage: 4188
page: 4144-4188
order: 4144
cycles: false
bibtex_author: Chaudhuri, Kamalika and Ahuja, Kartik and Arjovsky, Martin and Lopez-Paz,
  David
author:
- given: Kamalika
  family: Chaudhuri
- given: Kartik
  family: Ahuja
- given: Martin
  family: Arjovsky
- given: David
  family: Lopez-Paz
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/chaudhuri23a/chaudhuri23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
