---
title: Stabilizing Transformer Training by Preventing Attention Entropy Collapse
openreview: LL8gz8FHxH
abstract: Training stability is of great importance to Transformers. In this work,
  we investigate the training dynamics of Transformers by examining the evolution
  of the attention layers. In particular, we track the attention entropy for each
  attention head during the course of training, which is a proxy for model sharpness.
  We identify a common pattern across different architectures and tasks, where low
  attention entropy is accompanied by high training instability, which can take the
  form of oscillating loss or divergence. We denote the pathologically low attention
  entropy, corresponding to highly concentrated attention scores, as $\textit{entropy
  collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution
  where we reparametrize all linear layers with spectral normalization and an additional
  learned scalar. We demonstrate that $\sigma$Reparam successfully prevents entropy
  collapse in the attention layers, promoting more stable training. Additionally,
  we prove a tight lower bound of the attention entropy, which decreases exponentially
  fast with the spectral norm of the attention logits, providing additional motivation
  for our approach. We conduct experiments with $\sigma$Reparam on image classification,
  image self-supervised learning, machine translation, speech recognition, and language
  modeling tasks. We show that $\sigma$Reparam provides stability and robustness with
  respect to the choice of hyperparameters, going so far as enabling training (a)
  a Vision Transformer to competitive performance without warmup, weight decay, layer
  normalization or adaptive optimizers; (b) deep architectures in machine translation
  and (c) speech recognition to competitive performance without warmup and adaptive
  optimizers. Code is available at https://github.com/apple/ml-sigma-reparam.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhai23a
month: 0
tex_title: Stabilizing Transformer Training by Preventing Attention Entropy Collapse
firstpage: 40770
lastpage: 40803
page: 40770-40803
order: 40770
cycles: false
bibtex_author: Zhai, Shuangfei and Likhomanenko, Tatiana and Littwin, Etai and Busbridge,
  Dan and Ramapuram, Jason and Zhang, Yizhe and Gu, Jiatao and Susskind, Joshua M.
author:
- given: Shuangfei
  family: Zhai
- given: Tatiana
  family: Likhomanenko
- given: Etai
  family: Littwin
- given: Dan
  family: Busbridge
- given: Jason
  family: Ramapuram
- given: Yizhe
  family: Zhang
- given: Jiatao
  family: Gu
- given: Joshua M.
  family: Susskind
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhai23a/zhai23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
