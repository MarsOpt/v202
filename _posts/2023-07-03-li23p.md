---
title: 'How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding'
openreview: LMXgU4zrq6
abstract: 'While the successes of transformers across many domains are indisputable,
  accurate understanding of the learning mechanics is still largely lacking. Their
  capabilities have been probed on benchmarks which include a variety of structured
  and reasoning tasks—but mathematical understanding is lagging substantially behind.
  Recent lines of work have begun studying representational aspects of this question:
  that is, the size/depth/complexity of attention-based networks to perform certain
  tasks. However, there is no guarantee the learning dynamics will converge to the
  constructions proposed. In our paper, we provide fine-grained mechanistic understanding
  of how transformers learn “semantic structure”, understood as capturing co-occurrence
  structure of words. Precisely, we show, through a combination of mathematical analysis
  and experiments on Wikipedia data and synthetic data modeled by Latent Dirichlet
  Allocation (LDA), that the embedding layer and the self-attention layer encode the
  topical structure. In the former case, this manifests as higher average inner product
  of embeddings between same-topic words. In the latter, it manifests as higher average
  pairwise attention between same-topic words. The mathematical results involve several
  assumptions to make the analysis tractable, which we verify on data, and might be
  of independent interest as well.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li23p
month: 0
tex_title: 'How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding'
firstpage: 19689
lastpage: 19729
page: 19689-19729
order: 19689
cycles: false
bibtex_author: Li, Yuchen and Li, Yuanzhi and Risteski, Andrej
author:
- given: Yuchen
  family: Li
- given: Yuanzhi
  family: Li
- given: Andrej
  family: Risteski
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/li23p/li23p.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
