---
title: 'TIDE: Time Derivative Diffusion for Deep Learning on Graphs'
openreview: PWRIIwBJFo
abstract: A prominent paradigm for graph neural networks is based on the message-passing
  framework. In this framework, information communication is realized only between
  neighboring nodes. The challenge of approaches that use this paradigm is to ensure
  efficient and accurate long-distance communication between nodes, as deep convolutional
  networks are prone to over smoothing. In this paper, we present a novel method based
  on time derivative graph diffusion (TIDE) to overcome these structural limitations
  of the message-passing framework. Our approach allows for optimizing the spatial
  extent of diffusion across various tasks and network channels, thus enabling medium
  and long-distance communication efficiently. Furthermore, we show that our architecture
  design also enables local message-passing and thus inherits from the capabilities
  of local message-passing approaches. We show that on both widely used graph benchmarks
  and synthetic mesh and graph datasets, the proposed framework outperforms state-of-the-art
  methods by a significant margin.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: behmanesh23a
month: 0
tex_title: "{TIDE}: Time Derivative Diffusion for Deep Learning on Graphs"
firstpage: 2015
lastpage: 2030
page: 2015-2030
order: 2015
cycles: false
bibtex_author: Behmanesh, Maysam and Krahn, Maximilian and Ovsjanikov, Maks
author:
- given: Maysam
  family: Behmanesh
- given: Maximilian
  family: Krahn
- given: Maks
  family: Ovsjanikov
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/behmanesh23a/behmanesh23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
