---
title: 'Unit Scaling: Out-of-the-Box Low-Precision Training'
openreview: A8HOsNfish
abstract: 'We present unit scaling, a paradigm for designing deep learning models
  that simplifies the use of low-precision number formats. Training in FP16 or the
  recently proposed FP8 formats offers substantial efficiency gains, but can lack
  sufficient range for out-of-the-box training. Unit scaling addresses this by introducing
  a principled approach to model numerics: seeking unit variance of all weights, activations
  and gradients at initialisation. Unlike alternative methods, this approach neither
  requires multiple training runs to find a suitable scale nor has significant computational
  overhead. We demonstrate the efficacy of unit scaling across a range of models and
  optimisers. We further show that existing models can be adapted to be unit-scaled,
  training BERT-Large in FP16 and then FP8 with no degradation in accuracy.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: blake23a
month: 0
tex_title: 'Unit Scaling: Out-of-the-Box Low-Precision Training'
firstpage: 2548
lastpage: 2576
page: 2548-2576
order: 2548
cycles: false
bibtex_author: Blake, Charlie and Orr, Douglas and Luschi, Carlo
author:
- given: Charlie
  family: Blake
- given: Douglas
  family: Orr
- given: Carlo
  family: Luschi
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/blake23a/blake23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
