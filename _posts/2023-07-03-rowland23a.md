---
title: The Statistical Benefits of Quantile Temporal-Difference Learning for Value
  Estimation
openreview: 6EVUnWGBMU
abstract: We study the problem of temporal-difference-based policy evaluation in reinforcement
  learning. In particular, we analyse the use of a distributional reinforcement learning
  algorithm, quantile temporal-difference learning (QTD), for this task. We reach
  the surprising conclusion that even if a practitioner has no interest in the return
  distribution beyond the mean, QTD (which learns predictions about the full distribution
  of returns) may offer performance superior to approaches such as classical TD learning,
  which predict only the mean return, even in the tabular setting.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rowland23a
month: 0
tex_title: The Statistical Benefits of Quantile Temporal-Difference Learning for Value
  Estimation
firstpage: 29210
lastpage: 29231
page: 29210-29231
order: 29210
cycles: false
bibtex_author: Rowland, Mark and Tang, Yunhao and Lyle, Clare and Munos, Remi and
  Bellemare, Marc G and Dabney, Will
author:
- given: Mark
  family: Rowland
- given: Yunhao
  family: Tang
- given: Clare
  family: Lyle
- given: Remi
  family: Munos
- given: Marc G
  family: Bellemare
- given: Will
  family: Dabney
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/rowland23a/rowland23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
