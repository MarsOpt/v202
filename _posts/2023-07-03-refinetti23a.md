---
title: Neural networks trained with SGD learn distributions of increasing complexity
openreview: CPKMwyiyDv
abstract: The uncanny ability of over-parameterised neural networks to generalise
  well has been explained using various "simplicity biases". These theories postulate
  that neural networks avoid overfitting by first fitting simple, linear classifiers
  before learning more complex, non-linear functions. Meanwhile, data structure is
  also recognised as a key ingredient for good generalisation, yet its role in simplicity
  biases is not yet understood. Here, we show that neural networks trained using stochastic
  gradient descent initially classify their inputs using lower-order input statistics,
  like mean and covariance, and exploit higher-order statistics only later during
  training. We first demonstrate this <b>distributional simplicity bias</b> (DSB)
  in a solvable model of a single neuron trained on synthetic data. We then demonstrate
  DSB empirically in a range of deep convolutional networks and visual transformers
  trained on CIFAR10, and show that it even holds in networks pre-trained on ImageNet.
  We discuss the relation of DSB to other simplicity biases and consider its implications
  for the principle of Gaussian universality in learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: refinetti23a
month: 0
tex_title: Neural networks trained with {SGD} learn distributions of increasing complexity
firstpage: 28843
lastpage: 28863
page: 28843-28863
order: 28843
cycles: false
bibtex_author: Refinetti, Maria and Ingrosso, Alessandro and Goldt, Sebastian
author:
- given: Maria
  family: Refinetti
- given: Alessandro
  family: Ingrosso
- given: Sebastian
  family: Goldt
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/refinetti23a/refinetti23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
