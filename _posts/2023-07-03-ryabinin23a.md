---
title: 'SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient'
openreview: zZ5vSCvAxT
abstract: 'Many deep learning applications benefit from using large models with billions
  of parameters. Training these models is notoriously expensive due to the need for
  specialized HPC clusters. In this work, we consider alternative setups for training
  large models: using cheap “preemptible” instances or pooling existing resources
  from multiple regions. We analyze the performance of existing model-parallel algorithms
  in these conditions and find configurations where training larger models becomes
  less communication-intensive. Based on these findings, we propose SWARM Parallelism
  (Stochastically Wired Adaptively Rebalanced Model Parallelism), a model-parallel
  training algorithm designed for poorly connected, heterogeneous and unreliable devices.
  SWARM creates temporary randomized pipelines between nodes that are rebalanced in
  case of failure. We empirically validate our findings and compare SWARM Parallelism
  with existing large-scale training approaches. Finally, we combine our insights
  with compression strategies to train a large Transformer language model with 1B
  shared parameters ($\approx$13B before sharing) on preemptible T4 GPUs with less
  than 200 Mb/s network.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ryabinin23a
month: 0
tex_title: "{SWARM} Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient"
firstpage: 29416
lastpage: 29440
page: 29416-29440
order: 29416
cycles: false
bibtex_author: Ryabinin, Max and Dettmers, Tim and Diskin, Michael and Borzunov, Alexander
author:
- given: Max
  family: Ryabinin
- given: Tim
  family: Dettmers
- given: Michael
  family: Diskin
- given: Alexander
  family: Borzunov
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/ryabinin23a/ryabinin23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
