---
title: 'An SDE for Modeling SAM: Theory and Insights'
openreview: 3HHh17GBMO
abstract: We study the SAM (Sharpness-Aware Minimization) optimizer which has recently
  attracted a lot of interest due to its increased performance over more classical
  variants of stochastic gradient descent. Our main contribution is the derivation
  of continuous-time models (in the form of SDEs) for SAM and two of its variants,
  both for the full-batch and mini-batch settings. We demonstrate that these SDEs
  are rigorous approximations of the real discrete-time algorithms (in a weak sense,
  scaling linearly with the learning rate). Using these models, we then offer an explanation
  of why SAM prefers flat minima over sharp ones â€“ by showing that it minimizes an
  implicitly regularized loss with a Hessian-dependent noise structure. Finally, we
  prove that SAM is attracted to saddle points under some realistic conditions. Our
  theoretical results are supported by detailed experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: monzio-compagnoni23a
month: 0
tex_title: 'An {SDE} for Modeling {SAM}: Theory and Insights'
firstpage: 25209
lastpage: 25253
page: 25209-25253
order: 25209
cycles: false
bibtex_author: Monzio Compagnoni, Enea and Biggio, Luca and Orvieto, Antonio and Proske,
  Frank Norbert and Kersting, Hans and Lucchi, Aurelien
author:
- given: Enea
  family: Monzio Compagnoni
- given: Luca
  family: Biggio
- given: Antonio
  family: Orvieto
- given: Frank Norbert
  family: Proske
- given: Hans
  family: Kersting
- given: Aurelien
  family: Lucchi
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/monzio-compagnoni23a/monzio-compagnoni23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
