---
title: Lifelong Language Pretraining with Distribution-Specialized Experts
openreview: Q4QFG5Fe4O
abstract: Pretraining on a large-scale corpus has become a standard method to build
  general language models (LMs). Adapting a model to new data distributions targeting
  different downstream tasks poses significant challenges. Naive fine-tuning may incur
  catastrophic forgetting when the over-parameterized LMs overfit the new data but
  fail to preserve the pretrained features. Lifelong learning (LLL) aims to enable
  information systems to learn from a continuous data stream across time. However,
  most prior work modifies the training recipe assuming a static fixed network architecture.
  We find that additional model capacity and proper regularization are key elements
  to achieving strong LLL performance. Thus, we propose Lifelong-MoE, an extensible
  MoE (Mixture-of-Experts) architecture that dynamically adds model capacity via adding
  experts with regularized pretaining. Our results show that by only introducing a
  limited number of extra experts while keeping the computation cost constant, our
  model can steadily adapt to data distribution shifts while preserving the previous
  knowledge. Compared to existing lifelong learning approaches, Lifelong-MoE achieves
  better few-shot performance on NLP tasks. More impressively, Lifelong-MoE surpasses
  multi-task learning on 19 downstream NLU tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen23aq
month: 0
tex_title: Lifelong Language Pretraining with Distribution-Specialized Experts
firstpage: 5383
lastpage: 5395
page: 5383-5395
order: 5383
cycles: false
bibtex_author: Chen, Wuyang and Zhou, Yanqi and Du, Nan and Huang, Yanping and Laudon,
  James and Chen, Zhifeng and Cui, Claire
author:
- given: Wuyang
  family: Chen
- given: Yanqi
  family: Zhou
- given: Nan
  family: Du
- given: Yanping
  family: Huang
- given: James
  family: Laudon
- given: Zhifeng
  family: Chen
- given: Claire
  family: Cui
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/chen23aq/chen23aq.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
