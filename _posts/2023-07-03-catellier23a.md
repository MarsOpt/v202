---
title: On the Robustness of Text Vectorizers
openreview: T4XfQvi0Zo
abstract: A fundamental issue in machine learning is the robustness of the model with
  respect to changes in the input. In natural language processing, models typically
  contain a first embedding layer, transforming a sequence of tokens into vector representations.
  While the robustness with respect to changes of continuous inputs is well-understood,
  the situation is less clear when considering discrete changes, for instance replacing
  a word by another in an input sentence. Our work formally proves that popular embedding
  schemes, such as concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec), exhibit
  robustness in the Hölder or Lipschitz sense with respect to the Hamming distance.
  We provide quantitative bounds for these schemes and demonstrate how the constants
  involved are affected by the length of the document. These findings are exemplified
  through a series of numerical examples.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: catellier23a
month: 0
tex_title: On the Robustness of Text Vectorizers
firstpage: 3782
lastpage: 3814
page: 3782-3814
order: 3782
cycles: false
bibtex_author: Catellier, R\'{e}mi and Vaiter, Samuel and Garreau, Damien
author:
- given: Rémi
  family: Catellier
- given: Samuel
  family: Vaiter
- given: Damien
  family: Garreau
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/catellier23a/catellier23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
