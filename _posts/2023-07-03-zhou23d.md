---
title: Implicit Regularization Leads to Benign Overfitting for Sparse Linear Regression
openreview: dFhdAEjFAk
abstract: In deep learning, often the training process finds an interpolator (a solution
  with 0 training loss), but the test loss is still low. This phenomenon, known as
  <em>benign overfitting</em>, is a major mystery that received a lot of recent attention.
  One common mechanism for benign overfitting is <em>implicit regularization</em>,
  where the training process leads to additional properties for the interpolator,
  often characterized by minimizing certain norms. However, even for a simple sparse
  linear regression problem $y = \beta^{\ast\top} x +\xi$ with sparse $\beta^{\ast}$,
  neither minimum $\ell_1$ or $\ell_2$ norm interpolator gives the optimal test loss.
  In this work, we give a different parametrization of the model which leads to a
  new implicit regularization effect that combines the benefit of $\ell_1$ and $\ell_2$
  interpolators. We show that training our new model via gradient descent leads to
  an interpolator with near-optimal test loss. Our result is based on careful analysis
  of the training dynamics and provides another example of implicit regularization
  effect that goes beyond norm minimization.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhou23d
month: 0
tex_title: Implicit Regularization Leads to Benign Overfitting for Sparse Linear Regression
firstpage: 42543
lastpage: 42573
page: 42543-42573
order: 42543
cycles: false
bibtex_author: Zhou, Mo and Ge, Rong
author:
- given: Mo
  family: Zhou
- given: Rong
  family: Ge
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhou23d/zhou23d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
