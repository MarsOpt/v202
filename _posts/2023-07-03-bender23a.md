---
title: Continuously Parameterized Mixture Models
openreview: 3UHmUaOVWp
abstract: Mixture models are universal approximators of smooth densities but are difficult
  to utilize in complicated datasets due to restrictions on typically available modes
  and challenges with initialiations. We show that by continuously parameterizing
  a mixture of factor analyzers using a learned ordinary differential equation, we
  can improve the fit of mixture models over direct methods. Once trained, the mixture
  components can be extracted and the neural ODE can be discarded, leaving us with
  an effective, but low-resource model. We additionally explore the use of a training
  curriculum from an easy-to-model latent space extracted from a normalizing flow
  to the more complex input space and show that the smooth curriculum helps to stabilize
  and improve results with and without the continuous parameterization. Finally, we
  introduce a hierarchical version of the model to enable more flexible, robust classification
  and clustering, and show substantial improvements against traditional parameterizations
  of GMMs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bender23a
month: 0
tex_title: Continuously Parameterized Mixture Models
firstpage: 2050
lastpage: 2062
page: 2050-2062
order: 2050
cycles: false
bibtex_author: Bender, Christopher M and Shi, Yifeng and Niethammer, Marc and Oliva,
  Junier
author:
- given: Christopher M
  family: Bender
- given: Yifeng
  family: Shi
- given: Marc
  family: Niethammer
- given: Junier
  family: Oliva
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/bender23a/bender23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
