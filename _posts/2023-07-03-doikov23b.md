---
title: Polynomial Preconditioning for Gradient Methods
openreview: gJboa2IOua
abstract: We study first-order methods with preconditioning for solving structured
  convex optimization problems. We propose a new family of preconditioners generated
  by the symmetric polynomials. They provide the first-order optimization methods
  with a provable improvement of the condition number, cutting the gaps between highest
  eigenvalues, without explicit knowledge of the actual spectrum. We give a stochastic
  interpretation of this preconditioning in terms of the coordinate volume sampling
  and compare it with other classical approaches, including the Chebyshev polynomials.
  We show how to incorporate a polynomial preconditioning into the Gradient and Fast
  Gradient Methods and establish their better global complexity bounds. Finally, we
  propose a simple adaptive search procedure that automatically ensures the best polynomial
  preconditioning for the Gradient Method, minimizing the objective along a low-dimensional
  Krylov subspace. Numerical experiments confirm the efficiency of our preconditioning
  strategies for solving various machine learning problems.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: doikov23b
month: 0
tex_title: Polynomial Preconditioning for Gradient Methods
firstpage: 8162
lastpage: 8187
page: 8162-8187
order: 8162
cycles: false
bibtex_author: Doikov, Nikita and Rodomanov, Anton
author:
- given: Nikita
  family: Doikov
- given: Anton
  family: Rodomanov
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/doikov23b/doikov23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
