---
title: Tuning Language Models as Training Data Generators for Augmentation-Enhanced
  Few-Shot Learning
openreview: jbAjEhBuOZ
abstract: 'Recent studies have revealed the intriguing few-shot learning ability of
  pretrained language models (PLMs): They can quickly adapt to a new task when fine-tuned
  on a small amount of labeled data formulated as prompts, without requiring abundant
  task-specific annotations. Despite their promising performance, most existing few-shot
  approaches that only learn from the small training set still underperform fully
  supervised training by nontrivial margins. In this work, we study few-shot learning
  with PLMs from a different perspective: We first tune an autoregressive PLM on the
  few-shot samples and then use it as a generator to synthesize a large amount of
  novel training samples which augment the original training set. To encourage the
  generator to produce label-discriminative samples, we train it via weighted maximum
  likelihood where the weight of each token is automatically adjusted based on a discriminative
  meta-learning objective. A classification PLM can then be fine-tuned on both the
  few-shot and the synthetic samples with regularization for better generalization
  and stability. Our approach FewGen achieves an overall better result across seven
  classification tasks of the GLUE benchmark than existing few-shot learning methods,
  improving no-augmentation methods by 5+ average points, and outperforming augmentation
  methods by 3+ average points.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: meng23b
month: 0
tex_title: Tuning Language Models as Training Data Generators for Augmentation-Enhanced
  Few-Shot Learning
firstpage: 24457
lastpage: 24477
page: 24457-24477
order: 24457
cycles: false
bibtex_author: Meng, Yu and Michalski, Martin and Huang, Jiaxin and Zhang, Yu and
  Abdelzaher, Tarek and Han, Jiawei
author:
- given: Yu
  family: Meng
- given: Martin
  family: Michalski
- given: Jiaxin
  family: Huang
- given: Yu
  family: Zhang
- given: Tarek
  family: Abdelzaher
- given: Jiawei
  family: Han
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/meng23b/meng23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
