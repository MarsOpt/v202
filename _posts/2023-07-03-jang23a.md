---
title: Exploring the Benefits of Training Expert Language Models over Instruction
  Tuning
openreview: VAA1itvsNQ
abstract: Recently, Language Models (LMs) instruction-tuned on multiple tasks, also
  known as multitask-prompted fine-tuning (MT), have shown capabilities to generalize
  to unseen tasks. Previous work has shown that scaling the number of finetuning datasets
  and instructions is the key component in making stronger MT LMs. In this work, we
  report surprising findings that show an expert LM trained on just a single task
  can outperform an MT LM trained with 300+ different tasks on 11 different unseen
  datasets and on 13 datasets of the BIG-bench benchmark by an average of 3.20% and
  1.29%, respectively. This finding casts doubt on the previously held belief that
  simply scaling the number of tasks makes stronger MT LMs. Leveraging this finding,
  we further show that this distributed approach of training multiple expert LMs instead
  of a single MT LM for zero-shot inference possesses many benefits including (1)
  avoiding negative task transfer that often occurs during instruction tuning, (2)
  being able to continually learn new tasks without having to re-train on previous
  tasks to avoid catastrophic forgetting, and (3) showing compositional capabilities
  when merging individual experts together.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jang23a
month: 0
tex_title: Exploring the Benefits of Training Expert Language Models over Instruction
  Tuning
firstpage: 14702
lastpage: 14729
page: 14702-14729
order: 14702
cycles: false
bibtex_author: Jang, Joel and Kim, Seungone and Ye, Seonghyeon and Kim, Doyoung and
  Logeswaran, Lajanugen and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon
author:
- given: Joel
  family: Jang
- given: Seungone
  family: Kim
- given: Seonghyeon
  family: Ye
- given: Doyoung
  family: Kim
- given: Lajanugen
  family: Logeswaran
- given: Moontae
  family: Lee
- given: Kyungjae
  family: Lee
- given: Minjoon
  family: Seo
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/jang23a/jang23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
