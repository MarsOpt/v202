---
title: Markovian Gaussian Process Variational Autoencoders
openreview: Z8QlQ207V6
abstract: Sequential VAEs have been successfully considered for many high-dimensional
  time series modelling problems, with many variant models relying on discrete-time
  mechanisms such as recurrent neural networks (RNNs). On the other hand, continuous-time
  methods have recently gained attraction, especially in the context of irregularly-sampled
  time series, where they can better handle the data than discrete-time methods. One
  such class are Gaussian process variational autoencoders (GPVAEs), where the VAE
  prior is set as a Gaussian process (GP). However, a major limitation of GPVAEs is
  that it inherits the cubic computational cost as GPs, making it unattractive to
  practioners. In this work, we leverage the equivalent discrete state space representation
  of Markovian GPs to enable linear time GPVAE training via Kalman filtering and smoothing.
  For our model, Markovian GPVAE (MGPVAE), we show on a variety of high-dimensional
  temporal and spatiotemporal tasks that our method performs favourably compared to
  existing approaches whilst being computationally highly scalable.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhu23b
month: 0
tex_title: "{M}arkovian {G}aussian Process Variational Autoencoders"
firstpage: 42938
lastpage: 42961
page: 42938-42961
order: 42938
cycles: false
bibtex_author: Zhu, Harrison and Balsells-Rodas, Carles and Li, Yingzhen
author:
- given: Harrison
  family: Zhu
- given: Carles
  family: Balsells-Rodas
- given: Yingzhen
  family: Li
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhu23b/zhu23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
