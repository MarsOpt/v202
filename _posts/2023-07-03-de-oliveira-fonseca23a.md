---
title: Continuous Spatiotemporal Transformer
openreview: RnZhB7kNl0
abstract: Modeling spatiotemporal dynamical systems is a fundamental challenge in
  machine learning. Transformer models have been very successful in NLP and computer
  vision where they provide interpretable representations of data. However, a limitation
  of transformers in modeling continuous dynamical systems is that they are fundamentally
  discrete time and space models and thus have no guarantees regarding continuous
  sampling. To address this challenge, we present the Continuous Spatiotemporal Transformer
  (CST), a new transformer architecture that is designed for modeling of continuous
  systems. This new framework guarantees a continuous and smooth output via optimization
  in Sobolev space. We benchmark CST against traditional transformers as well as other
  spatiotemporal dynamics modeling methods and achieve superior performance in a number
  of tasks on synthetic and real systems, including learning brain dynamics from calcium
  imaging data.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: de-oliveira-fonseca23a
month: 0
tex_title: Continuous Spatiotemporal Transformer
firstpage: 7343
lastpage: 7365
page: 7343-7365
order: 7343
cycles: false
bibtex_author: De Oliveira Fonseca, Antonio Henrique and Zappala, Emanuele and Ortega
  Caro, Josue and Dijk, David Van
author:
- given: Antonio Henrique
  family: De Oliveira Fonseca
- given: Emanuele
  family: Zappala
- given: Josue
  family: Ortega Caro
- given: David Van
  family: Dijk
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/de-oliveira-fonseca23a/de-oliveira-fonseca23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
