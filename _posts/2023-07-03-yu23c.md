---
title: Bag of Tricks for Training Data Extraction from Language Models
openreview: 22hmxMop1O
abstract: With the advance of language models, privacy protection is receiving more
  attention. Training data extraction is therefore of great importance, as it can
  serve as a potential tool to assess privacy leakage. However, due to the difficulty
  of this task, most of the existing methods are proof-of-concept and still not effective
  enough. In this paper, we investigate and benchmark tricks for improving training
  data extraction using a publicly available dataset. Because most existing extraction
  methods use a pipeline of generating-then-ranking, i.e., generating text candidates
  as potential training data and then ranking them based on specific criteria, our
  research focuses on the tricks for both text generation (e.g., sampling strategy)
  and text ranking (e.g., token-level criteria). The experimental results show that
  several previously overlooked tricks can be crucial to the success of training data
  extraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricks outperform
  the baseline by a large margin in most cases, providing a much stronger baseline
  for future research. The code is available at https://github.com/weichen-yu/LM-Extraction.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yu23c
month: 0
tex_title: Bag of Tricks for Training Data Extraction from Language Models
firstpage: 40306
lastpage: 40320
page: 40306-40320
order: 40306
cycles: false
bibtex_author: Yu, Weichen and Pang, Tianyu and Liu, Qian and Du, Chao and Kang, Bingyi
  and Huang, Yan and Lin, Min and Yan, Shuicheng
author:
- given: Weichen
  family: Yu
- given: Tianyu
  family: Pang
- given: Qian
  family: Liu
- given: Chao
  family: Du
- given: Bingyi
  family: Kang
- given: Yan
  family: Huang
- given: Min
  family: Lin
- given: Shuicheng
  family: Yan
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/yu23c/yu23c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
