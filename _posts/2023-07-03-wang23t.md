---
title: 'CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks'
openreview: w2Vrl0zlzA
abstract: Distributed training of foundation models, especially large language models
  (LLMs), is communication-intensive and so has heavily relied on centralized data
  centers with fast interconnects. Can we train on slow networks and unlock the potential
  of decentralized infrastructure for foundation models? In this paper, we propose
  CocktailSGD, a novel communication-efficient training framework that combines three
  distinct compression techniques – random sparsification, top-K sparsification, and
  quantization – to achieve much greater compression than each individual technique
  alone. We justify the benefit of such a hybrid approach through a theoretical analysis
  of convergence. Empirically, we show that CocktailSGD achieves up to 117$\times$
  compression in fine-tuning LLMs up to 20 billion parameters without hurting convergence.
  On a 500Mbps network, CocktailSGD only incurs $\sim$1.2$\times$ slowdown compared
  with data center networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23t
month: 0
tex_title: "{C}ocktail{SGD}: Fine-tuning Foundation Models over 500{M}bps Networks"
firstpage: 36058
lastpage: 36076
page: 36058-36076
order: 36058
cycles: false
bibtex_author: Wang, Jue and Lu, Yucheng and Yuan, Binhang and Chen, Beidi and Liang,
  Percy and De Sa, Christopher and Re, Christopher and Zhang, Ce
author:
- given: Jue
  family: Wang
- given: Yucheng
  family: Lu
- given: Binhang
  family: Yuan
- given: Beidi
  family: Chen
- given: Percy
  family: Liang
- given: Christopher
  family: De Sa
- given: Christopher
  family: Re
- given: Ce
  family: Zhang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wang23t/wang23t.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
