---
title: 'BPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models'
openreview: HVKmLi1iR4
abstract: Pipeline parallelism is a key technique for training large language models
  within GPU clusters. However, it often leads to a memory imbalance problem, where
  certain GPUs face high memory pressure while others underutilize their capacity.
  This imbalance results in suboptimal training performance, even when the overall
  GPU memory capacity is sufficient for more efficient setups. To address this inefficiency,
  we propose BPipe, a novel approach for achieving memory balance in pipeline parallelism.
  BPipe employs an activation balancing method to transfer intermediate activations
  between GPUs during training, enabling all GPUs to utilize comparable amounts of
  memory. With balanced memory utilization, BPipe enhances the training efficiency
  of large language models like GPT-3 by eliminating redundant recomputations or increasing
  the micro-batch size. Our evaluation conducted on 48 A100 GPUs across six nodes
  interconnected with HDR InfiniBand shows that BPipe accelerates the training of
  GPT-3 96B and GPT-3 134B models by 1.25x-2.17x compared to Megatron-LM, a state-of-the-art
  framework for training large language models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim23l
month: 0
tex_title: "{BP}ipe: Memory-Balanced Pipeline Parallelism for Training Large Language
  Models"
firstpage: 16639
lastpage: 16653
page: 16639-16653
order: 16639
cycles: false
bibtex_author: Kim, Taebum and Kim, Hyoungjoo and Yu, Gyeong-In and Chun, Byung-Gon
author:
- given: Taebum
  family: Kim
- given: Hyoungjoo
  family: Kim
- given: Gyeong-In
  family: Yu
- given: Byung-Gon
  family: Chun
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kim23l/kim23l.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
