---
title: Understand and Modularize Generator Optimization in ELECTRA-style Pretraining
openreview: ikE60aXe8M
abstract: Despite the effectiveness of ELECTRA-style pre-training, their performance
  is dependent on the careful selection of the model size for the auxiliary generator,
  leading to high trial-and-error costs. In this paper, we present the first systematic
  study of this problem. Our theoretical investigation highlights the importance of
  controlling the generator capacity in ELECTRA-style training. Meanwhile, we found
  it is <em>not</em> handled properly in the original ELECTRA design, leading to the
  sensitivity issue. Specifically, since adaptive optimizers like Adam will cripple
  the weighing of individual losses in the joint optimization, the original design
  fails to control the generator training effectively. To regain control over the
  generator, we modularize the generator optimization by decoupling the generator
  optimizer and discriminator optimizer completely, instead of simply relying on the
  weighted objective combination. Our simple technique reduced the sensitivity of
  ELECTRA training significantly and obtains considerable performance gain compared
  to the original design.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dong23c
month: 0
tex_title: Understand and Modularize Generator Optimization in {ELECTRA}-style Pretraining
firstpage: 8244
lastpage: 8259
page: 8244-8259
order: 8244
cycles: false
bibtex_author: Dong, Chengyu and Liu, Liyuan and Cheng, Hao and Shang, Jingbo and
  Gao, Jianfeng and Liu, Xiaodong
author:
- given: Chengyu
  family: Dong
- given: Liyuan
  family: Liu
- given: Hao
  family: Cheng
- given: Jingbo
  family: Shang
- given: Jianfeng
  family: Gao
- given: Xiaodong
  family: Liu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/dong23c/dong23c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
