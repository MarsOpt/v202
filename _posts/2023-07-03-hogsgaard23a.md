---
title: AdaBoost is not an Optimal Weak to Strong Learner
openreview: xH0zbCNR5E
abstract: AdaBoost is a classic boosting algorithm for combining multiple inaccurate
  classifiers produced by a weak learner, to produce a strong learner with arbitrarily
  high accuracy when given enough training data. Determining the optimal number of
  samples necessary to obtain a given accuracy of the strong learner, is a basic learning
  theoretic question. Larsen and Ritzert (NeurIPS’22) recently presented the first
  provably optimal weak-to-strong learner. However, their algorithm is somewhat complicated
  and it remains an intriguing question whether the prototypical boosting algorithm
  AdaBoost also makes optimal use of training samples. In this work, we answer this
  question in the negative. Concretely, we show that the sample complexity of AdaBoost,
  and other classic variations thereof, are sub-optimal by at least one logarithmic
  factor in the desired accuracy of the strong learner.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hogsgaard23a
month: 0
tex_title: "{A}da{B}oost is not an Optimal Weak to Strong Learner"
firstpage: 13118
lastpage: 13140
page: 13118-13140
order: 13118
cycles: false
bibtex_author: H{\o}gsgaard, Mikael M{\o}ller and Larsen, Kasper Green and Ritzert,
  Martin
author:
- given: Mikael Møller
  family: Høgsgaard
- given: Kasper Green
  family: Larsen
- given: Martin
  family: Ritzert
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/hogsgaard23a/hogsgaard23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
