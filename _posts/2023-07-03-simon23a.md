---
title: On the Stepwise Nature of Self-Supervised Learning
openreview: aqnGHvzrqL
abstract: We present a simple picture of the training process of self-supervised learning
  methods with dual deep networks. In our picture, these methods learn their high-dimensional
  embeddings one dimension at a time in a sequence of discrete, well-separated steps.
  We arrive at this picture via the study of a linear toy model of Barlow Twins, applicable
  to the case in which the trained network is infinitely wide. We solve the training
  dynamics of our toy model from small initialization, finding that the model learns
  the top eigenmodes of a certain contrastive kernel in a discrete, stepwise fashion,
  and find a closed-form expression for the final learned representations. Remarkably,
  we see the same stepwise learning phenomenon when training deep ResNets using the
  Barlow Twins, SimCLR, and VICReg losses. This stepwise picture partially demystifies
  the process of self-supervised training.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: simon23a
month: 0
tex_title: On the Stepwise Nature of Self-Supervised Learning
firstpage: 31852
lastpage: 31876
page: 31852-31876
order: 31852
cycles: false
bibtex_author: Simon, James B and Knutins, Maksis and Ziyin, Liu and Geisz, Daniel
  and Fetterman, Abraham J and Albrecht, Joshua
author:
- given: James B
  family: Simon
- given: Maksis
  family: Knutins
- given: Liu
  family: Ziyin
- given: Daniel
  family: Geisz
- given: Abraham J
  family: Fetterman
- given: Joshua
  family: Albrecht
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/simon23a/simon23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
