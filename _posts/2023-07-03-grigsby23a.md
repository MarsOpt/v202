---
title: Hidden Symmetries of ReLU Networks
openreview: rGL49h4x9h
abstract: 'The parameter space for any fixed architecture of feedforward ReLU neural
  networks serves as a proxy during training for the associated class of functions
  - but how faithful is this representation? It is known that many different parameter
  settings $\theta$ can determine the same function $f$. Moreover, the degree of this
  redundancy is inhomogeneous: for some networks, the only symmetries are permutation
  of neurons in a layer and positive scaling of parameters at a neuron, while other
  networks admit additional hidden symmetries. In this work, we prove that, for any
  network architecture where no layer is narrower than the input, there exist parameter
  settings with no hidden symmetries. We also describe a number of mechanisms through
  which hidden symmetries can arise, and empirically approximate the functional dimension
  of different network architectures at initialization. These experiments indicate
  that the probability that a network has no hidden symmetries decreases towards 0
  as depth increases, while increasing towards 1 as width and input dimension increase.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: grigsby23a
month: 0
tex_title: Hidden Symmetries of {R}e{LU} Networks
firstpage: 11734
lastpage: 11760
page: 11734-11760
order: 11734
cycles: false
bibtex_author: Grigsby, Elisenda and Lindsey, Kathryn and Rolnick, David
author:
- given: Elisenda
  family: Grigsby
- given: Kathryn
  family: Lindsey
- given: David
  family: Rolnick
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/grigsby23a/grigsby23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
