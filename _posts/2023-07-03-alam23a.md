---
title: Recasting Self-Attention with Holographic Reduced Representations
openreview: CTZHb6PrHF
abstract: 'In recent years, self-attention has become the dominant paradigm for sequence
  modeling in a variety of domains. However, in domains with very long sequence lengths
  the $\mathcal{O}(T^2)$ memory and $\mathcal{O}(T^2 H)$ compute costs can make using
  transformers infeasible. Motivated by problems in malware detection, where sequence
  lengths of $T \geq 100,000$ are a roadblock to deep learning, we re-cast self-attention
  using the neuro-symbolic approach of Holographic Reduced Representations (HRR).
  In doing so we perform the same high-level strategy of the standard self-attention:
  a set of queries matching against a set of keys, and returning a weighted response
  of the values for each key. Implemented as a “Hrrformer” we obtain several benefits
  including $\mathcal{O}(T H \log H)$ time complexity, $\mathcal{O}(T H)$ space complexity,
  and convergence in $10\times$ fewer epochs. Nevertheless, the Hrrformer achieves
  near state-of-the-art accuracy on LRA benchmarks and we are able to learn with just
  a single layer. Combined, these benefits make our Hrrformer the first viable Transformer
  for such long malware classification sequences and up to $280\times$ faster to train
  on the Long Range Arena benchmark.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: alam23a
month: 0
tex_title: Recasting Self-Attention with Holographic Reduced Representations
firstpage: 490
lastpage: 507
page: 490-507
order: 490
cycles: false
bibtex_author: Alam, Mohammad Mahmudul and Raff, Edward and Biderman, Stella and Oates,
  Tim and Holt, James
author:
- given: Mohammad Mahmudul
  family: Alam
- given: Edward
  family: Raff
- given: Stella
  family: Biderman
- given: Tim
  family: Oates
- given: James
  family: Holt
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/alam23a/alam23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
