---
title: Near-Minimax-Optimal Risk-Sensitive Reinforcement Learning with CVaR
openreview: o6YrAc8XRm
abstract: In this paper, we study risk-sensitive Reinforcement Learning (RL), focusing
  on the objective of Conditional Value at Risk (CVaR) with risk tolerance $\tau$.
  Starting with multi-arm bandits (MABs), we show the minimax CVaR regret rate is
  $\Omega(\sqrt{\tau^{-1}AK})$, where $A$ is the number of actions and $K$ is the
  number of episodes, and that it is achieved by an Upper Confidence Bound algorithm
  with a novel Bernstein bonus. For online RL in tabular Markov Decision Processes
  (MDPs), we show a minimax regret lower bound of $\Omega(\sqrt{\tau^{-1}SAK})$ (with
  normalized cumulative rewards), where $S$ is the number of states, and we propose
  a novel bonus-driven Value Iteration procedure. We show that our algorithm achieves
  the optimal regret of $\widetilde O(\sqrt{\tau^{-1}SAK})$ under a continuity assumption
  and in general attains a near-optimal regret of $\widetilde O(\tau^{-1}\sqrt{SAK})$,
  which is minimax-optimal for constant $\tau$. This improves on the best available
  bounds. By discretizing rewards appropriately, our algorithms are computationally
  efficient.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23m
month: 0
tex_title: Near-Minimax-Optimal Risk-Sensitive Reinforcement Learning with {CV}a{R}
firstpage: 35864
lastpage: 35907
page: 35864-35907
order: 35864
cycles: false
bibtex_author: Wang, Kaiwen and Kallus, Nathan and Sun, Wen
author:
- given: Kaiwen
  family: Wang
- given: Nathan
  family: Kallus
- given: Wen
  family: Sun
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wang23m/wang23m.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
