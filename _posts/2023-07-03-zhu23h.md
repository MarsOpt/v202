---
title: Benign Overfitting in Deep Neural Networks under Lazy Training
openreview: LvT0l1CD81
abstract: This paper focuses on over-parameterized deep neural networks (DNNs) with
  ReLU activation functions and proves that when the data distribution is well-separated,
  DNNs can achieve Bayes-optimal test error for classification while obtaining (nearly)
  zero-training error under the lazy training regime. For this purpose, we unify three
  interrelated concepts of overparameterization, benign overfitting, and the Lipschitz
  constant of DNNs. Our results indicate that interpolating with smoother functions
  leads to better generalization. Furthermore, we investigate the special case where
  interpolating smooth ground-truth functions is performed by DNNs under the Neural
  Tangent Kernel (NTK) regime for generalization. Our result demonstrates that the
  generalization error converges to a constant order that only depends on label noise
  and initialization noise, which theoretically verifies benign overfitting. Our analysis
  provides a tight lower bound on the normalized margin under non-smooth activation
  functions, as well as the minimum eigenvalue of NTK under high-dimensional settings,
  which has its own interest in learning theory.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhu23h
month: 0
tex_title: Benign Overfitting in Deep Neural Networks under Lazy Training
firstpage: 43105
lastpage: 43128
page: 43105-43128
order: 43105
cycles: false
bibtex_author: Zhu, Zhenyu and Liu, Fanghui and Chrysos, Grigorios and Locatello,
  Francesco and Cevher, Volkan
author:
- given: Zhenyu
  family: Zhu
- given: Fanghui
  family: Liu
- given: Grigorios
  family: Chrysos
- given: Francesco
  family: Locatello
- given: Volkan
  family: Cevher
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhu23h/zhu23h.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
