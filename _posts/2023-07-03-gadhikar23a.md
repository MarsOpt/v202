---
title: Why Random Pruning Is All We Need to Start Sparse
openreview: cKYIyT9wvo
abstract: Random masks define surprisingly effective sparse neural network models,
  as has been shown empirically. The resulting sparse networks can often compete with
  dense architectures and state-of-the-art lottery ticket pruning algorithms, even
  though they do not rely on computationally expensive prune-train iterations and
  can be drawn initially without significant computational overhead. We offer a theoretical
  explanation of how random masks can approximate arbitrary target networks if they
  are wider by a logarithmic factor in the inverse sparsity $1 / \log(1/\text{sparsity})$.
  This overparameterization factor is necessary at least for 3-layer random networks,
  which elucidates the observed degrading performance of random networks at higher
  sparsity. At moderate to high sparsity levels, however, our results imply that sparser
  networks are contained within random source networks so that any dense-to-sparse
  training scheme can be turned into a computationally more efficient sparse-to-sparse
  one by constraining the search to a fixed random mask. We demonstrate the feasibility
  of this approach in experiments for different pruning methods and propose particularly
  effective choices of initial layer-wise sparsity ratios of the random source network.
  As a special case, we show theoretically and experimentally that random source networks
  also contain strong lottery tickets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gadhikar23a
month: 0
tex_title: Why Random Pruning Is All We Need to Start Sparse
firstpage: 10542
lastpage: 10570
page: 10542-10570
order: 10542
cycles: false
bibtex_author: Gadhikar, Advait Harshal and Mukherjee, Sohom and Burkholz, Rebekka
author:
- given: Advait Harshal
  family: Gadhikar
- given: Sohom
  family: Mukherjee
- given: Rebekka
  family: Burkholz
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/gadhikar23a/gadhikar23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
