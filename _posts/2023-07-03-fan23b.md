---
title: Optimizing DDPM Sampling with Shortcut Fine-Tuning
openreview: 3SiELE1Wzl
abstract: 'In this study, we propose Shortcut Fine-Tuning (SFT), a new approach for
  addressing the challenge of fast sampling of pretrained Denoising Diffusion Probabilistic
  Models (DDPMs). SFT advocates for the fine-tuning of DDPM samplers through the direct
  minimization of Integral Probability Metrics (IPM), instead of learning the backward
  diffusion process. This enables samplers to discover an alternative and more efficient
  sampling shortcut, deviating from the backward diffusion process. Inspired by a
  control perspective, we propose a new algorithm SFT-PG: Shortcut Fine-Tuning with
  Policy Gradient, and prove that under certain assumptions, gradient descent of diffusion
  models with respect to IPM is equivalent to performing policy gradient. To our best
  knowledge, this is the first attempt to utilize reinforcement learning (RL) methods
  to train diffusion models. Through empirical evaluation, we demonstrate that our
  fine-tuning method can further enhance existing fast DDPM samplers, resulting in
  sample quality comparable to or even surpassing that of the full-step model across
  various datasets.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fan23b
month: 0
tex_title: Optimizing {DDPM} Sampling with Shortcut Fine-Tuning
firstpage: 9623
lastpage: 9639
page: 9623-9639
order: 9623
cycles: false
bibtex_author: Fan, Ying and Lee, Kangwook
author:
- given: Ying
  family: Fan
- given: Kangwook
  family: Lee
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/fan23b/fan23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
