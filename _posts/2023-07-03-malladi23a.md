---
title: A Kernel-Based View of Language Model Fine-Tuning
openreview: 49dTFIGdx8
abstract: It has become standard to solve NLP tasks by fine-tuning pre-trained language
  models (LMs), especially in low-data settings. There is minimal theoretical understanding
  of empirical success, e.g., why fine-tuning a model with $10^8$ or more parameters
  on a couple dozen training points does not result in overfitting. We investigate
  whether the Neural Tangent Kernel (NTK)—which originated as a model to study the
  gradient descent dynamics of infinitely wide networks with suitable random initialization—describes
  fine-tuning of pre-trained LMs. This study was inspired by the decent performance
  of NTK for computer vision tasks (Wei et al., 2022). We extend the NTK formalism
  to Adam and use Tensor Programs (Yang, 2020) to characterize conditions under which
  the NTK lens may describe fine-tuning updates to pre-trained language models. Extensive
  experiments on 14 NLP tasks validate our theory and show that formulating the downstream
  task as a masked word prediction problem through prompting often induces kernel-based
  dynamics during fine-tuning. Finally, we use this kernel view to propose an explanation
  for the success of parameter-efficient subspace-based fine-tuning methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: malladi23a
month: 0
tex_title: A Kernel-Based View of Language Model Fine-Tuning
firstpage: 23610
lastpage: 23641
page: 23610-23641
order: 23610
cycles: false
bibtex_author: Malladi, Sadhika and Wettig, Alexander and Yu, Dingli and Chen, Danqi
  and Arora, Sanjeev
author:
- given: Sadhika
  family: Malladi
- given: Alexander
  family: Wettig
- given: Dingli
  family: Yu
- given: Danqi
  family: Chen
- given: Sanjeev
  family: Arora
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/malladi23a/malladi23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
