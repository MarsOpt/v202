---
title: Mimetic Initialization of Self-Attention Layers
openreview: HxN8K1esES
abstract: 'It is notoriously difficult to train Transformers on small datasets; typically,
  large pre-trained models are instead used as the starting point. We explore the
  weights of such pre-trained Transformers (particularly for vision) to attempt to
  find reasons for this discrepancy. Surprisingly, we find that simply initializing
  the weights of self-attention layers so that they "look" more like their pre-trained
  counterparts allows us to train vanilla Transformers faster and to higher final
  accuracies, particularly on vision tasks such as CIFAR-10 and ImageNet classification,
  where we see gains in accuracy of over 5% and 4%, respectively. Our initialization
  scheme is closed form, learning-free, and very simple: we set the product of the
  query and key weights to be approximately the identity, and the product of the value
  and projection weights to approximately the negative identity. As this mimics the
  patterns we saw in pre-trained Transformers, we call the technique "mimetic initialization".'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: trockman23a
month: 0
tex_title: Mimetic Initialization of Self-Attention Layers
firstpage: 34456
lastpage: 34468
page: 34456-34468
order: 34456
cycles: false
bibtex_author: Trockman, Asher and Kolter, J Zico
author:
- given: Asher
  family: Trockman
- given: J Zico
  family: Kolter
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/trockman23a/trockman23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
