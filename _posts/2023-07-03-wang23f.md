---
title: 'PreNAS: Preferred One-Shot Learning Towards Efficient Neural Architecture
  Search'
openreview: gIJGcGoz44
abstract: The wide application of pre-trained models is driving the trend of once-for-all
  training in one-shot neural architecture search (NAS). However, training within
  a huge sample space damages the performance of individual subnets and requires much
  computation to search for a optimal model. In this paper, we present PreNAS, a search-free
  NAS approach that accentuates target models in one-shot training. Specifically,
  the sample space is dramatically reduced in advance by a zero-cost selector, and
  weight-sharing one-shot training is performed on the preferred architectures to
  alleviate update conflicts. Extensive experiments have demonstrated that PreNAS
  consistently outperforms state-of-the-art one-shot NAS competitors for both Vision
  Transformer and convolutional architectures, and importantly, enables instant specialization
  with zero search cost. Our code is available at https://github.com/tinyvision/PreNAS.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23f
month: 0
tex_title: "{P}re{NAS}: Preferred One-Shot Learning Towards Efficient Neural Architecture
  Search"
firstpage: 35642
lastpage: 35654
page: 35642-35654
order: 35642
cycles: false
bibtex_author: Wang, Haibin and Ge, Ce and Chen, Hesen and Sun, Xiuyu
author:
- given: Haibin
  family: Wang
- given: Ce
  family: Ge
- given: Hesen
  family: Chen
- given: Xiuyu
  family: Sun
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wang23f/wang23f.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
