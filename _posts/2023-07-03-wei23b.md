---
title: NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning
openreview: LOy8W3UGFK
abstract: Fine-tuning a pre-trained language model (PLM) emerges as the predominant
  strategy in many natural language processing applications. However, even fine-tuning
  the PLMs and doing inference are expensive, especially on edge devices with low
  computing power. Some general approaches (e.g. quantization and distillation) have
  been widely studied to reduce the compute/memory of PLM fine-tuning, while very
  few one-shot compression techniques are explored. In this paper, we investigate
  the neural tangent kernel (NTK)–which reveals the gradient descent dynamics of neural
  networks–of the multilayer perceptrons (MLP) modules in a PLM and propose to coin
  a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider
  the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids,
  which can then be restored as a compressed MLP and surprisingly shown to well approximate
  the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural
  language understanding (NLU) and generation (NLG) tasks are provided to verify the
  effectiveness of the proposed method MLP fusion. Our code is available at https://github.com/weitianxin/MLP_Fusion.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wei23b
month: 0
tex_title: "{NTK}-approximating {MLP} Fusion for Efficient Language Model Fine-tuning"
firstpage: 36821
lastpage: 36838
page: 36821-36838
order: 36821
cycles: false
bibtex_author: Wei, Tianxin and Guo, Zeming and Chen, Yifan and He, Jingrui
author:
- given: Tianxin
  family: Wei
- given: Zeming
  family: Guo
- given: Yifan
  family: Chen
- given: Jingrui
  family: He
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wei23b/wei23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
