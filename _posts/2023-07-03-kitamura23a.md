---
title: 'Regularization and Variance-Weighted Regression Achieves Minimax Optimality
  in Linear MDPs: Theory and Practice'
openreview: 0rZvMIfECW
abstract: Mirror descent value iteration (MDVI), an abstraction of Kullback-Leibler
  (KL) and entropy-regularized reinforcement learning (RL), has served as the basis
  for recent high-performing practical RL algorithms. However, despite the use of
  function approximation in practice, the theoretical understanding of MDVI has been
  limited to tabular Markov decision processes (MDPs). We study MDVI with linear function
  approximation through its sample complexity required to identify an $\varepsilon$-optimal
  policy with probability $1-\delta$ under the settings of an infinite-horizon linear
  MDP, generative model, and G-optimal design. We demonstrate that least-squares regression
  weighted by the variance of an estimated optimal value function of the next state
  is crucial to achieving minimax optimality. Based on this observation, we present
  Variance-Weighted Least-Squares MDVI (VWLS-MDVI), the first theoretical algorithm
  that achieves nearly minimax optimal sample complexity for infinite-horizon linear
  MDPs. Furthermore, we propose a practical VWLS algorithm for value-based deep RL,
  Deep Variance Weighting (DVW). Our experiments demonstrate that DVW improves the
  performance of popular value-based deep RL algorithms on a set of MinAtar benchmarks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kitamura23a
month: 0
tex_title: 'Regularization and Variance-Weighted Regression Achieves Minimax Optimality
  in Linear {MDP}s: Theory and Practice'
firstpage: 17135
lastpage: 17175
page: 17135-17175
order: 17135
cycles: false
bibtex_author: Kitamura, Toshinori and Kozuno, Tadashi and Tang, Yunhao and Vieillard,
  Nino and Valko, Michal and Yang, Wenhao and Mei, Jincheng and Menard, Pierre and
  Gheshlaghi Azar, Mohammad and Munos, Remi and Pietquin, Olivier and Geist, Matthieu
  and Szepesvari, Csaba and Kumagai, Wataru and Matsuo, Yutaka
author:
- given: Toshinori
  family: Kitamura
- given: Tadashi
  family: Kozuno
- given: Yunhao
  family: Tang
- given: Nino
  family: Vieillard
- given: Michal
  family: Valko
- given: Wenhao
  family: Yang
- given: Jincheng
  family: Mei
- given: Pierre
  family: Menard
- given: Mohammad
  family: Gheshlaghi Azar
- given: Remi
  family: Munos
- given: Olivier
  family: Pietquin
- given: Matthieu
  family: Geist
- given: Csaba
  family: Szepesvari
- given: Wataru
  family: Kumagai
- given: Yutaka
  family: Matsuo
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kitamura23a/kitamura23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
