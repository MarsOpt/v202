---
title: 'Lowering the Pre-training Tax for Gradient-based Subset Training: A Lightweight
  Distributed Pre-Training Toolkit'
openreview: 1pMC4ScIXn
abstract: Training data and model sizes are increasing exponentially. One way to reduce
  training time and resources is to train with a carefully selected subset of the
  full dataset. Prior work uses the gradient signals obtained during a warm-up or
  â€œpre-training" phase over the full dataset, for determining the core subset; if
  the pre-training phase is too small, the gradients obtained are chaotic and unreliable.
  As a result, the pre-training phase itself incurs significant time/resource overhead,
  and prior work has not gone beyond hyperparameter search to reduce pre-training
  time. Our work explicitly aims to reduce this $\textbf{pre-training tax}$ in gradient-based
  subset training. We develop a principled, scalable approach for pre-training in
  a distributed setup. Our approach is $\textit{lightweight}$ and $\textit{minimizes
  communication}$ between distributed worker nodes. It is the first to utilize the
  concept of model-soup based distributed training $\textit{at initialization}$. The
  key idea is to minimally train an ensemble of models on small, disjointed subsets
  of the data; we further employ data-driven sparsity and data augmentation for local
  worker training to boost ensemble diversity. The centralized model, obtained at
  the end of pre-training by merging the per-worker models, is found to offer stabilized
  gradient signals to select subsets, on which the main model is further trained.
  We have validated the effectiveness of our method through extensive experiments
  on CIFAR-10/100, and ImageNet, using ResNet and WideResNet models. For example,
  our approach is shown to achieve $\textbf{15.4$\times$}$ pre-training speedup and
  $\textbf{2.8$\times$}$ end-to-end speedup on CIFAR10 and ResNet18 without loss of
  accuracy. The code is at https://github.com/moonbucks/LiPT.git.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ro23a
month: 0
tex_title: 'Lowering the Pre-training Tax for Gradient-based Subset Training: A Lightweight
  Distributed Pre-Training Toolkit'
firstpage: 29130
lastpage: 29142
page: 29130-29142
order: 29130
cycles: false
bibtex_author: Ro, Yeonju and Wang, Zhangyang and Chidambaram, Vijay and Akella, Aditya
author:
- given: Yeonju
  family: Ro
- given: Zhangyang
  family: Wang
- given: Vijay
  family: Chidambaram
- given: Aditya
  family: Akella
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/ro23a/ro23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
