---
title: 'Beyond Lipschitz Smoothness: A Tighter Analysis for Nonconvex Optimization'
openreview: ehfsxFKgNu
abstract: Negative and positive curvatures affect optimization in different ways.
  However, a lot of existing optimization theories are based on the Lipschitz smoothness
  assumption, which cannot differentiate between the two. In this paper, we propose
  to use two separate assumptions for positive and negative curvatures, so that we
  can study the different implications of the two. We analyze the Lookahead and Local
  SGD methods as concrete examples. Both of them require multiple copies of model
  parameters and communication among them for every certain period of time in order
  to prevent divergence. We show that the minimum communication frequency is inversely
  proportional to the negative curvature, and when the negative curvature becomes
  zero, we recover the existing theory results for convex optimization. Finally, both
  experimentally and theoretically, we demonstrate that modern neural networks have
  highly unbalanced positive/negative curvatures. Thus, an analysis based on separate
  positive and negative curvatures is more pertinent.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hu23i
month: 0
tex_title: 'Beyond {L}ipschitz Smoothness: A Tighter Analysis for Nonconvex Optimization'
firstpage: 13652
lastpage: 13678
page: 13652-13678
order: 13652
cycles: false
bibtex_author: Hu, Zhengmian and Wu, Xidong and Huang, Heng
author:
- given: Zhengmian
  family: Hu
- given: Xidong
  family: Wu
- given: Heng
  family: Huang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/hu23i/hu23i.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
