---
title: Stochastic Gradient Succeeds for Bandits
openreview: XqyXhjVRxR
abstract: 'We show that the stochastic gradient bandit algorithm converges to a globally
  optimal policy at an $O(1/t)$ rate, even with a constant step size. Remarkably,
  global convergence of the stochastic gradient bandit algorithm has not been previously
  established, even though it is an old algorithm known to be applicable to bandits.
  The new result is achieved by establishing two novel technical findings: first,
  the noise of the stochastic updates in the gradient bandit algorithm satisfies a
  strong “growth condition” property, where the variance diminishes whenever progress
  becomes small, implying that additional noise control via diminishing step sizes
  is unnecessary; second, a form of “weak exploration” is automatically achieved through
  the stochastic gradient updates, since they prevent the action probabilities from
  decaying faster than $O(1/t)$, thus ensuring that every action is sampled infinitely
  often with probability $1$. These two findings can be used to show that the stochastic
  gradient update is already “sufficient” for bandits in the sense that exploration
  versus exploitation is automatically balanced in a manner that ensures almost sure
  convergence to a global optimum. These novel theoretical findings are further verified
  by experimental results.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mei23a
month: 0
tex_title: Stochastic Gradient Succeeds for Bandits
firstpage: 24325
lastpage: 24360
page: 24325-24360
order: 24325
cycles: false
bibtex_author: Mei, Jincheng and Zhong, Zixin and Dai, Bo and Agarwal, Alekh and Szepesvari,
  Csaba and Schuurmans, Dale
author:
- given: Jincheng
  family: Mei
- given: Zixin
  family: Zhong
- given: Bo
  family: Dai
- given: Alekh
  family: Agarwal
- given: Csaba
  family: Szepesvari
- given: Dale
  family: Schuurmans
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/mei23a/mei23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
