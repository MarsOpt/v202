---
title: Specializing Smaller Language Models towards Multi-Step Reasoning
openreview: MXuLl38AEm
abstract: 'The surprising ability of Large Language Models (LLMs) to perform well
  on complex reasoning with only few-shot chain-of-thought prompts is believed to
  emerge only in very large-scale models. We show that such abilities can, in fact,
  be distilled down from GPT-3.5 (≥ 175B) to T5 variants (≤ 11B). We propose model
  specialization, to specialize the model’s ability towards a target task. The hypothesis
  is that large models (commonly viewed as larger than 100B) have strong modeling
  power such that they can perform a large spectrum of tasks. Small models (commonly
  viewed as smaller than 10B) have limited model capacity, but if we specialize their
  capacity towards a target task, the model can achieve decent performance improvements.
  We use multi-step math reasoning as our testbed because it is a very typical emergent
  ability. We show two important aspects of model abilities: (1) balancing language
  model’s performance on multiple tasks is a delicate matter, as improvements on one
  task may compromise other tasks; (2) yet by intentionally paying the price of decreased
  generic ability, we can clearly improve across different model scales smaller than
  10B towards a specialized multi-step math reasoning ability. We further give comprehensive
  discussions about important design choices for better generalization, including
  the data format mixture and the start model checkpoint. We hope our practice and
  discoveries can serve as an important attempt towards specialized smaller models
  in the new research paradigm set by LLMs.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fu23d
month: 0
tex_title: Specializing Smaller Language Models towards Multi-Step Reasoning
firstpage: 10421
lastpage: 10430
page: 10421-10430
order: 10421
cycles: false
bibtex_author: Fu, Yao and Peng, Hao and Ou, Litu and Sabharwal, Ashish and Khot,
  Tushar
author:
- given: Yao
  family: Fu
- given: Hao
  family: Peng
- given: Litu
  family: Ou
- given: Ashish
  family: Sabharwal
- given: Tushar
  family: Khot
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/fu23d/fu23d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
