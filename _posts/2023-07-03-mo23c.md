---
title: 'Pruning via Sparsity-indexed ODE: a Continuous Sparsity Viewpoint'
openreview: xN4eYXdY64
abstract: Neural pruning, which involves identifying the optimal sparse subnetwork,
  is a key technique for reducing the complexity and improving the efficiency of deep
  neural networks. To address the challenge of solving neural pruning at a specific
  sparsity level directly, we investigate the evolution of optimal subnetworks with
  continuously increasing sparsity, which can provide insight into how to transform
  an unpruned dense model into an optimal subnetwork with any desired level of sparsity.
  In this paper, we proposed a novel pruning framework, coined Sparsity-indexed ODE
  (SpODE) that provides explicit guidance on how to best preserve model performance
  while ensuring an infinitesimal increase in model sparsity. On top of this, we develop
  a pruning algorithm, termed Pruning via Sparsity-indexed ODE (PSO), that enables
  effective pruning via traveling along the SpODE path. Empirical experiments show
  that PSO achieves either better or comparable performance compared to state-of-the-art
  baselines across various pruning settings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mo23c
month: 0
tex_title: 'Pruning via Sparsity-indexed {ODE}: a Continuous Sparsity Viewpoint'
firstpage: 25018
lastpage: 25036
page: 25018-25036
order: 25018
cycles: false
bibtex_author: Mo, Zhanfeng and Shi, Haosen and Pan, Sinno Jialin
author:
- given: Zhanfeng
  family: Mo
- given: Haosen
  family: Shi
- given: Sinno Jialin
  family: Pan
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/mo23c/mo23c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
