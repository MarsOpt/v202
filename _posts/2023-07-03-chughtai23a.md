---
title: 'A Toy Model of Universality: Reverse Engineering how Networks Learn Group
  Operations'
openreview: jCOrkuUpss
abstract: 'Universality is a key hypothesis in mechanistic interpretability – that
  different models learn similar features and circuits when trained on similar tasks.
  In this work, we study the universality hypothesis by examining how small networks
  learn to implement group compositions. We present a novel algorithm by which neural
  networks may implement composition for any finite group via mathematical representation
  theory. We then show that these networks consistently learn this algorithm by reverse
  engineering model logits and weights, and confirm our understanding using ablations.
  By studying networks trained on various groups and architectures, we find mixed
  evidence for universality: using our algorithm, we can completely characterize the
  family of circuits and features that networks learn on this task, but for a given
  network the precise circuits learned – as well as the order they develop – are arbitrary.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chughtai23a
month: 0
tex_title: 'A Toy Model of Universality: Reverse Engineering how Networks Learn Group
  Operations'
firstpage: 6243
lastpage: 6267
page: 6243-6267
order: 6243
cycles: false
bibtex_author: Chughtai, Bilal and Chan, Lawrence and Nanda, Neel
author:
- given: Bilal
  family: Chughtai
- given: Lawrence
  family: Chan
- given: Neel
  family: Nanda
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/chughtai23a/chughtai23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
