---
title: 'DoG is SGD’s Best Friend: A Parameter-Free Dynamic Step Size Schedule'
openreview: ZrvQmCqiqh
abstract: We propose a tuning-free dynamic SGD step size formula, which we call Distance
  over Gradients (DoG). The DoG step sizes depend on simple empirical quantities (distance
  from the initial point and norms of gradients) and have no “learning rate” parameter.
  Theoretically, we show that, for stochastic convex optimization, a slight variation
  of the DoG formula enjoys strong, high-probability parameter-free convergence guarantees
  and iterate movement bounds. Empirically, we consider a broad range of vision and
  language transfer learning tasks, and show that DoG’s performance is close to that
  of SGD with tuned learning rate. We also propose a per-layer variant of DoG that
  generally outperforms tuned SGD, approaching the performance of tuned Adam. A PyTorch
  implementation of our algorithms is available at https://github.com/formll/dog.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ivgi23a
month: 0
tex_title: "{D}o{G} is {SGD}’s Best Friend: A Parameter-Free Dynamic Step Size Schedule"
firstpage: 14465
lastpage: 14499
page: 14465-14499
order: 14465
cycles: false
bibtex_author: Ivgi, Maor and Hinder, Oliver and Carmon, Yair
author:
- given: Maor
  family: Ivgi
- given: Oliver
  family: Hinder
- given: Yair
  family: Carmon
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/ivgi23a/ivgi23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
