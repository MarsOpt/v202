---
title: Efficient displacement convex optimization with particle gradient descent
openreview: 7snQRkYh6I
abstract: Particle gradient descent, which uses particles to represent a probability
  measure and performs gradient descent on particles in parallel, is widely used to
  optimize functions of probability measures. This paper considers particle gradient
  descent with a finite number of particles and establishes its theoretical guarantees
  to optimize functions that are <em>displacement convex</em> in measures. Concretely,
  for Lipschitz displacement convex functions defined on probability over $R^d$, we
  prove that $O(1/\epsilon^2)$ particles and $O(d/\epsilon^4)$ iterations are sufficient
  to find the $\epsilon$-optimal solutions. We further provide improved complexity
  bounds for optimizing smooth displacement convex functions. An application of our
  results proves the conjecture of <em>no optimization-barrier up to permutation invariance</em>,
  proposed by Entezari et al. (2022), for specific two-layer neural networks with
  two-dimensional inputs uniformly drawn from unit circle.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: daneshmand23a
month: 0
tex_title: Efficient displacement convex optimization with particle gradient descent
firstpage: 6836
lastpage: 6854
page: 6836-6854
order: 6836
cycles: false
bibtex_author: Daneshmand, Hadi and Lee, Jason D. and Jin, Chi
author:
- given: Hadi
  family: Daneshmand
- given: Jason D.
  family: Lee
- given: Chi
  family: Jin
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/daneshmand23a/daneshmand23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
