---
title: Quantitative Universal Approximation Bounds for Deep Belief Networks
openreview: 1VxLNhSVMp
abstract: We show that deep belief networks with binary hidden units can approximate
  any multivariate probability density under very mild integrability requirements
  on the parental density of the visible nodes. The approximation is measured in the
  $L^q$-norm for $q\in[1,\infty]$ ($q=\infty$ corresponding to the supremum norm)
  and in Kullback-Leibler divergence. Furthermore, we establish sharp quantitative
  bounds on the approximation error in terms of the number of hidden units.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sieber23a
month: 0
tex_title: Quantitative Universal Approximation Bounds for Deep Belief Networks
firstpage: 31773
lastpage: 31787
page: 31773-31787
order: 31773
cycles: false
bibtex_author: Sieber, Julian and Gehringer, Johann
author:
- given: Julian
  family: Sieber
- given: Johann
  family: Gehringer
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/sieber23a/sieber23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
