---
title: Continual Learners are Incremental Model Generalizers
openreview: xFXjnK8Ksl
abstract: Motivated by the efficiency and rapid convergence of pre-trained models
  for solving downstream tasks, this paper extensively studies the impact of Continual
  Learning (CL) models as pre-trainers. We find that, in both supervised and unsupervised
  CL, the transfer quality of representations does not show a noticeable degradation
  of fine-tuning performance but rather increases gradually. This is because CL models
  can learn improved task-general features when easily forgetting task-specific knowledge.
  Based on this observation, we suggest a new unsupervised CL framework with masked
  modeling, which aims to capture fluent task-generic representation during training.
  Furthermore, we propose a new fine-tuning scheme, GLobal Attention Discretization
  (GLAD), that preserves rich task-generic representation during solving downstream
  tasks. The model fine-tuned with GLAD achieves competitive performance and can also
  be used as a good pre-trained model itself. We believe this paper breaks the barriers
  between pre-training and fine-tuning steps and leads to a sustainable learning framework
  in which the continual learner incrementally improves model generalization, yielding
  better transfer to unseen tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yoon23b
month: 0
tex_title: Continual Learners are Incremental Model Generalizers
firstpage: 40129
lastpage: 40146
page: 40129-40146
order: 40129
cycles: false
bibtex_author: Yoon, Jaehong and Hwang, Sung Ju and Cao, Yue
author:
- given: Jaehong
  family: Yoon
- given: Sung Ju
  family: Hwang
- given: Yue
  family: Cao
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/yoon23b/yoon23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
