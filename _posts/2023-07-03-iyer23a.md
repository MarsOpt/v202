---
title: Maximal Initial Learning Rates in Deep ReLU Networks
openreview: 2BCSilfrJk
abstract: Training a neural network requires choosing a suitable learning rate, which
  involves a trade-off between speed and effectiveness of convergence. While there
  has been considerable theoretical and empirical analysis of how large the learning
  rate can be, most prior work focuses only on late-stage training. In this work,
  we introduce the maximal initial learning rate $\eta^{\ast}$ - the largest learning
  rate at which a randomly initialized neural network can successfully begin training
  and achieve (at least) a given threshold accuracy. Using a simple approach to estimate
  $\eta^{\ast}$, we observe that in constant-width fully-connected ReLU networks,
  $\eta^{\ast}$ behaves differently from the maximum learning rate later in training.
  Specifically, we find that $\eta^{\ast}$ is well predicted as a power of depth $\times$
  width, provided that (i) the width of the network is sufficiently large compared
  to the depth, and (ii) the input layer is trained at a relatively small learning
  rate. We further analyze the relationship between $\eta^{\ast}$ and the sharpness
  $\lambda_{1}$ of the network at initialization, indicating they are closely though
  not inversely related. We formally prove bounds for $\lambda_{1}$ in terms of depth
  $\times$ width that align with our empirical results.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: iyer23a
month: 0
tex_title: Maximal Initial Learning Rates in Deep {R}e{LU} Networks
firstpage: 14500
lastpage: 14530
page: 14500-14530
order: 14500
cycles: false
bibtex_author: Iyer, Gaurav and Hanin, Boris and Rolnick, David
author:
- given: Gaurav
  family: Iyer
- given: Boris
  family: Hanin
- given: David
  family: Rolnick
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/iyer23a/iyer23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
