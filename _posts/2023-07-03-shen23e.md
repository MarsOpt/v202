---
title: 'Cross-Modal Fine-Tuning: Align then Refine'
openreview: 2C8Y6iao2I
abstract: 'Fine-tuning large-scale pretrained models has led to tremendous progress
  in well-studied modalities such as vision and NLP. However, similar gains have not
  been observed in many other modalities due to a lack of relevant pretrained models.
  In this work, we propose ORCA, a general cross-modal fine-tuning framework that
  extends the applicability of a single large-scale pretrained model to diverse modalities.
  ORCA adapts to a target task via an align-then-refine workflow: given the target
  input, ORCA first learns an embedding network that aligns the embedded feature distribution
  with the pretraining modality. The pretrained model is then fine-tuned on the embedded
  data to exploit the knowledge shared across modalities. Through extensive experiments,
  we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over
  60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML,
  general-purpose, and task-specific cross-modal methods. We highlight the importance
  of data alignment via a series of ablation studies and exemplify ORCAâ€™s utility
  in data-limited regimes.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shen23e
month: 0
tex_title: 'Cross-Modal Fine-Tuning: Align then Refine'
firstpage: 31030
lastpage: 31056
page: 31030-31056
order: 31030
cycles: false
bibtex_author: Shen, Junhong and Li, Liam and Dery, Lucio M. and Staten, Corey and
  Khodak, Mikhail and Neubig, Graham and Talwalkar, Ameet
author:
- given: Junhong
  family: Shen
- given: Liam
  family: Li
- given: Lucio M.
  family: Dery
- given: Corey
  family: Staten
- given: Mikhail
  family: Khodak
- given: Graham
  family: Neubig
- given: Ameet
  family: Talwalkar
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/shen23e/shen23e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
