---
title: A Fast, Well-Founded Approximation to the Empirical Neural Tangent Kernel
openreview: 3UXsGzUJc5
abstract: 'Empirical neural tangent kernels (eNTKs) can provide a good understanding
  of a given networkâ€™s representation: they are often far less expensive to compute
  and applicable more broadly than infinite-width NTKs. For networks with $O$ output
  units (e.g. an $O$-class classifier), however, the eNTK on $N$ inputs is of size
  $NO \times NO$, taking $\mathcal O\big( (N O)^2\big)$ memory and up to $\mathcal
  O\big( (N O)^3 \big)$ computation to use. Most existing applications have therefore
  used one of a handful of approximations yielding $N \times N$ kernel matrices, saving
  orders of magnitude of computation, but with limited to no justification. We prove
  that one such approximation, which we call "sum of logits," converges to the true
  eNTK at initialization. Our experiments demonstrate the quality of this approximation
  for various uses across a range of settings.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mohamadi23a
month: 0
tex_title: A Fast, Well-Founded Approximation to the Empirical Neural Tangent Kernel
firstpage: 25061
lastpage: 25081
page: 25061-25081
order: 25061
cycles: false
bibtex_author: Mohamadi, Mohamad Amin and Bae, Wonho and Sutherland, Danica J.
author:
- given: Mohamad Amin
  family: Mohamadi
- given: Wonho
  family: Bae
- given: Danica J.
  family: Sutherland
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/mohamadi23a/mohamadi23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
