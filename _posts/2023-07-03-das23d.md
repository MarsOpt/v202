---
title: Understanding Self-Distillation in the Presence of Label Noise
openreview: UL9purXHyB
abstract: Self-distillation (SD) is the process of first training a "teacher" model
  and then using its predictions to train a "student" model that has the <em>same</em>
  architecture. Specifically, the student’s loss is $\big(\xi*\ell(\text{teacher’s
  predictions}, \text{ student’s predictions}) + (1-\xi)*\ell(\text{given labels},
  \text{ student’s predictions})\big)$, where $\ell$ is the loss function and $\xi$
  is some parameter $\in [0,1]$. SD has been empirically observed to provide performance
  gains in several settings. In this paper, we theoretically characterize the effect
  of SD in two supervised learning problems with <em>noisy labels</em>. We first analyze
  SD for regularized linear regression and show that in the high label noise regime,
  the optimal value of $\xi$ that minimizes the expected error in estimating the ground
  truth parameter is surprisingly greater than 1. Empirically, we show that $\xi >
  1$ works better than $\xi \leq 1$ even with the cross-entropy loss for several classification
  datasets when 50% or 30% of the labels are corrupted. Further, we quantify when
  optimal SD is better than optimal regularization. Next, we analyze SD in the case
  of logistic regression for binary classification with random label corruption and
  quantify the range of label corruption in which the student outperforms the teacher
  (w.r.t. accuracy). To our knowledge, this is the first result of its kind for the
  cross-entropy loss.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: das23d
month: 0
tex_title: Understanding Self-Distillation in the Presence of Label Noise
firstpage: 7102
lastpage: 7140
page: 7102-7140
order: 7102
cycles: false
bibtex_author: Das, Rudrajit and Sanghavi, Sujay
author:
- given: Rudrajit
  family: Das
- given: Sujay
  family: Sanghavi
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/das23d/das23d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
