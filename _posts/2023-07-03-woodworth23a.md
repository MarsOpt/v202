---
title: 'Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy'
openreview: FR2F4QzWFp
abstract: We present an algorithm for minimizing an objective with hard-to-compute
  gradients by using a related, easier-to-access function as a proxy. Our algorithm
  is based on approximate proximal-point iterations on the proxy combined with relatively
  few stochastic gradients from the objective. When the difference between the objective
  and the proxy is $\delta$-smooth, our algorithm guarantees convergence at a rate
  matching stochastic gradient descent on a $\delta$-smooth objective, which can lead
  to substantially better sample efficiency. Our algorithm has many potential applications
  in machine learning, and provides a principled means of leveraging synthetic data,
  physics simulators, mixed public and private data, and more.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: woodworth23a
month: 0
tex_title: 'Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy'
firstpage: 37273
lastpage: 37292
page: 37273-37292
order: 37273
cycles: false
bibtex_author: Woodworth, Blake and Mishchenko, Konstantin and Bach, Francis
author:
- given: Blake
  family: Woodworth
- given: Konstantin
  family: Mishchenko
- given: Francis
  family: Bach
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/woodworth23a/woodworth23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
