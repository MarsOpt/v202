---
title: Unconstrained Online Learning with Unbounded Losses
openreview: 2K2vEVBm5G
abstract: 'Algorithms for online learning typically require one or more boundedness
  assumptions: that the domain is bounded, that the losses are Lipschitz, or both.
  In this paper, we develop a new setting for online learning with unbounded domains
  and non-Lipschitz losses. For this setting we provide an algorithm which guarantees
  $R_{T}(u)\le \tilde O(G\|u\|\sqrt{T}+L\|u\|^{2}\sqrt{T})$ regret on any problem
  where the subgradients satisfy $\|g_{t}\|\le G+L\|w_{t}\|$, and show that this bound
  is unimprovable without further assumptions. We leverage this algorithm to develop
  new saddle-point optimization algorithms that converge in duality gap in unbounded
  domains, even in the absence of meaningful curvature. Finally, we provide the first
  algorithm achieving non-trivial dynamic regret in an unbounded domain for non-Lipschitz
  losses, as well as a matching lower bound. The regret of our dynamic regret algorithm
  automatically improves to a novel $L^{*}$ bound when the losses are smooth.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jacobsen23a
month: 0
tex_title: Unconstrained Online Learning with Unbounded Losses
firstpage: 14590
lastpage: 14630
page: 14590-14630
order: 14590
cycles: false
bibtex_author: Jacobsen, Andrew and Cutkosky, Ashok
author:
- given: Andrew
  family: Jacobsen
- given: Ashok
  family: Cutkosky
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/jacobsen23a/jacobsen23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
