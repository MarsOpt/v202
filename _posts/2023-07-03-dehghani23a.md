---
title: Scaling Vision Transformers to 22 Billion Parameters
openreview: Lhyy8H75KA
abstract: The scaling of Transformers has driven breakthrough capabilities for language
  models. At present, the largest large language models (LLMs) contain upwards of
  100B parameters. Vision Transformers (ViT) have introduced the same architecture
  to image and video modelling, but these have not yet been successfully scaled to
  nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al.,
  2022). We present a recipe for highly efficient and stable training of a 22B-parameter
  ViT (ViT-22B) and perform a wide variety of experiments on the resulting model.
  When evaluated on downstream tasks (often with a lightweight linear model on frozen
  features), ViT-22B demonstrates increasing performance with scale. We further observe
  other interesting benefits of scale, including an improved tradeoff between fairness
  and performance, state-of-the-art alignment to human visual perception in terms
  of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential
  for "LLM-like" scaling in vision, and provides key steps towards getting there.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dehghani23a
month: 0
tex_title: Scaling Vision Transformers to 22 Billion Parameters
firstpage: 7480
lastpage: 7512
page: 7480-7512
order: 7480
cycles: false
bibtex_author: Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski,
  Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron,
  Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and Jenatton, Rodolphe and
  Beyer, Lucas and Tschannen, Michael and Arnab, Anurag and Wang, Xiao and Riquelme
  Ruiz, Carlos and Minderer, Matthias and Puigcerver, Joan and Evci, Utku and Kumar,
  Manoj and Steenkiste, Sjoerd Van and Elsayed, Gamaleldin Fathy and Mahendran, Aravindh
  and Yu, Fisher and Oliver, Avital and Huot, Fantine and Bastings, Jasmijn and Collier,
  Mark and Gritsenko, Alexey A. and Birodkar, Vighnesh and Vasconcelos, Cristina Nader
  and Tay, Yi and Mensink, Thomas and Kolesnikov, Alexander and Pavetic, Filip and
  Tran, Dustin and Kipf, Thomas and Lucic, Mario and Zhai, Xiaohua and Keysers, Daniel
  and Harmsen, Jeremiah J. and Houlsby, Neil
author:
- given: Mostafa
  family: Dehghani
- given: Josip
  family: Djolonga
- given: Basil
  family: Mustafa
- given: Piotr
  family: Padlewski
- given: Jonathan
  family: Heek
- given: Justin
  family: Gilmer
- given: Andreas Peter
  family: Steiner
- given: Mathilde
  family: Caron
- given: Robert
  family: Geirhos
- given: Ibrahim
  family: Alabdulmohsin
- given: Rodolphe
  family: Jenatton
- given: Lucas
  family: Beyer
- given: Michael
  family: Tschannen
- given: Anurag
  family: Arnab
- given: Xiao
  family: Wang
- given: Carlos
  family: Riquelme Ruiz
- given: Matthias
  family: Minderer
- given: Joan
  family: Puigcerver
- given: Utku
  family: Evci
- given: Manoj
  family: Kumar
- given: Sjoerd Van
  family: Steenkiste
- given: Gamaleldin Fathy
  family: Elsayed
- given: Aravindh
  family: Mahendran
- given: Fisher
  family: Yu
- given: Avital
  family: Oliver
- given: Fantine
  family: Huot
- given: Jasmijn
  family: Bastings
- given: Mark
  family: Collier
- given: Alexey A.
  family: Gritsenko
- given: Vighnesh
  family: Birodkar
- given: Cristina Nader
  family: Vasconcelos
- given: Yi
  family: Tay
- given: Thomas
  family: Mensink
- given: Alexander
  family: Kolesnikov
- given: Filip
  family: Pavetic
- given: Dustin
  family: Tran
- given: Thomas
  family: Kipf
- given: Mario
  family: Lucic
- given: Xiaohua
  family: Zhai
- given: Daniel
  family: Keysers
- given: Jeremiah J.
  family: Harmsen
- given: Neil
  family: Houlsby
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/dehghani23a/dehghani23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
