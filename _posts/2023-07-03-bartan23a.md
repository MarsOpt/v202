---
title: 'Moccasin: Efficient Tensor Rematerialization for Neural Networks'
openreview: GN9bGEWvkx
abstract: The deployment and training of neural networks on edge computing devices
  pose many challenges. The low memory nature of edge devices is often one of the
  biggest limiting factors encountered in the deployment of large neural network models.
  Tensor rematerialization or recompute is a way to address high memory requirements
  for neural network training and inference. In this paper we consider the problem
  of execution time minimization of compute graphs subject to a memory budget. In
  particular, we develop a new constraint programming formulation called Moccasin
  with only $O(n)$ integer variables, where $n$ is the number of nodes in the compute
  graph. This is a significant improvement over the works in the recent literature
  that propose formulations with $O(n^2)$ Boolean variables. We present numerical
  studies that show that our approach is up to an order of magnitude faster than recent
  work especially for large-scale graphs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bartan23a
month: 0
tex_title: 'Moccasin: Efficient Tensor Rematerialization for Neural Networks'
firstpage: 1826
lastpage: 1837
page: 1826-1837
order: 1826
cycles: false
bibtex_author: Bartan, Burak and Li, Haoming and Teague, Harris and Lott, Christopher
  and Dilkina, Bistra
author:
- given: Burak
  family: Bartan
- given: Haoming
  family: Li
- given: Harris
  family: Teague
- given: Christopher
  family: Lott
- given: Bistra
  family: Dilkina
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/bartan23a/bartan23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
