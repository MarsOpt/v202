---
title: A Large-Scale Study of Probabilistic Calibration in Neural Network Regression
openreview: 3fNVNNyKyV
abstract: Accurate probabilistic predictions are essential for optimal decision making.
  While neural network miscalibration has been studied primarily in classification,
  we investigate this in the less-explored domain of regression. We conduct the largest
  empirical study to date to assess the probabilistic calibration of neural networks.
  We also analyze the performance of recalibration, conformal, and regularization
  methods to enhance probabilistic calibration. Additionally, we introduce novel differentiable
  recalibration and regularization methods, uncovering new insights into their effectiveness.
  Our findings reveal that regularization methods offer a favorable tradeoff between
  calibration and sharpness. Post-hoc methods exhibit superior probabilistic calibration,
  which we attribute to the finite-sample coverage guarantee of conformal prediction.
  Furthermore, we demonstrate that quantile recalibration can be considered as a specific
  case of conformal prediction. Our study is fully reproducible and implemented in
  a common code base for fair comparisons.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dheur23a
month: 0
tex_title: A Large-Scale Study of Probabilistic Calibration in Neural Network Regression
firstpage: 7813
lastpage: 7836
page: 7813-7836
order: 7813
cycles: false
bibtex_author: Dheur, Victor and Ben Taieb, Souhaib
author:
- given: Victor
  family: Dheur
- given: Souhaib
  family: Ben Taieb
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/dheur23a/dheur23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
