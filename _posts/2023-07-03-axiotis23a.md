---
title: Gradient Descent Converges Linearly for Logistic Regression on Separable Data
openreview: a4bMHPm0Ji
abstract: We show that running gradient descent with variable learning rate guarantees
  loss $f(x) â‰¤ 1.1 \cdot f(x^*)+\epsilon$ for the logistic regression objective, where
  the error $\epsilon$ decays exponentially with the number of iterations and polynomially
  with the magnitude of the entries of an arbitrary fixed solution $x$. This is in
  contrast to the common intuition that the absence of strong convexity precludes
  linear convergence of first-order methods, and highlights the importance of variable
  learning rates for gradient descent. We also apply our ideas to sparse logistic
  regression, where they lead to an exponential improvement of the sparsity-error
  tradeoff.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: axiotis23a
month: 0
tex_title: Gradient Descent Converges Linearly for Logistic Regression on Separable
  Data
firstpage: 1302
lastpage: 1319
page: 1302-1319
order: 1302
cycles: false
bibtex_author: Axiotis, Kyriakos and Sviridenko, Maxim
author:
- given: Kyriakos
  family: Axiotis
- given: Maxim
  family: Sviridenko
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/axiotis23a/axiotis23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
