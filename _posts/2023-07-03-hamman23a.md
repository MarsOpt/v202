---
title: Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees
openreview: gzjK23oK9i
abstract: There is an emerging interest in generating robust counterfactual explanations
  that would remain valid if the model is updated or changed even slightly. Towards
  finding robust counterfactuals, existing literature often assumes that the original
  model $m$ and the new model $M$ are bounded in the parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{<}\Delta$.
  However, models can often change significantly in the parameter space with little
  to no change in their predictions or accuracy on the given dataset. In this work,
  we introduce a mathematical abstraction termed <em>naturally-occurring</em> model
  change, which allows for arbitrary changes in the parameter space such that the
  change in predictions on points that lie on the data manifold is limited. Next,
  we propose a measure – that we call <em>Stability</em> – to quantify the robustness
  of counterfactuals to potential model changes for differentiable models, e.g., neural
  networks. Our main contribution is to show that counterfactuals with sufficiently
  high value of <em>Stability</em> as defined by our measure will remain valid after
  potential “naturally-occurring” model changes with high probability (leveraging
  concentration bounds for Lipschitz function of independent Gaussians). Since our
  quantification depends on the local Lipschitz constant around a data point which
  is not always available, we also examine practical relaxations of our proposed measure
  and demonstrate experimentally how they can be incorporated to find robust counterfactuals
  for neural networks that are close, realistic, and remain valid after potential
  model changes.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hamman23a
month: 0
tex_title: Robust Counterfactual Explanations for Neural Networks With Probabilistic
  Guarantees
firstpage: 12351
lastpage: 12367
page: 12351-12367
order: 12351
cycles: false
bibtex_author: Hamman, Faisal and Noorani, Erfaun and Mishra, Saumitra and Magazzeni,
  Daniele and Dutta, Sanghamitra
author:
- given: Faisal
  family: Hamman
- given: Erfaun
  family: Noorani
- given: Saumitra
  family: Mishra
- given: Daniele
  family: Magazzeni
- given: Sanghamitra
  family: Dutta
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/hamman23a/hamman23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
