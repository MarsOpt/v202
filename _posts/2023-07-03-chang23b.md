---
title: 'Muse: Text-To-Image Generation via Masked Generative Transformers'
openreview: hi9UssZdHR
abstract: 'We present Muse, a text-to-image Transformermodel that achieves state-of-the-art
  image genera-tion performance while being significantly moreefficient than diffusion
  or autoregressive models.Muse is trained on a masked modeling task indiscrete token
  space: given the text embeddingextracted from a pre-trained large language model(LLM),
  Muse learns to predict randomly maskedimage tokens. Compared to pixel-space diffusionmodels,
  such as Imagen and DALL-E 2, Muse issignificantly more efficient due to the use
  of dis-crete tokens and requires fewer sampling itera-tions; compared to autoregressive
  models such asParti, Muse is more efficient due to the use of par-allel decoding.
  The use of a pre-trained LLM en-ables fine-grained language understanding, whichtranslates
  to high-fidelity image generation andthe understanding of visual concepts such as
  ob-jects, their spatial relationships, pose, cardinalityetc. Our 900M parameter
  model achieves a newSOTA on CC3M, with an FID score of 6.06. TheMuse 3B parameter
  model achieves an FID of7.88 on zero-shot COCO evaluation, along with aCLIP score
  of 0.32. Muse also directly enables anumber of image editing applications without
  theneed to fine-tune or invert the model: inpainting,outpainting, and mask-free
  editing. More resultsand videos demonstrating editing are available at https://muse-icml.github.io/'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chang23b
month: 0
tex_title: 'Muse: Text-To-Image Generation via Masked Generative Transformers'
firstpage: 4055
lastpage: 4075
page: 4055-4075
order: 4055
cycles: false
bibtex_author: Chang, Huiwen and Zhang, Han and Barber, Jarred and Maschinot, Aaron
  and Lezama, Jose and Jiang, Lu and Yang, Ming-Hsuan and Murphy, Kevin Patrick and
  Freeman, William T. and Rubinstein, Michael and Li, Yuanzhen and Krishnan, Dilip
author:
- given: Huiwen
  family: Chang
- given: Han
  family: Zhang
- given: Jarred
  family: Barber
- given: Aaron
  family: Maschinot
- given: Jose
  family: Lezama
- given: Lu
  family: Jiang
- given: Ming-Hsuan
  family: Yang
- given: Kevin Patrick
  family: Murphy
- given: William T.
  family: Freeman
- given: Michael
  family: Rubinstein
- given: Yuanzhen
  family: Li
- given: Dilip
  family: Krishnan
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/chang23b/chang23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
