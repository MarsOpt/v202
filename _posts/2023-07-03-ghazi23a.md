---
title: On User-Level Private Convex Optimization
openreview: KfkSyUJyqg
abstract: We introduce a new mechanism for stochastic convex optimization (SCO) with
  user-level differential privacy guarantees. The convergence rates of this mechanism
  are similar to those in the prior work of Levy et al. 2021 and Narayanan et al.
  2022, but with two important improvements. Our mechanism does not require any smoothness
  assumptions on the loss. Furthermore, our bounds are also the first where the minimum
  number of users needed for user-level privacy has no dependence on the dimension
  and only a logarithmic dependence on the desired excess error. The main idea underlying
  the new mechanism is to show that the optimizers of strongly convex losses have
  low local deletion sensitivity, along with a new output perturbation method for
  functions with low local deletion sensitivity, which could be of independent interest.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ghazi23a
month: 0
tex_title: On User-Level Private Convex Optimization
firstpage: 11283
lastpage: 11299
page: 11283-11299
order: 11283
cycles: false
bibtex_author: Ghazi, Badih and Kamath, Pritish and Kumar, Ravi and Manurangsi, Pasin
  and Meka, Raghu and Zhang, Chiyuan
author:
- given: Badih
  family: Ghazi
- given: Pritish
  family: Kamath
- given: Ravi
  family: Kumar
- given: Pasin
  family: Manurangsi
- given: Raghu
  family: Meka
- given: Chiyuan
  family: Zhang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/ghazi23a/ghazi23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
