---
title: 'Tied-Augment: Controlling Representation Similarity Improves Data Augmentation'
openreview: cw6Zb0sEiT
abstract: Data augmentation methods have played an important role in the recent advance
  of deep learning models, and have become an indispensable component of state-of-the-art
  models in semi-supervised, self-supervised, and supervised training for vision.
  Despite incurring no additional latency at test time, data augmentation often requires
  more epochs of training to be effective. For example, even the simple flips-and-crops
  augmentation requires training for more than 5 epochs to improve performance, whereas
  RandAugment requires more than 90 epochs. We propose a general framework called
  Tied-Augment, which improves the efficacy of data augmentation in a wide range of
  applications by adding a simple term to the loss that can control the similarity
  of representations under distortions. Tied-Augment can improve state-of-the-art
  methods from data augmentation (e.g. RandAugment, mixup), optimization (e.g. SAM),
  and semi-supervised learning (e.g. FixMatch). For example, Tied-RandAugment can
  outperform RandAugment by 2.0% on ImageNet. Notably, using Tied-Augment, data augmentation
  can be made to improve generalization even when training for a few epochs and when
  fine-tuning. We open source our code at https://github.com/ekurtulus/tied-augment/tree/main.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kurtulus23a
month: 0
tex_title: 'Tied-Augment: Controlling Representation Similarity Improves Data Augmentation'
firstpage: 17994
lastpage: 18007
page: 17994-18007
order: 17994
cycles: false
bibtex_author: Kurtulu\c{s}, Emirhan and Li, Zichao and Dauphin, Yann and Cubuk, Ekin
  Dogus
author:
- given: Emirhan
  family: Kurtulu≈ü
- given: Zichao
  family: Li
- given: Yann
  family: Dauphin
- given: Ekin Dogus
  family: Cubuk
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kurtulus23a/kurtulus23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
