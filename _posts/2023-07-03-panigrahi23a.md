---
title: Task-Specific Skill Localization in Fine-tuned Language Models
openreview: Rgnaj43Pk0
abstract: 'Pre-trained language models can be fine-tuned to solve diverse NLP tasks,
  including in few-shot settings. Thus fine-tuning allows the model to quickly pick
  up task-specific "skills," but there has been limited study of <em>where</em> these
  newly-learnt skills reside inside the massive model. This paper introduces the term
  <em>skill localization</em> for this problem and proposes a solution. Given the
  downstream task and a model fine-tuned on that task, a simple optimization is used
  to identify a very small subset of parameters ($\sim$0.01% of model parameters)
  responsible for ($>$95%) of the modelâ€™s performance, in the sense that <em>grafting</em>
  the fine-tuned values for just this tiny subset onto the pre-trained model gives
  performance almost as well as the fine-tuned model. While reminiscent of recent
  works on parameter-efficient fine-tuning, the novel aspects here are that: (i) No
  further retraining is needed on the subset (unlike, say, with lottery tickets).
  (ii) Notable improvements are seen over vanilla fine-tuning with respect to calibration
  of predictions in-distribution (40-90% error reduction) as well as quality of predictions
  out-of-distribution (OOD). In models trained on multiple tasks, a stronger notion
  of skill localization is observed, where the sparse regions corresponding to different
  tasks are almost disjoint, and their overlap (when it happens) is a proxy for task
  similarity. Experiments suggest that localization via grafting can assist certain
  forms continual learning.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: panigrahi23a
month: 0
tex_title: Task-Specific Skill Localization in Fine-tuned Language Models
firstpage: 27011
lastpage: 27033
page: 27011-27033
order: 27011
cycles: false
bibtex_author: Panigrahi, Abhishek and Saunshi, Nikunj and Zhao, Haoyu and Arora,
  Sanjeev
author:
- given: Abhishek
  family: Panigrahi
- given: Nikunj
  family: Saunshi
- given: Haoyu
  family: Zhao
- given: Sanjeev
  family: Arora
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/panigrahi23a/panigrahi23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
