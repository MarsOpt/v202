---
title: Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for
  Convolutional Neural Networks
openreview: kNzaZ0jbIg
abstract: In deep learning, mixture-of-experts (MoE) activates one or few experts
  (sub-networks) on a per-sample or per-token basis, resulting in significant computation
  reduction. The recently proposed patch-level routing in MoE (pMoE) divides each
  input into $n$ patches (or tokens) and sends $l$ patches ($l\ll n$) to each expert
  through prioritized routing. pMoE has demonstrated great empirical success in reducing
  training and inference costs while maintaining test accuracy. However, the theoretical
  explanation of pMoE and the general MoE remains elusive. Focusing on a supervised
  classification task using a mixture of two-layer convolutional neural networks (CNNs),
  we show for the first time that pMoE provably reduces the required number of training
  samples to achieve desirable generalization (referred to as the sample complexity)
  by a factor in the polynomial order of $n/l$, and outperforms its single-expert
  counterpart of the same or even larger capacity. The advantage results from the
  discriminative routing property, which is justified in both theory and practice
  that pMoE routers can filter label-irrelevant patches and route similar class-discriminative
  patches to the same expert. Our experimental results on MNIST, CIFAR-10, and CelebA
  support our theoretical findings on pMoEâ€™s generalization and show that pMoE can
  avoid learning spurious correlations.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chowdhury23a
month: 0
tex_title: Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient
  for Convolutional Neural Networks
firstpage: 6074
lastpage: 6114
page: 6074-6114
order: 6074
cycles: false
bibtex_author: Chowdhury, Mohammed Nowaz Rabbani and Zhang, Shuai and Wang, Meng and
  Liu, Sijia and Chen, Pin-Yu
author:
- given: Mohammed Nowaz Rabbani
  family: Chowdhury
- given: Shuai
  family: Zhang
- given: Meng
  family: Wang
- given: Sijia
  family: Liu
- given: Pin-Yu
  family: Chen
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/chowdhury23a/chowdhury23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
