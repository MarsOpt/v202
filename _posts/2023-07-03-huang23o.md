---
title: Are Large Kernels Better Teachers than Transformers for ConvNets?
openreview: C0Sf2fEX4e
abstract: 'This paper reveals a new appeal of the recently emerged large-kernel Convolutional
  Neural Networks (ConvNets): as the teacher in Knowledge Distillation (KD) for small-kernel
  ConvNets. While Transformers have led state-of-the-art (SOTA) performance in various
  fields with ever-larger models and labeled data, small-kernel ConvNets are considered
  more suitable for resource-limited applications due to the efficient convolution
  operation and compact weight sharing. KD is widely used to boost the performance
  of small-kernel ConvNets. However, previous research shows that it is not quite
  effective to distill knowledge (e.g., global information) from Transformers to small-kernel
  ConvNets, presumably due to their disparate architectures. We hereby carry out a
  first-of-its-kind study unveiling that modern large-kernel ConvNets, a compelling
  competitor to Vision Transformers, are remarkably more effective teachers for small-kernel
  ConvNets, due to more similar architectures. Our findings are backed up by extensive
  experiments on both logit-level and feature-level KD "out of the box", with no dedicated
  architectural nor training recipe modifications. Notably, we obtain the <b>best-ever
  pure ConvNet</b> under 30M parameters with 83.1% top-1 accuracy on ImageNet, outperforming
  current SOTA methods including ConvNeXt V2 and Swin V2. We also find that beneficial
  characteristics of large-kernel ConvNets, e.g., larger effective receptive fields,
  can be seamlessly transferred to students through this large-to-small kernel distillation.
  Code is available at: https://github.com/VITA-Group/SLaK.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huang23o
month: 0
tex_title: Are Large Kernels Better Teachers than Transformers for {C}onv{N}ets?
firstpage: 14023
lastpage: 14038
page: 14023-14038
order: 14023
cycles: false
bibtex_author: Huang, Tianjin and Yin, Lu and Zhang, Zhenyu and Shen, Li and Fang,
  Meng and Pechenizkiy, Mykola and Wang, Zhangyang and Liu, Shiwei
author:
- given: Tianjin
  family: Huang
- given: Lu
  family: Yin
- given: Zhenyu
  family: Zhang
- given: Li
  family: Shen
- given: Meng
  family: Fang
- given: Mykola
  family: Pechenizkiy
- given: Zhangyang
  family: Wang
- given: Shiwei
  family: Liu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/huang23o/huang23o.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
