---
title: Cut your Losses with Squentropy
openreview: teYGEHBSYC
abstract: 'Nearly all practical neural models for classification are trained using
  the cross-entropy loss. Yet this ubiquitous choice is supported by little theoretical
  or empirical evidence. Recent work (Hui & Belkin, 2020) suggests that training using
  the (rescaled) square loss is often superior in terms of the classification accuracy.
  In this paper we propose the "squentropy" loss, which is the sum of two terms: the
  cross-entropy loss and the average square loss over the incorrect classes. We provide
  an extensive set of experiment on multi-class classification problems showing that
  the squentropy loss outperforms both the pure cross-entropy and rescaled square
  losses in terms of the classification accuracy. We also demonstrate that it provides
  significantly better model calibration than either of these alternative losses and,
  furthermore, has less variance with respect to the random initialization. Additionally,
  in contrast to the square loss, squentropy loss can frequently be trained using
  exactly the same optimization parameters, including the learning rate, as the standard
  cross-entropy loss, making it a true ”plug-and-play” replacement. Finally, unlike
  the rescaled square loss, multiclass squentropy contains no parameters that need
  to be adjusted.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hui23a
month: 0
tex_title: Cut your Losses with Squentropy
firstpage: 14114
lastpage: 14131
page: 14114-14131
order: 14114
cycles: false
bibtex_author: Hui, Like and Belkin, Mikhail and Wright, Stephen
author:
- given: Like
  family: Hui
- given: Mikhail
  family: Belkin
- given: Stephen
  family: Wright
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/hui23a/hui23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
