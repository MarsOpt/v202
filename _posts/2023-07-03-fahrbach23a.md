---
title: Learning Rate Schedules in the Presence of Distribution Shift
openreview: mSofpvUxCL
abstract: We design learning rate schedules that minimize regret for SGD-based online
  learning in the presence of a changing data distribution. We fully characterize
  the optimal learning rate schedule for online linear regression via a novel analysis
  with stochastic differential equations. For general convex loss functions, we propose
  new learning rate schedules that are robust to distribution shift, and give upper
  and lower bounds for the regret that only differ by constants. For non-convex loss
  functions, we define a notion of regret based on the gradient norm of the estimated
  models and propose a learning schedule that minimizes an upper bound on the total
  expected regret. Intuitively, one expects changing loss landscapes to require more
  exploration, and we confirm that optimal learning rate schedules typically have
  higher learning rates in the presence of distribution shift. Finally, we provide
  experiments that illustrate these learning rate schedules and their regret.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fahrbach23a
month: 0
tex_title: Learning Rate Schedules in the Presence of Distribution Shift
firstpage: 9523
lastpage: 9546
page: 9523-9546
order: 9523
cycles: false
bibtex_author: Fahrbach, Matthew and Javanmard, Adel and Mirrokni, Vahab and Worah,
  Pratik
author:
- given: Matthew
  family: Fahrbach
- given: Adel
  family: Javanmard
- given: Vahab
  family: Mirrokni
- given: Pratik
  family: Worah
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/fahrbach23a/fahrbach23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
