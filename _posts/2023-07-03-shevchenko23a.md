---
title: Fundamental Limits of Two-layer Autoencoders, and Achieving Them with Gradient
  Methods
openreview: eStrtvtXiN
abstract: Autoencoders are a popular model in many branches of machine learning and
  lossy data compression. However, their fundamental limits, the performance of gradient
  methods and the features learnt during optimization remain poorly understood, even
  in the two-layer setting. In fact, earlier work has considered either linear autoencoders
  or specific training regimes (leading to vanishing or diverging compression rates).
  Our paper addresses this gap by focusing on non-linear two-layer autoencoders trained
  in the challenging proportional regime in which the input dimension scales linearly
  with the size of the representation. Our results characterize the minimizers of
  the population risk, and show that such minimizers are achieved by gradient methods;
  their structure is also unveiled, thus leading to a concise description of the features
  obtained via training. For the special case of a sign activation function, our analysis
  establishes the fundamental limits for the lossy compression of Gaussian sources
  via (shallow) autoencoders. Finally, while the results are proved for Gaussian data,
  numerical simulations on standard datasets display the universality of the theoretical
  predictions.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shevchenko23a
month: 0
tex_title: Fundamental Limits of Two-layer Autoencoders, and Achieving Them with Gradient
  Methods
firstpage: 31151
lastpage: 31209
page: 31151-31209
order: 31151
cycles: false
bibtex_author: Shevchenko, Aleksandr and K\"{o}gler, Kevin and Hassani, Hamed and
  Mondelli, Marco
author:
- given: Aleksandr
  family: Shevchenko
- given: Kevin
  family: KÃ¶gler
- given: Hamed
  family: Hassani
- given: Marco
  family: Mondelli
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/shevchenko23a/shevchenko23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
