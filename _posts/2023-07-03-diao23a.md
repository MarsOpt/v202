---
title: Forward-Backward Gaussian Variational Inference via JKO in the Bures-Wasserstein
  Space
openreview: 9Kf9I2nqCh
abstract: Variational inference (VI) seeks to approximate a target distribution $\pi$
  by an element of a tractable family of distributions. Of key interest in statistics
  and machine learning is Gaussian VI, which approximates $\pi$ by minimizing the
  Kullback-Leibler (KL) divergence to $\pi$ over the space of Gaussians. In this work,
  we develop the (Stochastic) Forward-Backward Gaussian Variational Inference (FB-GVI)
  algorithm to solve Gaussian VI. Our approach exploits the composite structure of
  the KL divergence, which can be written as the sum of a smooth term (the potential)
  and a non-smooth term (the entropy) over the Bures-Wasserstein (BW) space of Gaussians
  endowed with the Wasserstein distance. For our proposed algorithm, we obtain state-of-the-art
  convergence guarantees when $\pi$ is log-smooth and log-concave, as well as the
  first convergence guarantees to first-order stationary solutions when $\pi$ is only
  log-smooth.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: diao23a
month: 0
tex_title: Forward-Backward {G}aussian Variational Inference via {JKO} in the Bures-{W}asserstein
  Space
firstpage: 7960
lastpage: 7991
page: 7960-7991
order: 7960
cycles: false
bibtex_author: Diao, Michael Ziyang and Balasubramanian, Krishna and Chewi, Sinho
  and Salim, Adil
author:
- given: Michael Ziyang
  family: Diao
- given: Krishna
  family: Balasubramanian
- given: Sinho
  family: Chewi
- given: Adil
  family: Salim
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/diao23a/diao23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
