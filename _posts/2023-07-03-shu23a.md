---
title: 'CLIPood: Generalizing CLIP to Out-of-Distributions'
openreview: DTM83ccsMA
abstract: Out-of-distribution (OOD) generalization, where the model needs to handle
  distribution shifts from training, is a major challenge of machine learning. Contrastive
  language-image pre-training (CLIP) models have shown impressive zero-shot ability,
  but the further adaptation of CLIP on downstream tasks undesirably degrades OOD
  performances. This paper aims at generalizing CLIP to out-of-distribution test data
  on downstream tasks. We propose CLIPood, a fine-tuning method that can adapt CLIP
  models to OOD situations where both domain shifts and open classes may occur on
  the unseen test data. To exploit the semantic relations between classes from the
  text modality, CLIPood introduces a new training objective, margin metric softmax
  (MMS), with class adaptive margins for fine-tuning. To incorporate both pre-trained
  zero-shot model and fine-tuned task-adaptive model, CLIPood leverages a new optimization
  strategy, Beta moving average (BMA), to maintain a temporal ensemble weighted by
  Beta distribution. Experiments on diverse datasets with different OOD scenarios
  show that CLIPood consistently outperforms existing generalization techniques.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shu23a
month: 0
tex_title: "{CLIP}ood: Generalizing {CLIP} to Out-of-Distributions"
firstpage: 31716
lastpage: 31731
page: 31716-31731
order: 31716
cycles: false
bibtex_author: Shu, Yang and Guo, Xingzhuo and Wu, Jialong and Wang, Ximei and Wang,
  Jianmin and Long, Mingsheng
author:
- given: Yang
  family: Shu
- given: Xingzhuo
  family: Guo
- given: Jialong
  family: Wu
- given: Ximei
  family: Wang
- given: Jianmin
  family: Wang
- given: Mingsheng
  family: Long
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/shu23a/shu23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
