---
title: On the Convergence of Gradient Flow on Multi-layer Linear Models
openreview: 63rNiH4mgG
abstract: 'In this paper, we analyze the convergence of gradient flow on a multi-layer
  linear model with a loss function of the form $f(W_1W_2\cdots W_L)$. We show that
  when $f$ satisfies the gradient dominance property, proper weight initialization
  leads to exponential convergence of the gradient flow to a global minimum of the
  loss. Moreover, the convergence rate depends on two trajectory-specific quantities
  that are controlled by the weight initialization: the <em>imbalance matrices</em>,
  which measure the difference between the weights of adjacent layers, and the least
  singular value of the <em>weight product</em> $W=W_1W_2\cdots W_L$. Our analysis
  exploits the fact that the gradient of the overparameterized loss can be written
  as the composition of the non-overparametrized gradient with a time-varying (weight-dependent)
  linear operator whose smallest eigenvalue controls the convergence rate. The key
  challenge we address is to derive a uniform lower bound for this time-varying eigenvalue
  that lead to improved rates for several multi-layer network models studied in the
  literature.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: min23d
month: 0
tex_title: On the Convergence of Gradient Flow on Multi-layer Linear Models
firstpage: 24850
lastpage: 24887
page: 24850-24887
order: 24850
cycles: false
bibtex_author: Min, Hancheng and Vidal, Rene and Mallada, Enrique
author:
- given: Hancheng
  family: Min
- given: Rene
  family: Vidal
- given: Enrique
  family: Mallada
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/min23d/min23d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
