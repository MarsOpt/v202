---
title: Actor-Critic Alignment for Offline-to-Online Reinforcement Learning
openreview: f6I3ZehFmu
abstract: Deep offline reinforcement learning has recently demonstrated considerable
  promises in leveraging offline datasets, providing high-quality models that significantly
  reduce the online interactions required for fine-tuning. However, such a benefit
  is often diminished due to the marked state-action distribution shift, which causes
  significant bootstrap error and wipes out the good initial policy. Existing solutions
  resort to constraining the policy shift or balancing the sample replay based on
  their online-ness. However, they require online estimation of distribution divergence
  or density ratio. To avoid such complications, we propose deviating from existing
  actor-critic approaches that directly transfer the state-action value functions.
  Instead, we post-process them by aligning with the offline learned policy, so that
  the $Q$-values for actions <em>outside</em> the offline policy are also tamed. As
  a result, the online fine-tuning can be simply performed as in the standard actor-critic
  algorithms. We show empirically that the proposed method improves the performance
  of the fine-tuned robotic agents on various simulated tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yu23k
month: 0
tex_title: Actor-Critic Alignment for Offline-to-Online Reinforcement Learning
firstpage: 40452
lastpage: 40474
page: 40452-40474
order: 40452
cycles: false
bibtex_author: Yu, Zishun and Zhang, Xinhua
author:
- given: Zishun
  family: Yu
- given: Xinhua
  family: Zhang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/yu23k/yu23k.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
