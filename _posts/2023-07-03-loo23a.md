---
title: Dataset Distillation with Convexified Implicit Gradients
openreview: D0fHlOIyhZ
abstract: We propose a new dataset distillation algorithm using reparameterization
  and convexification of implicit gradients (RCIG), that substantially improves the
  state-of-the-art. To this end, we first formulate dataset distillation as a bi-level
  optimization problem. Then, we show how implicit gradients can be effectively used
  to compute meta-gradient updates. We further equip the algorithm with a convexified
  approximation that corresponds to learning on top of a frozen finite-width neural
  tangent kernel. Finally, we improve bias in implicit gradients by parameterizing
  the neural network to enable analytical computation of final-layer parameters given
  the body parameters. RCIG establishes the new state-of-the-art on a diverse series
  of dataset distillation tasks. Notably, with one image per class, on resized ImageNet,
  RCIG sees on average a 108% improvement over the previous state-of-the-art distillation
  algorithm. Similarly, we observed a 66% gain over SOTA on Tiny-ImageNet and 37%
  on CIFAR-100.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: loo23a
month: 0
tex_title: Dataset Distillation with Convexified Implicit Gradients
firstpage: 22649
lastpage: 22674
page: 22649-22674
order: 22649
cycles: false
bibtex_author: Loo, Noel and Hasani, Ramin and Lechner, Mathias and Rus, Daniela
author:
- given: Noel
  family: Loo
- given: Ramin
  family: Hasani
- given: Mathias
  family: Lechner
- given: Daniela
  family: Rus
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/loo23a/loo23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
