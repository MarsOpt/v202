---
title: Data Representations’ Study of Latent Image Manifolds
openreview: jYHW7ueI3V
abstract: 'Deep neural networks have been demonstrated to achieve phenomenal success
  in many domains, and yet their inner mechanisms are not well understood. In this
  paper, we investigate the curvature of image manifolds, i.e., the manifold deviation
  from being flat in its principal directions. We find that state-of-the-art trained
  convolutional neural networks for image classification have a characteristic curvature
  profile along layers: an initial steep increase, followed by a long phase of a plateau,
  and followed by another increase. In contrast, this behavior does not appear in
  untrained networks in which the curvature flattens. We also show that the curvature
  gap between the last two layers has a strong correlation with the generalization
  capability of the network. Moreover, we find that the intrinsic dimension of latent
  codes is not necessarily indicative of curvature. Finally, we observe that common
  regularization methods such as mixup yield flatter representations when compared
  to other methods. Our experiments show consistent results over a variety of deep
  learning architectures and multiple data sets.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kaufman23a
month: 0
tex_title: Data Representations’ Study of Latent Image Manifolds
firstpage: 15928
lastpage: 15945
page: 15928-15945
order: 15928
cycles: false
bibtex_author: Kaufman, Ilya and Azencot, Omri
author:
- given: Ilya
  family: Kaufman
- given: Omri
  family: Azencot
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kaufman23a/kaufman23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
