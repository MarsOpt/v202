---
title: Decoding Layer Saliency in Language Transformers
openreview: hG6OSZV11v
abstract: In this paper, we introduce a strategy for identifying textual saliency
  in large-scale language models applied to classification tasks. In visual networks
  where saliency is more well-studied, saliency is naturally localized through the
  convolutional layers of the network; however, the same is not true in modern transformer-stack
  networks used to process natural language. We adapt gradient-based saliency methods
  for these networks, propose a method for evaluating the degree of semantic coherence
  of each layer, and demonstrate consistent improvement over numerous other methods
  for textual saliency on multiple benchmark classification datasets. Our approach
  requires no additional training or access to labelled data, and is comparatively
  very computationally efficient.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hou23a
month: 0
tex_title: Decoding Layer Saliency in Language Transformers
firstpage: 13285
lastpage: 13308
page: 13285-13308
order: 13285
cycles: false
bibtex_author: Hou, Elizabeth Mary and Castanon, Gregory David
author:
- given: Elizabeth Mary
  family: Hou
- given: Gregory David
  family: Castanon
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/hou23a/hou23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
