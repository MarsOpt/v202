---
title: Learning Unnormalized Statistical Models via Compositional Optimization
openreview: 1o84hNMmDd
abstract: Learning unnormalized statistical models (e.g., energy-based models) is
  computationally challenging due to the complexity of handling the partition function.
  To eschew this complexity, noise-contrastive estimation (NCE) has been proposed
  by formulating the objective as the logistic loss of the real data and the artificial
  noise. However, as found in previous works, NCE may perform poorly in many tasks
  due to its flat loss landscape and slow convergence. In this paper, we study a direct
  approach for optimizing the negative log-likelihood of unnormalized models from
  the perspective of compositional optimization. To tackle the partition function,
  a noise distribution is introduced such that the log partition function can be written
  as a compositional function whose inner function can be estimated with stochastic
  samples. Hence, the objective can be optimized by stochastic compositional optimization
  algorithms. Despite being a simple method, we demonstrate that it is more favorable
  than NCE by (1) establishing a fast convergence rate and quantifying its dependence
  on the noise distribution through the variance of stochastic estimators; (2) developing
  better results for one-dimensional Gaussian mean estimation by showing our objective
  has a much favorable loss landscape and hence our method enjoys faster convergence;
  (3) demonstrating better performance on multiple applications, including density
  estimation, out-of-distribution detection, and real image generation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jiang23g
month: 0
tex_title: Learning Unnormalized Statistical Models via Compositional Optimization
firstpage: 15105
lastpage: 15124
page: 15105-15124
order: 15105
cycles: false
bibtex_author: Jiang, Wei and Qin, Jiayu and Wu, Lingyu and Chen, Changyou and Yang,
  Tianbao and Zhang, Lijun
author:
- given: Wei
  family: Jiang
- given: Jiayu
  family: Qin
- given: Lingyu
  family: Wu
- given: Changyou
  family: Chen
- given: Tianbao
  family: Yang
- given: Lijun
  family: Zhang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/jiang23g/jiang23g.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
