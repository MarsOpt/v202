---
title: Learning Unforeseen Robustness from Out-of-distribution Data Using Equivariant
  Domain Translator
openreview: CPQW3uXIa6
abstract: Current approaches for training robust models are typically tailored to
  scenarios where data variations are accessible in the training set. While shown
  effective in achieving robustness to these foreseen variations, these approaches
  are ineffective in learning <em>unforeseen</em> robustness, i.e., robustness to
  data variations without known characterization or training examples reflecting them.
  In this work, we learn unforeseen robustness by harnessing the variations in the
  abundant out-of-distribution data. To overcome the main challenge of using such
  data, the domain gap, we use a domain translator to bridge it and bound the unforeseen
  robustness on the target distribution. As implied by our analysis, we propose a
  two-step algorithm that first trains an equivariant domain translator to map out-of-distribution
  data to the target distribution while preserving the considered variation, and then
  regularizes a modelâ€™s output consistency on the domain-translated data to improve
  its robustness. We empirically show the effectiveness of our approach in improving
  unforeseen and foreseen robustness compared to existing approaches. Additionally,
  we show that training the equivariant domain translator serves as an effective criterion
  for source data selection.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhu23a
month: 0
tex_title: Learning Unforeseen Robustness from Out-of-distribution Data Using Equivariant
  Domain Translator
firstpage: 42915
lastpage: 42937
page: 42915-42937
order: 42915
cycles: false
bibtex_author: Zhu, Sicheng and An, Bang and Huang, Furong and Hong, Sanghyun
author:
- given: Sicheng
  family: Zhu
- given: Bang
  family: An
- given: Furong
  family: Huang
- given: Sanghyun
  family: Hong
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhu23a/zhu23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
