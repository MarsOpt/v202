---
title: Fast Inference from Transformers via Speculative Decoding
openreview: C9NEblP8vS
abstract: Inference from large autoregressive models like Transformers is slow - decoding
  K tokens takes K serial runs of the model. In this work we introduce speculative
  decoding - an algorithm to sample from autoregressive models faster without any
  changes to the outputs, by computing several tokens in parallel. At the heart of
  our approach lie the observations that (1) hard language-modeling tasks often include
  easier subtasks that can be approximated well by more efficient models, and (2)
  using speculative execution and a novel sampling method, we can make exact decoding
  from the large models faster, by running them in parallel on the outputs of the
  approximation models, potentially generating several tokens concurrently, and without
  changing the distribution. Our method can accelerate existing off-the-shelf models
  without retraining or architecture changes. We demonstrate it on T5-XXL and show
  a 2X-3X acceleration compared to the standard T5X implementation, with identical
  outputs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: leviathan23a
month: 0
tex_title: Fast Inference from Transformers via Speculative Decoding
firstpage: 19274
lastpage: 19286
page: 19274-19286
order: 19274
cycles: false
bibtex_author: Leviathan, Yaniv and Kalman, Matan and Matias, Yossi
author:
- given: Yaniv
  family: Leviathan
- given: Matan
  family: Kalman
- given: Yossi
  family: Matias
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/leviathan23a/leviathan23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
