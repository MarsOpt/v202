---
title: 'KDEformer: Accelerating Transformers via Kernel Density Estimation'
openreview: 7405Po1JTk
abstract: Dot-product attention mechanism plays a crucial role in modern deep architectures
  (e.g., Transformer) for sequence modeling, however, naïve exact computation of this
  model incurs quadratic time and memory complexities in sequence length, hindering
  the training of long-sequence models. Critical bottlenecks are due to the computation
  of partition functions in the denominator of softmax function as well as the multiplication
  of the softmax matrix with the matrix of values. Our key observation is that the
  former can be reduced to a variant of the kernel density estimation (KDE) problem,
  and an efficient KDE solver can be further utilized to accelerate the latter via
  subsampling-based fast matrix products. Our proposed KDEformer can approximate the
  attention in sub-quadratic time with provable spectral norm bounds, while all prior
  results merely provide entry-wise error bounds. Empirically, we verify that KDEformer
  outperforms other attention approximations in terms of accuracy, memory, and arithmetic
  operations on various pre-trained models. For instance, on BigGAN image generation
  we achieve better generative scores than the exact computation with over 4× speedup.
  For ImageNet classification with T2T-ViT, KDEformer shows over 18× speedup while
  the accuracy drop is less than 0.5%.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zandieh23a
month: 0
tex_title: "{KDE}former: Accelerating Transformers via Kernel Density Estimation"
firstpage: 40605
lastpage: 40623
page: 40605-40623
order: 40605
cycles: false
bibtex_author: Zandieh, Amir and Han, Insu and Daliri, Majid and Karbasi, Amin
author:
- given: Amir
  family: Zandieh
- given: Insu
  family: Han
- given: Majid
  family: Daliri
- given: Amin
  family: Karbasi
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zandieh23a/zandieh23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
