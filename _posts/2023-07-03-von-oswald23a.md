---
title: Transformers Learn In-Context by Gradient Descent
openreview: tHvXrFQma5
abstract: At present, the mechanisms of in-context learning in Transformers are not
  well understood and remain mostly an intuition. In this paper, we suggest that training
  Transformers on auto-regressive objectives is closely related to gradient-based
  meta-learning formulations. We start by providing a simple weight construction that
  shows the equivalence of data transformations induced by 1) a single linear self-attention
  layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction,
  we show empirically that when training self-attention-only Transformers on simple
  regression tasks either the models learned by GD and Transformers show great similarity
  or, remarkably, the weights found by optimization match the construction. Thus we
  show how trained Transformers become mesa-optimizers i.e. learn models by gradient
  descent in their forward pass. This allows us, at least in the domain of regression
  problems, to mechanistically understand the inner workings of in-context learning
  in optimized Transformers. Building on this insight, we furthermore identify how
  Transformers surpass the performance of plain gradient descent by learning an iterative
  curvature correction and learn linear models on deep data representations to solve
  non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism
  identified to be crucial for in-context learning termed induction-head (Olsson et
  al., 2022) and show how it could be understood as a specific case of in-context
  learning by gradient descent learning within Transformers.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: von-oswald23a
month: 0
tex_title: Transformers Learn In-Context by Gradient Descent
firstpage: 35151
lastpage: 35174
page: 35151-35174
order: 35151
cycles: false
bibtex_author: Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and
  Sacramento, Joao and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov,
  Max
author:
- given: Johannes
  family: Von Oswald
- given: Eyvind
  family: Niklasson
- given: Ettore
  family: Randazzo
- given: Joao
  family: Sacramento
- given: Alexander
  family: Mordvintsev
- given: Andrey
  family: Zhmoginov
- given: Max
  family: Vladymyrov
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/von-oswald23a/von-oswald23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
