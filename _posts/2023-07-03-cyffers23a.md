---
title: From Noisy Fixed-Point Iterations to Private ADMM for Centralized and Federated
  Learning
openreview: CBLDv6SFMn
abstract: We study differentially private (DP) machine learning algorithms as instances
  of noisy fixed-point iterations, in order to derive privacy and utility results
  from this well-studied framework. We show that this new perspective recovers popular
  private gradient-based methods like DP-SGD and provides a principled way to design
  and analyze new private optimization algorithms in a flexible manner. Focusing on
  the widely-used Alternating Directions Method of Multipliers (ADMM) method, we use
  our general framework derive novel private ADMM algorithms for centralized, federated
  and fully decentralized learning. We establish strong privacy guarantees for these
  algorithms, leveraging privacy amplification by iteration and by subsampling. Finally,
  we provide utility guarantees for the three algorithms using a unified analysis
  that exploits a recent linear convergence result for noisy fixed-point iterations.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cyffers23a
month: 0
tex_title: From Noisy Fixed-Point Iterations to Private {ADMM} for Centralized and
  Federated Learning
firstpage: 6683
lastpage: 6711
page: 6683-6711
order: 6683
cycles: false
bibtex_author: Cyffers, Edwige and Bellet, Aur\'{e}lien and Basu, Debabrota
author:
- given: Edwige
  family: Cyffers
- given: Aur√©lien
  family: Bellet
- given: Debabrota
  family: Basu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/cyffers23a/cyffers23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
