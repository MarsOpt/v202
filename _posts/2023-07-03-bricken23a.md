---
title: Emergence of Sparse Representations from Noise
openreview: cxYaBAXVKg
abstract: 'A hallmark of biological neural networks, which distinguishes them from
  their artificial counterparts, is the high degree of sparsity in their activations.
  This discrepancy raises three questions our work helps to answer: (i) Why are biological
  networks so sparse? (ii) What are the benefits of this sparsity? (iii) How can these
  benefits be utilized by deep learning models? Our answers to all of these questions
  center around training networks to handle random noise. Surprisingly, we discover
  that noisy training introduces three implicit loss terms that result in sparsely
  firing neurons specializing to high variance features of the dataset. When trained
  to reconstruct noisy-CIFAR10, neurons learn biological receptive fields. More broadly,
  noisy training presents a new approach to potentially increase model interpretability
  with additional benefits to robustness and computational efficiency.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bricken23a
month: 0
tex_title: Emergence of Sparse Representations from Noise
firstpage: 3148
lastpage: 3191
page: 3148-3191
order: 3148
cycles: false
bibtex_author: Bricken, Trenton and Schaeffer, Rylan and Olshausen, Bruno and Kreiman,
  Gabriel
author:
- given: Trenton
  family: Bricken
- given: Rylan
  family: Schaeffer
- given: Bruno
  family: Olshausen
- given: Gabriel
  family: Kreiman
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/bricken23a/bricken23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
