---
title: 'SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot'
openreview: gsP05g8IeK
abstract: 'We show for the first time that large-scale generative pretrained transformer
  (GPT) family models can be pruned to at least 50% sparsity in <em>one-shot, without
  any retraining</em>, at minimal loss of accuracy. This is achieved via a new pruning
  method called SparseGPT, specifically designed to work efficiently and accurately
  on massive GPT-family models. We can execute SparseGPT on the largest available
  open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60%
  unstructured sparsity with negligible increase in perplexity: remarkably, more than
  100 billion weights from these models can be ignored at inference time. SparseGPT
  generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight
  quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: frantar23a
month: 0
tex_title: "{S}parse{GPT}: Massive Language Models Can be Accurately Pruned in One-Shot"
firstpage: 10323
lastpage: 10337
page: 10323-10337
order: 10323
cycles: false
bibtex_author: Frantar, Elias and Alistarh, Dan
author:
- given: Elias
  family: Frantar
- given: Dan
  family: Alistarh
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/frantar23a/frantar23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
