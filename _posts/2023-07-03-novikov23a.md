---
title: 'Few-bit Backward: Quantized Gradients of Activation Functions for Memory Footprint
  Reduction'
openreview: m2S96Qf2R3
abstract: Memory footprint is one of the main limiting factors for large neural network
  training. In backpropagation, one needs to store the input to each operation in
  the computational graph. Every modern neural network model has quite a few pointwise
  nonlinearities in its architecture, and such operations induce additional memory
  costs that, as we show, can be significantly reduced by quantization of the gradients.
  We propose a systematic approach to compute optimal quantization of the retained
  gradients of the pointwise nonlinear functions with only a few bits per each element.
  We show that such approximation can be achieved by computing an optimal piecewise-constant
  approximation of the derivative of the activation function, which can be done by
  dynamic programming. The drop-in replacements are implemented for all popular nonlinearities
  and can be used in any existing pipeline. We confirm the memory reduction and the
  same convergence on several open benchmarks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: novikov23a
month: 0
tex_title: 'Few-bit Backward: Quantized Gradients of Activation Functions for Memory
  Footprint Reduction'
firstpage: 26363
lastpage: 26381
page: 26363-26381
order: 26363
cycles: false
bibtex_author: Novikov, Georgii Sergeevich and Bershatsky, Daniel and Gusak, Julia
  and Shonenkov, Alex and Dimitrov, Denis Valerievich and Oseledets, Ivan
author:
- given: Georgii Sergeevich
  family: Novikov
- given: Daniel
  family: Bershatsky
- given: Julia
  family: Gusak
- given: Alex
  family: Shonenkov
- given: Denis Valerievich
  family: Dimitrov
- given: Ivan
  family: Oseledets
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/novikov23a/novikov23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
