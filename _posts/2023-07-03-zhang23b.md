---
title: Learning useful representations for shifting tasks and distributions
openreview: 85G1qtikHO
abstract: 'Does the dominant approach to learn representations (as a side effect of
  optimizing an expected cost for a single training distribution) remain a good approach
  when we are dealing with multiple distributions? Our thesis is that <em>such scenarios
  are better served by representations that are richer than those obtained with a
  single optimization episode.</em> We support this thesis with simple theoretical
  arguments and with experiments utilizing an apparently n√§ive ensembling technique:
  concatenating the representations obtained from multiple training episodes using
  the same data, model, algorithm, and hyper-parameters, but different random seeds.
  These independently trained networks perform similarly. Yet, in a number of scenarios
  involving new distributions, the concatenated representation performs substantially
  better than an equivalently sized network trained with a single training run. This
  proves that the representations constructed by multiple training episodes are in
  fact different. Although their concatenation carries little additional information
  about the training task under the training distribution, it becomes substantially
  more informative when tasks or distributions change. Meanwhile, a single training
  episode is unlikely to yield such a redundant representation because the optimization
  process has no reason to accumulate features that do not incrementally improve the
  training performance.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang23b
month: 0
tex_title: Learning useful representations for shifting tasks and distributions
firstpage: 40830
lastpage: 40850
page: 40830-40850
order: 40830
cycles: false
bibtex_author: Zhang, Jianyu and Bottou, Leon
author:
- given: Jianyu
  family: Zhang
- given: Leon
  family: Bottou
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhang23b/zhang23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
