---
title: Polynomial Time and Private Learning of Unbounded Gaussian Mixture Models
openreview: b6Hxt4Jw10
abstract: We study the problem of privately estimating the parameters of $d$-dimensional
  Gaussian Mixture Models (GMMs) with $k$ components. For this, we develop a technique
  to reduce the problem to its non-private counterpart. This allows us to privatize
  existing non-private algorithms in a blackbox manner, while incurring only a small
  overhead in the sample complexity and running time. As the main application of our
  framework, we develop an $(\varepsilon, \delta)$-differentially private algorithm
  to learn GMMs using the non-private algorithm of Moitra and Valiant (2010) as a
  blackbox. Consequently, this gives the first sample complexity upper bound and first
  polynomial time algorithm for privately learning GMMs without any boundedness assumptions
  on the parameters. As part of our analysis, we prove a tight (up to a constant factor)
  lower bound on the total variation distance of high-dimensional Gaussians which
  can be of independent interest.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: arbas23a
month: 0
tex_title: Polynomial Time and Private Learning of Unbounded {G}aussian Mixture Models
firstpage: 1018
lastpage: 1040
page: 1018-1040
order: 1018
cycles: false
bibtex_author: Arbas, Jamil and Ashtiani, Hassan and Liaw, Christopher
author:
- given: Jamil
  family: Arbas
- given: Hassan
  family: Ashtiani
- given: Christopher
  family: Liaw
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/arbas23a/arbas23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
