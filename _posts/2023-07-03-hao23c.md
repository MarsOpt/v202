---
title: 'GNOT: A General Neural Operator Transformer for Operator Learning'
openreview: JomvpMQ6NF
abstract: Learning partial differential equations’ (PDEs) solution operators is an
  essential problem in machine learning. However, there are several challenges for
  learning operators in practical applications like the irregular mesh, multiple input
  functions, and complexity of the PDEs’ solution. To address these challenges, we
  propose a general neural operator transformer (GNOT), a scalable and effective transformer-based
  framework for learning operators. By designing a novel heterogeneous normalized
  attention layer, our model is highly flexible to handle multiple input functions
  and irregular meshes. Besides, we introduce a geometric gating mechanism which could
  be viewed as a soft domain decomposition to solve the multi-scale problems. The
  large model capacity of the transformer architecture grants our model the possibility
  to scale to large datasets and practical problems. We conduct extensive experiments
  on multiple challenging datasets from different domains and achieve a remarkable
  improvement compared with alternative methods. Our code and data are publicly available
  at https://github.com/thu-ml/GNOT.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hao23c
month: 0
tex_title: "{GNOT}: A General Neural Operator Transformer for Operator Learning"
firstpage: 12556
lastpage: 12569
page: 12556-12569
order: 12556
cycles: false
bibtex_author: Hao, Zhongkai and Wang, Zhengyi and Su, Hang and Ying, Chengyang and
  Dong, Yinpeng and Liu, Songming and Cheng, Ze and Song, Jian and Zhu, Jun
author:
- given: Zhongkai
  family: Hao
- given: Zhengyi
  family: Wang
- given: Hang
  family: Su
- given: Chengyang
  family: Ying
- given: Yinpeng
  family: Dong
- given: Songming
  family: Liu
- given: Ze
  family: Cheng
- given: Jian
  family: Song
- given: Jun
  family: Zhu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/hao23c/hao23c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
