---
title: Approximation and Estimation Ability of Transformers for Sequence-to-Sequence
  Functions with Infinite Dimensional Input
openreview: DCjUPvimM2
abstract: Despite the great success of Transformer networks in various applications
  such as natural language processing and computer vision, their theoretical aspects
  are not well understood. In this paper, we study the approximation and estimation
  ability of Transformers as sequence-to-sequence functions with infinite dimensional
  inputs. Although inputs and outputs are both infinite dimensional, we show that
  when the target function has anisotropic smoothness, Transformers can avoid the
  curse of dimensionality due to their feature extraction ability and parameter sharing
  property. In addition, we show that even if the smoothness changes depending on
  each input, Transformers can estimate the importance of features for each input
  and extract important features dynamically. Then, we proved that Transformers achieve
  similar convergence rate as in the case of the fixed smoothness. Our theoretical
  results support the practical success of Transformers for high dimensional data.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: takakura23a
month: 0
tex_title: Approximation and Estimation Ability of Transformers for Sequence-to-Sequence
  Functions with Infinite Dimensional Input
firstpage: 33416
lastpage: 33447
page: 33416-33447
order: 33416
cycles: false
bibtex_author: Takakura, Shokichi and Suzuki, Taiji
author:
- given: Shokichi
  family: Takakura
- given: Taiji
  family: Suzuki
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/takakura23a/takakura23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
