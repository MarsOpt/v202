---
title: Text-To-Concept (and Back) via Cross-Model Alignment
openreview: C6zz7ivXyM
abstract: We observe that the mapping between an image’s representation in one model
  to its representation in another can be learned surprisingly well with just a linear
  layer, even across diverse models. Building on this observation, we propose <em>text-to-concept</em>,
  where features from a fixed pretrained model are aligned linearly to the CLIP space,
  so that text embeddings from CLIP’s text encoder become directly comparable to the
  aligned features. With text-to-concept, we convert fixed off-the-shelf vision encoders
  to surprisingly strong zero-shot classifiers for free, with accuracy at times even
  surpassing that of CLIP, despite being much smaller models and trained on a small
  fraction of the data compared to CLIP. We show other immediate use-cases of text-to-concept,
  like building concept bottleneck models with no concept supervision, diagnosing
  distribution shifts in terms of human concepts, and retrieving images satisfying
  a set of text-based constraints. Lastly, we demonstrate the feasibility of <em>concept-to-text</em>,
  where vectors in a model’s feature space are decoded by first aligning to the CLIP
  before being fed to a GPT-based generative model. Our work suggests existing deep
  models, with presumably diverse architectures and training, represent input samples
  relatively similarly, and a two-way communication across model representation spaces
  and to humans (through language) is viable.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: moayeri23a
month: 0
tex_title: Text-To-Concept (and Back) via Cross-Model Alignment
firstpage: 25037
lastpage: 25060
page: 25037-25060
order: 25037
cycles: false
bibtex_author: Moayeri, Mazda and Rezaei, Keivan and Sanjabi, Maziar and Feizi, Soheil
author:
- given: Mazda
  family: Moayeri
- given: Keivan
  family: Rezaei
- given: Maziar
  family: Sanjabi
- given: Soheil
  family: Feizi
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/moayeri23a/moayeri23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
