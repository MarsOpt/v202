---
title: A Closer Look at Self-Supervised Lightweight Vision Transformers
openreview: UwEMFweZmC
abstract: Self-supervised learning on large-scale Vision Transformers (ViTs) as pre-training
  methods has achieved promising downstream performance. Yet, how much these pre-training
  paradigms promote lightweight ViTsâ€™ performance is considerably less studied. In
  this work, we develop and benchmark several self-supervised pre-training methods
  on image classification tasks and some downstream dense prediction tasks. We surprisingly
  find that if proper pre-training is adopted, even vanilla lightweight ViTs show
  comparable performance to previous SOTA networks with delicate architecture design.
  It breaks the recently popular conception that vanilla ViTs are not suitable for
  vision tasks in lightweight regimes. We also point out some defects of such pre-training,
  e.g., failing to benefit from large-scale pre-training data and showing inferior
  performance on data-insufficient downstream tasks. Furthermore, we analyze and clearly
  show the effect of such pre-training by analyzing the properties of the layer representation
  and attention maps for related models. Finally, based on the above analyses, a distillation
  strategy during pre-training is developed, which leads to further downstream performance
  improvement for MAE-based pre-training. Code is available at https://github.com/wangsr126/mae-lite.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23e
month: 0
tex_title: A Closer Look at Self-Supervised Lightweight Vision Transformers
firstpage: 35624
lastpage: 35641
page: 35624-35641
order: 35624
cycles: false
bibtex_author: Wang, Shaoru and Gao, Jin and Li, Zeming and Zhang, Xiaoqin and Hu,
  Weiming
author:
- given: Shaoru
  family: Wang
- given: Jin
  family: Gao
- given: Zeming
  family: Li
- given: Xiaoqin
  family: Zhang
- given: Weiming
  family: Hu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wang23e/wang23e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
