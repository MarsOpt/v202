---
title: Hardware-Aware Compression with Random Operation Access Specific Tile (ROAST)
  Hashing
openreview: kvvHeldUfw
abstract: 'Advancements in deep learning are often associated with increasing model
  sizes. Training and deploying large models require sophisticated hardware and incur
  significantly higher costs. Thus, model compression is a widely explored approach
  to solving the problem. However, SOTA techniques fall short in one or more desirable
  aspects of compression - for instance, pruning does not reduce memory for training,
  quantization can only provide up to 32$\times$ compression, HashedNet is cache-inefficient,
  etc. This paper proposes a model-agnostic, cache-friendly, and hardware-aware model
  compression approach: Random Operation Access Specific Tile (ROAST) hashing. ROAST
  collapses the parameters by clubbing them through a lightweight mapping. While clubbing
  these parameters, ROAST utilizes cache hierarchies by aligning the memory access
  pattern with the parameter access pattern. ROAST is up to ${\sim}25\times$ faster
  to train and ${\sim}50\times$ faster to infer than the popular parameter sharing
  method HashedNet. Additionally, ROAST introduces global weight sharing, which is
  empirically and theoretically superior to local weight sharing in HashedNet, and
  can be of independent interest. With ROAST, we can efficiently train and deploy
  the model using a much smaller memory footprint ($\sim 10 - 100\times$ lesser) in
  text and image classification tasks. ROAST-MM kernel implementation is open-source
  (https://github.com/apd10/RzLinear/tree/stable)'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: desai23b
month: 0
tex_title: Hardware-Aware Compression with Random Operation Access Specific Tile ({ROAST})
  Hashing
firstpage: 7732
lastpage: 7749
page: 7732-7749
order: 7732
cycles: false
bibtex_author: Desai, Aditya and Zhou, Keren and Shrivastava, Anshumali
author:
- given: Aditya
  family: Desai
- given: Keren
  family: Zhou
- given: Anshumali
  family: Shrivastava
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/desai23b/desai23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
