---
title: Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive
  Concepts
openreview: UkG4Nn634P
abstract: In this paper, we focus on mean-field variational Bayesian Neural Networks
  (BNNs) and explore the representation capacity of such BNNs by investigating which
  types of concepts are less likely to be encoded by the BNN. It has been observed
  and studied that a relatively small set of interactive concepts usually emerge in
  the knowledge representation of a sufficiently-trained neural network, and such
  concepts can faithfully explain the network output. Based on this, our study proves
  that compared to standard deep neural networks (DNNs), it is less likely for BNNs
  to encode complex concepts. Experiments verify our theoretical proofs. Note that
  the tendency to encode less complex concepts does not necessarily imply weak representation
  power, considering that complex concepts exhibit low generalization power and high
  adversarial vulnerability. The code is available at https://github.com/sjtu-xai-lab/BNN-concepts.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ren23a
month: 0
tex_title: "{B}ayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive
  Concepts"
firstpage: 28889
lastpage: 28913
page: 28889-28913
order: 28889
cycles: false
bibtex_author: Ren, Qihan and Deng, Huiqi and Chen, Yunuo and Lou, Siyu and Zhang,
  Quanshi
author:
- given: Qihan
  family: Ren
- given: Huiqi
  family: Deng
- given: Yunuo
  family: Chen
- given: Siyu
  family: Lou
- given: Quanshi
  family: Zhang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/ren23a/ren23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
