---
title: 'Exphormer: Sparse Transformers for Graphs'
openreview: 3Ge74dgjjU
abstract: 'Graph transformers have emerged as a promising architecture for a variety
  of graph learning and representation tasks. Despite their successes, though, it
  remains challenging to scale graph transformers to large graphs while maintaining
  accuracy competitive with message-passing networks. In this paper, we introduce
  Exphormer, a framework for building powerful and scalable graph transformers. Exphormer
  consists of a sparse attention mechanism based on two mechanisms: virtual global
  nodes and expander graphs, whose mathematical characteristics, such as spectral
  expansion, pseduorandomness, and sparsity, yield graph transformers with complexity
  only linear in the size of the graph, while allowing us to prove desirable theoretical
  properties of the resulting transformer models. We show that incorporating Exphormer
  into the recently-proposed GraphGPS framework produces models with competitive empirical
  results on a wide variety of graph datasets, including state-of-the-art results
  on three datasets. We also show that Exphormer can scale to datasets on larger graphs
  than shown in previous graph transformer architectures.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shirzad23a
month: 0
tex_title: 'Exphormer: Sparse Transformers for Graphs'
firstpage: 31613
lastpage: 31632
page: 31613-31632
order: 31613
cycles: false
bibtex_author: Shirzad, Hamed and Velingker, Ameya and Venkatachalam, Balaji and Sutherland,
  Danica J. and Sinop, Ali Kemal
author:
- given: Hamed
  family: Shirzad
- given: Ameya
  family: Velingker
- given: Balaji
  family: Venkatachalam
- given: Danica J.
  family: Sutherland
- given: Ali Kemal
  family: Sinop
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/shirzad23a/shirzad23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
