---
title: 'TAN Without a Burn: Scaling Laws of DP-SGD'
openreview: DIkGgI9baJ
abstract: Differentially Private methods for training Deep Neural Networks (DNNs)
  have progressed recently, in particular with the use of massive batches and aggregated
  data augmentations for a large number of training steps. These techniques require
  much more computing resources than their non-private counterparts, shifting the
  traditional privacy-accuracy trade-off to a privacy-accuracy-compute trade-off and
  making hyper-parameter search virtually impossible for realistic scenarios. In this
  work, we decouple privacy analysis and experimental behavior of noisy training to
  explore the trade-off with minimal computational requirements. We first use the
  tools of Renyi Differential Privacy (RDP) to highlight that the privacy budget,
  when not overcharged, only depends on the total amount of noise (TAN) injected throughout
  training. We then derive scaling laws for training models with DP-SGD to optimize
  hyper-parameters with more than a $100\times$ reduction in computational budget.
  We apply the proposed method on CIFAR-10 and ImageNet and, in particular, strongly
  improve the state-of-the-art on ImageNet with a $+9$ points gain in top-1 accuracy
  for a privacy budget $\varepsilon=8$.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sander23b
month: 0
tex_title: "{TAN} Without a Burn: Scaling Laws of {DP}-{SGD}"
firstpage: 29937
lastpage: 29949
page: 29937-29949
order: 29937
cycles: false
bibtex_author: Sander, Tom and Stock, Pierre and Sablayrolles, Alexandre
author:
- given: Tom
  family: Sander
- given: Pierre
  family: Stock
- given: Alexandre
  family: Sablayrolles
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/sander23b/sander23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
