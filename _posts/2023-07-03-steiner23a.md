---
title: 'MODeL: Memory Optimizations for Deep Learning'
openreview: 9v29agPZkj
abstract: The size of deep neural networks has grown exponentially in recent years.
  Unfortunately, hardware devices have not kept pace with the rapidly increasing memory
  requirements. To cope with this, researchers have proposed various techniques including
  spilling, rematerialization, reduced precision training, model pruning, and so on.
  However, these approaches suffer from various limitations, such as increasing training
  time, affecting model accuracy, or requiring extensive manual modifications to the
  neural networks. We present MODeL, an algorithm that optimizes the lifetime and
  memory location of the tensors used to train neural networks. Our method automatically
  reduces the memory usage of existing neural networks without any of the drawbacks
  of other techniques. We formulate the problem as a joint integer linear program
  (ILP). We present several techniques to simplify the encoding of the problem, and
  enable our approach to scale to the size of state-of-the-art neural networks using
  an off-the-shelf ILP solver. We experimentally demonstrate that MODeL only takes
  seconds to allow the training of neural networks using 30% less memory on average.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: steiner23a
month: 0
tex_title: "{MOD}e{L}: Memory Optimizations for Deep Learning"
firstpage: 32618
lastpage: 32632
page: 32618-32632
order: 32618
cycles: false
bibtex_author: Steiner, Benoit and Elhoushi, Mostafa and Kahn, Jacob and Hegarty,
  James
author:
- given: Benoit
  family: Steiner
- given: Mostafa
  family: Elhoushi
- given: Jacob
  family: Kahn
- given: James
  family: Hegarty
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/steiner23a/steiner23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
