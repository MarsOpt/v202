---
title: Why Is Public Pretraining Necessary for Private Model Training?
openreview: 1d3O0b1rbL
abstract: In the privacy-utility tradeoff of a model trained on benchmark language
  and vision tasks, remarkable improvements have been widely reported when the model
  is pretrained on public data. Some gain is expected as these models inherit the
  benefits of transfer learning, which is the standard motivation in non-private settings.
  However, the stark contrast in the gain of pretraining between non-private and private
  machine learning suggests that the gain in the latter is rooted in a fundamentally
  different cause. To explain this phenomenon, we hypothesize that the non-convex
  loss landscape of a model training necessitates the optimization algorithm to go
  through two phases. In the first, the algorithm needs to select a good “basin” in
  the loss landscape. In the second, the algorithm solves an easy optimization within
  that basin. The former is a harder problem to solve with private data, while the
  latter is harder to solve with public data due to a distribution shift or data scarcity.
  Guided by this intuition, we provide theoretical constructions that provably demonstrate
  the separation between private training with and without public pretraining. Further,
  systematic experiments on CIFAR10 and Librispeech provide supporting evidence for
  our hypothesis.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ganesh23a
month: 0
tex_title: Why Is Public Pretraining Necessary for Private Model Training?
firstpage: 10611
lastpage: 10627
page: 10611-10627
order: 10611
cycles: false
bibtex_author: Ganesh, Arun and Haghifam, Mahdi and Nasr, Milad and Oh, Sewoong and
  Steinke, Thomas and Thakkar, Om and Guha Thakurta, Abhradeep and Wang, Lun
author:
- given: Arun
  family: Ganesh
- given: Mahdi
  family: Haghifam
- given: Milad
  family: Nasr
- given: Sewoong
  family: Oh
- given: Thomas
  family: Steinke
- given: Om
  family: Thakkar
- given: Abhradeep
  family: Guha Thakurta
- given: Lun
  family: Wang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/ganesh23a/ganesh23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
