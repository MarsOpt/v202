---
title: Can Neural Network Memorization Be Localized?
openreview: Pbaiy3fRCt
abstract: 'Recent efforts at explaining the interplay of memorization and generalization
  in deep overparametrized networks have posited that neural networks <em>memorize</em>
  “hard” examples in the final few layers of the model. Memorization refers to the
  ability to correctly predict on <em>atypical</em> examples of the training set.
  In this work, we show that rather than being confined to individual layers, memorization
  is a phenomenon confined to a small set of neurons in various layers of the model.
  First, via three experimental sources of converging evidence, we find that most
  layers are redundant for the memorization of examples and the layers that contribute
  to example memorization are, in general, not the final layers. The three sources
  are <em>gradient accounting</em> (measuring the contribution to the gradient norms
  from memorized and clean examples), <em>layer rewinding</em> (replacing specific
  model weights of a converged model with previous training checkpoints), and <em>retraining</em>
  (training rewound layers only on clean examples). Second, we ask a more generic
  question: can memorization be localized <em>anywhere</em> in a model? We discover
  that memorization is often confined to a small number of neurons or channels (around
  5) of the model. Based on these insights we propose a new form of dropout—<em>example-tied
  dropout</em> that enables us to direct the memorization of examples to an aprior
  determined set of neurons. By dropping out these neurons, we are able to reduce
  the accuracy on memorized examples from 100% to 3%, while also reducing the generalization
  gap.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: maini23a
month: 0
tex_title: Can Neural Network Memorization Be Localized?
firstpage: 23536
lastpage: 23557
page: 23536-23557
order: 23536
cycles: false
bibtex_author: Maini, Pratyush and Mozer, Michael Curtis and Sedghi, Hanie and Lipton,
  Zachary Chase and Kolter, J Zico and Zhang, Chiyuan
author:
- given: Pratyush
  family: Maini
- given: Michael Curtis
  family: Mozer
- given: Hanie
  family: Sedghi
- given: Zachary Chase
  family: Lipton
- given: J Zico
  family: Kolter
- given: Chiyuan
  family: Zhang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/maini23a/maini23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
