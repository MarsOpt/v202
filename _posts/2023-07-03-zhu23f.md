---
title: Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise
  Comparisons
openreview: JI6K8sYUxP
abstract: We provide a theoretical framework for Reinforcement Learning with Human
  Feedback (RLHF). We show that when the underlying true reward is linear, under both
  Bradley-Terry-Luce (BTL) model (pairwise comparison) and Plackett-Luce (PL) model
  ($K$-wise comparison), MLE converges under certain semi-norm for the family of linear
  reward. On the other hand, when training a policy based on the learned reward model,
  we show that MLE fails while a pessimistic MLE provides policies with good performance
  under certain coverage assumption. We also show that under the PL model, both the
  true MLE and a different MLE which splits the $K$-wise comparison into pairwise
  comparisons converge, while the true MLE is asymptotically more efficient. Our results
  validate the empirical success of the existing RLHF algorithms, and provide new
  insights for algorithm design. Our analysis can also be applied for the problem
  of online RLHF and inverse reinforcement learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhu23f
month: 0
tex_title: Principled Reinforcement Learning with Human Feedback from Pairwise or
  K-wise Comparisons
firstpage: 43037
lastpage: 43067
page: 43037-43067
order: 43037
cycles: false
bibtex_author: Zhu, Banghua and Jordan, Michael and Jiao, Jiantao
author:
- given: Banghua
  family: Zhu
- given: Michael
  family: Jordan
- given: Jiantao
  family: Jiao
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhu23f/zhu23f.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
