---
title: 'DoCoFL: Downlink Compression for Cross-Device Federated Learning'
openreview: VxKr51JjWC
abstract: Many compression techniques have been proposed to reduce the communication
  overhead of Federated Learning training procedures. However, these are typically
  designed for compressing model updates, which are expected to decay throughout training.
  As a result, such methods are inapplicable to downlink (i.e., from the parameter
  server to clients) compression in the cross-device setting, where heterogeneous
  clients <em>may appear only once</em> during training and thus must download the
  model parameters. Accordingly, we propose DoCoFL â€“ a new framework for downlink
  compression in the cross-device setting. Importantly, DoCoFL can be seamlessly combined
  with many uplink compression schemes, rendering it suitable for bi-directional compression.
  Through extensive evaluation, we show that DoCoFL offers significant bi-directional
  bandwidth reduction while achieving competitive accuracy to that of a baseline without
  any compression.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dorfman23a
month: 0
tex_title: "{D}o{C}o{FL}: Downlink Compression for Cross-Device Federated Learning"
firstpage: 8356
lastpage: 8388
page: 8356-8388
order: 8356
cycles: false
bibtex_author: Dorfman, Ron and Vargaftik, Shay and Ben-Itzhak, Yaniv and Levy, Kfir
  Yehuda
author:
- given: Ron
  family: Dorfman
- given: Shay
  family: Vargaftik
- given: Yaniv
  family: Ben-Itzhak
- given: Kfir Yehuda
  family: Levy
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/dorfman23a/dorfman23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
