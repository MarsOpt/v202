---
title: On the Global Convergence of Risk-Averse Policy Gradient Methods with Expected
  Conditional Risk Measures
openreview: ksE1uptwtM
abstract: Risk-sensitive reinforcement learning (RL) has become a popular tool to
  control the risk of uncertain outcomes and ensure reliable performance in various
  sequential decision-making problems. While policy gradient methods have been developed
  for risk-sensitive RL, it remains unclear if these methods enjoy the same global
  convergence guarantees as in the risk-neutral case. In this paper, we consider a
  class of dynamic time-consistent risk measures, called Expected Conditional Risk
  Measures (ECRMs), and derive policy gradient updates for ECRM-based objective functions.
  Under both constrained direct parameterization and unconstrained softmax parameterization,
  we provide global convergence and iteration complexities of the corresponding risk-averse
  policy gradient algorithms. We further test risk-averse variants of REINFORCE and
  actor-critic algorithms to demonstrate the efficacy of our method and the importance
  of risk control.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yu23j
month: 0
tex_title: On the Global Convergence of Risk-Averse Policy Gradient Methods with Expected
  Conditional Risk Measures
firstpage: 40425
lastpage: 40451
page: 40425-40451
order: 40425
cycles: false
bibtex_author: Yu, Xian and Ying, Lei
author:
- given: Xian
  family: Yu
- given: Lei
  family: Ying
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/yu23j/yu23j.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
