---
title: 'Straightening Out the Straight-Through Estimator: Overcoming Optimization
  Challenges in Vector Quantized Networks'
openreview: LJn8O3FcTN
abstract: This work examines the challenges of training neural networks using vector
  quantization using straight-through estimation. We find that the main cause of training
  instability is the discrepancy between the model embedding and the code-vector distribution.
  We identify the factors that contribute to this issue, including the codebook gradient
  sparsity and the asymmetric nature of the commitment loss, which leads to misaligned
  code-vector assignments. We propose to address this issue via affine re-parameterization
  of the code vectors. Additionally, we introduce an alternating optimization to reduce
  the gradient error introduced by the straight-through estimation. Moreover, we propose
  an improvement to the commitment loss to ensure better alignment between the codebook
  representation and the model embedding. These optimization methods improve the mathematical
  approximation of the straight-through estimation and, ultimately, the model performance.
  We demonstrate the effectiveness of our methods on several common model architectures,
  such as AlexNet, ResNet, and ViT, across various tasks, including image classification
  and generative modeling.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huh23a
month: 0
tex_title: 'Straightening Out the Straight-Through Estimator: Overcoming Optimization
  Challenges in Vector Quantized Networks'
firstpage: 14096
lastpage: 14113
page: 14096-14113
order: 14096
cycles: false
bibtex_author: Huh, Minyoung and Cheung, Brian and Agrawal, Pulkit and Isola, Phillip
author:
- given: Minyoung
  family: Huh
- given: Brian
  family: Cheung
- given: Pulkit
  family: Agrawal
- given: Phillip
  family: Isola
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/huh23a/huh23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
