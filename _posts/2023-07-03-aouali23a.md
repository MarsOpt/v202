---
title: Exponential Smoothing for Off-Policy Learning
openreview: LJ9iKElXpl
abstract: Off-policy learning (OPL) aims at finding improved policies from logged
  bandit data, often by minimizing the inverse propensity scoring (IPS) estimator
  of the risk. In this work, we investigate a smooth regularization for IPS, for which
  we derive a two-sided PAC-Bayes generalization bound. The bound is tractable, scalable,
  interpretable and provides learning certificates. In particular, it is also valid
  for standard IPS without making the assumption that the importance weights are bounded.
  We demonstrate the relevance of our approach and its favorable performance through
  a set of learning tasks. Since our bound holds for standard IPS, we are able to
  provide insight into when regularizing IPS is useful. Namely, we identify cases
  where regularization might not be needed. This goes against the belief that, in
  practice, clipped IPS often enjoys favorable performance than standard IPS in OPL.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: aouali23a
month: 0
tex_title: Exponential Smoothing for Off-Policy Learning
firstpage: 984
lastpage: 1017
page: 984-1017
order: 984
cycles: false
bibtex_author: Aouali, Imad and Brunel, Victor-Emmanuel and Rohde, David and Korba,
  Anna
author:
- given: Imad
  family: Aouali
- given: Victor-Emmanuel
  family: Brunel
- given: David
  family: Rohde
- given: Anna
  family: Korba
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/aouali23a/aouali23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
