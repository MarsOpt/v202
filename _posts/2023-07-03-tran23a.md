---
title: Fully Bayesian Autoencoders with Latent Sparse Gaussian Processes
openreview: cq5IwsiW5Z
abstract: We present a fully Bayesian autoencoder model that treats both local latent
  variables and global decoder parameters in a Bayesian fashion. This approach allows
  for flexible priors and posterior approximations while keeping the inference costs
  low. To achieve this, we introduce an amortized MCMC approach by utilizing an implicit
  stochastic network to learn sampling from the posterior over local latent variables.
  Furthermore, we extend the model by incorporating a Sparse Gaussian Process prior
  over the latent space, allowing for a fully Bayesian treatment of inducing points
  and kernel hyperparameters and leading to improved scalability. Additionally, we
  enable Deep Gaussian Process priors on the latent space and the handling of missing
  data. We evaluate our model on a range of experiments focusing on dynamic representation
  learning and generative modeling, demonstrating the strong performance of our approach
  in comparison to existing methods that combine Gaussian Processes and autoencoders.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tran23a
month: 0
tex_title: Fully {B}ayesian Autoencoders with Latent Sparse {G}aussian Processes
firstpage: 34409
lastpage: 34430
page: 34409-34430
order: 34409
cycles: false
bibtex_author: Tran, Ba-Hien and Shahbaba, Babak and Mandt, Stephan and Filippone,
  Maurizio
author:
- given: Ba-Hien
  family: Tran
- given: Babak
  family: Shahbaba
- given: Stephan
  family: Mandt
- given: Maurizio
  family: Filippone
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/tran23a/tran23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
