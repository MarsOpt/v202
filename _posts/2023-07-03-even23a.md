---
title: Stochastic Gradient Descent under Markovian Sampling Schemes
openreview: Wc7XppSfRo
abstract: We study a variation of vanilla stochastic gradient descent where the optimizer
  only has access to a Markovian sampling scheme. These schemes encompass applications
  that range from decentralized optimization with a random walker (token algorithms),
  to RL and online system identification problems. We focus on obtaining rates of
  convergence under the least restrictive assumptions possible on the underlying Markov
  chain and on the functions optimized. We first unveil the theoretical lower bound
  for methods that sample stochastic gradients along the path of a Markov chain, making
  appear a dependency in the hitting time of the underlying Markov chain. We then
  study Markov chain SGD (MC-SGD) under much milder regularity assumptions than prior
  works. We finally introduce MC-SAG, an alternative to MC-SGD with variance reduction,
  that only depends on the hitting time of the Markov chain, therefore obtaining a
  communication-efficient token algorithm.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: even23a
month: 0
tex_title: Stochastic Gradient Descent under {M}arkovian Sampling Schemes
firstpage: 9412
lastpage: 9439
page: 9412-9439
order: 9412
cycles: false
bibtex_author: Even, Mathieu
author:
- given: Mathieu
  family: Even
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/even23a/even23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
