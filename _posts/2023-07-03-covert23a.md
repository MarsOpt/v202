---
title: Learning to Maximize Mutual Information for Dynamic Feature Selection
openreview: dOaCuOsdmb
abstract: Feature selection helps reduce data acquisition costs in ML, but the standard
  approach is to train models with static feature subsets. Here, we consider the dynamic
  feature selection (DFS) problem where a model sequentially queries features based
  on the presently available information. DFS is often addressed with reinforcement
  learning, but we explore a simpler approach of greedily selecting features based
  on their conditional mutual information. This method is theoretically appealing
  but requires oracle access to the data distribution, so we develop a learning approach
  based on amortized optimization. The proposed method is shown to recover the greedy
  policy when trained to optimality, and it outperforms numerous existing feature
  selection methods in our experiments, thus validating it as a simple but powerful
  approach for this problem.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: covert23a
month: 0
tex_title: Learning to Maximize Mutual Information for Dynamic Feature Selection
firstpage: 6424
lastpage: 6447
page: 6424-6447
order: 6424
cycles: false
bibtex_author: Covert, Ian Connick and Qiu, Wei and Lu, Mingyu and Kim, Na Yoon and
  White, Nathan J and Lee, Su-In
author:
- given: Ian Connick
  family: Covert
- given: Wei
  family: Qiu
- given: Mingyu
  family: Lu
- given: Na Yoon
  family: Kim
- given: Nathan J
  family: White
- given: Su-In
  family: Lee
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/covert23a/covert23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
