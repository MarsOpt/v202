---
title: 'Beyond Reward: Offline Preference-guided Policy Optimization'
openreview: 0BgDXE6vJJ
abstract: 'This study focuses on the topic of offline preference-based reinforcement
  learning (PbRL), a variant of conventional reinforcement learning that dispenses
  with the need for online interaction or specification of reward functions. Instead,
  the agent is provided with fixed offline trajectories and human preferences between
  pairs of trajectories to extract the dynamics and task information, respectively.
  Since the dynamics and task information are orthogonal, a naive approach would involve
  using preference-based reward learning followed by an off-the-shelf offline RL algorithm.
  However, this requires the separate learning of a scalar reward function, which
  is assumed to be an information bottleneck of the learning process. To address this
  issue, we propose the offline preference-guided policy optimization (OPPO) paradigm,
  which models offline trajectories and preferences in a one-step process, eliminating
  the need for separately learning a reward function. OPPO achieves this by introducing
  an offline hindsight information matching objective for optimizing a contextual
  policy and a preference modeling objective for finding the optimal context. OPPO
  further integrates a well-performing decision policy by optimizing the two objectives
  iteratively. Our empirical results demonstrate that OPPO effectively models offline
  preferences and outperforms prior competing baselines, including offline RL algorithms
  performed over either true or pseudo reward function specifications. Our code is
  available on the project website: https://sites.google.com/view/oppo-icml-2023.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kang23b
month: 0
tex_title: 'Beyond Reward: Offline Preference-guided Policy Optimization'
firstpage: 15753
lastpage: 15768
page: 15753-15768
order: 15753
cycles: false
bibtex_author: Kang, Yachen and Shi, Diyuan and Liu, Jinxin and He, Li and Wang, Donglin
author:
- given: Yachen
  family: Kang
- given: Diyuan
  family: Shi
- given: Jinxin
  family: Liu
- given: Li
  family: He
- given: Donglin
  family: Wang
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/kang23b/kang23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
