---
title: Dual Focal Loss for Calibration
openreview: VQiPMxKCdm
abstract: The use of deep neural networks in real-world applications require well-calibrated
  networks with confidence scores that accurately reflect the actual probability.
  However, it has been found that these networks often provide over-confident predictions,
  which leads to poor calibration. Recent efforts have sought to address this issue
  by focal loss to reduce over-confidence, but this approach can also lead to under-confident
  predictions. While different variants of focal loss have been explored, it is difficult
  to find a balance between over-confidence and under-confidence. In our work, we
  propose a new loss function by focusing on dual logits. Our method not only considers
  the ground truth logit, but also take into account the highest logit ranked after
  the ground truth logit. By maximizing the gap between these two logits, our proposed
  dual focal loss can achieve a better balance between over-confidence and under-confidence.
  We provide theoretical evidence to support our approach and demonstrate its effectiveness
  through evaluations on multiple models and datasets, where it achieves state-of-the-art
  performance. Code is available at https://github.com/Linwei94/DualFocalLoss
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tao23a
month: 0
tex_title: Dual Focal Loss for Calibration
firstpage: 33833
lastpage: 33849
page: 33833-33849
order: 33833
cycles: false
bibtex_author: Tao, Linwei and Dong, Minjing and Xu, Chang
author:
- given: Linwei
  family: Tao
- given: Minjing
  family: Dong
- given: Chang
  family: Xu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/tao23a/tao23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
