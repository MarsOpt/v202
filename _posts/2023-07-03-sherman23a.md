---
title: Improved Regret for Efficient Online Reinforcement Learning with Linear Function
  Approximation
openreview: DF6ypWrepg
abstract: We study reinforcement learning with linear function approximation and adversarially
  changing cost functions, a setup that has mostly been considered under simplifying
  assumptions such as full information feedback or exploratory conditions. We present
  a computationally efficient policy optimization algorithm for the challenging general
  setting of unknown dynamics and bandit feedback, featuring a combination of mirror-descent
  and least squares policy evaluation in an auxiliary MDP used to compute exploration
  bonuses. Our algorithm obtains an $\widetilde O(K^{6/7})$ regret bound, improving
  significantly over previous state-of-the-art of $\widetilde O (K^{14/15})$ in this
  setting. In addition, we present a version of the same algorithm under the assumption
  a simulator of the environment is available to the learner (but otherwise no exploratory
  assumptions are made), and prove it obtains state-of-the-art regret of $\widetilde
  O (K^{2/3})$.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sherman23a
month: 0
tex_title: Improved Regret for Efficient Online Reinforcement Learning with Linear
  Function Approximation
firstpage: 31117
lastpage: 31150
page: 31117-31150
order: 31117
cycles: false
bibtex_author: Sherman, Uri and Koren, Tomer and Mansour, Yishay
author:
- given: Uri
  family: Sherman
- given: Tomer
  family: Koren
- given: Yishay
  family: Mansour
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/sherman23a/sherman23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
