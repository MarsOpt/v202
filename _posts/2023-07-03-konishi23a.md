---
title: Parameter-Level Soft-Masking for Continual Learning
openreview: wxFXvPdVqi
abstract: 'Existing research on task incremental learning in continual learning has
  primarily focused on preventing catastrophic forgetting (CF). Although several techniques
  have achieved learning with no CF, they attain it by letting each task monopolize
  a sub-network in a shared network, which seriously limits knowledge transfer (KT)
  and causes over-consumption of the network capacity, i.e., as more tasks are learned,
  the performance deteriorates. The goal of this paper is threefold: (1) overcoming
  CF, (2) encouraging KT, and (3) tackling the capacity problem. A novel technique
  (called SPG) is proposed that soft-masks (partially blocks) parameter updating in
  training based on the importance of each parameter to old tasks. Each task still
  uses the full network, i.e., no monopoly of any part of the network by any task,
  which enables maximum KT and reduction in capacity usage. To our knowledge, this
  is the first work that soft-masks a model at the parameter-level for continual learning.
  Extensive experiments demonstrate the effectiveness of SPG in achieving all three
  objectives. More notably, it attains significant transfer of knowledge not only
  among similar tasks (with shared knowledge) but also among dissimilar tasks (with
  little shared knowledge) while mitigating CF.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: konishi23a
month: 0
tex_title: Parameter-Level Soft-Masking for Continual Learning
firstpage: 17492
lastpage: 17505
page: 17492-17505
order: 17492
cycles: false
bibtex_author: Konishi, Tatsuya and Kurokawa, Mori and Ono, Chihiro and Ke, Zixuan
  and Kim, Gyuhak and Liu, Bing
author:
- given: Tatsuya
  family: Konishi
- given: Mori
  family: Kurokawa
- given: Chihiro
  family: Ono
- given: Zixuan
  family: Ke
- given: Gyuhak
  family: Kim
- given: Bing
  family: Liu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/konishi23a/konishi23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
