---
title: One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale
openreview: Urp3atR1Z3
abstract: This paper proposes a unified diffusion framework (dubbed UniDiffuser) to
  fit all distributions relevant to a set of multi-modal data in one model. Our key
  insight is – learning diffusion models for marginal, conditional, and joint distributions
  can be unified as predicting the noise in the perturbed data, where the perturbation
  levels (i.e. timesteps) can be different for different modalities. Inspired by the
  unified view, UniDiffuser learns all distributions simultaneously with a minimal
  modification to the original diffusion model – perturbs data in all modalities instead
  of a single modality, inputs individual timesteps in different modalities, and predicts
  the noise of all modalities instead of a single modality. UniDiffuser is parameterized
  by a transformer for diffusion models to handle input types of different modalities.
  Implemented on large-scale paired image-text data, UniDiffuser is able to perform
  image, text, text-to-image, image-to-text, and image-text pair generation by setting
  proper timesteps without additional overhead. In particular, UniDiffuser is able
  to produce perceptually realistic samples in all tasks and its quantitative results
  (e.g., the FID and CLIP score) are not only superior to existing general-purpose
  models but also comparable to the bespoken models (e.g., Stable Diffusion and DALL-E
  2) in representative tasks (e.g., text-to-image generation).
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bao23a
month: 0
tex_title: One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale
firstpage: 1692
lastpage: 1717
page: 1692-1717
order: 1692
cycles: false
bibtex_author: Bao, Fan and Nie, Shen and Xue, Kaiwen and Li, Chongxuan and Pu, Shi
  and Wang, Yaole and Yue, Gang and Cao, Yue and Su, Hang and Zhu, Jun
author:
- given: Fan
  family: Bao
- given: Shen
  family: Nie
- given: Kaiwen
  family: Xue
- given: Chongxuan
  family: Li
- given: Shi
  family: Pu
- given: Yaole
  family: Wang
- given: Gang
  family: Yue
- given: Yue
  family: Cao
- given: Hang
  family: Su
- given: Jun
  family: Zhu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/bao23a/bao23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
