---
title: "$\\pi$-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task
  Interpolation"
openreview: B5dh3UDjVs
abstract: Foundation models have achieved great advances in multi-task learning with
  a unified interface of unimodal and multimodal tasks. However, the potential of
  such multi-task learners has not been exploited during transfer learning. In this
  work, we present a universal parameter-efficient transfer learning method, termed
  Predict-Interpolate Tuning ($\pi$-Tuning), for vision, language, and vision-language
  tasks. It aggregates the parameters of lightweight task-specific experts learned
  from similar tasks to aid the target downstream task. The task similarities are
  predicted in a unified modality-independent space, yielding a scalable graph to
  demonstrate task relationships. $\pi$-Tuning has several appealing benefits. First,
  it flexibly explores both intra- and inter-modal transferability between similar
  tasks to improve the accuracy and robustness of transfer learning, especially in
  data-scarce scenarios. Second, it offers a systematical solution for transfer learning
  with multi-task prediction-and-then-interpolation, compatible with diverse types
  of parameter-efficient experts, such as prompt and adapter. Third, an extensive
  study of task-level mutual benefits on 14 unimodal and 6 multimodal datasets shows
  that $\pi$-Tuning surpasses fine-tuning and other parameter-efficient transfer learning
  methods both in full-shot and low-shot regimes. The task graph also enables an in-depth
  interpretable analysis of task transferability across modalities. The code will
  be available at https://github.com/TencentARC/pi-Tuning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu23t
month: 0
tex_title: "$\\pi$-Tuning: Transferring Multimodal Foundation Models with Optimal
  Multi-task Interpolation"
firstpage: 37713
lastpage: 37727
page: 37713-37727
order: 37713
cycles: false
bibtex_author: Wu, Chengyue and Wang, Teng and Ge, Yixiao and Lu, Zeyu and Zhou, Ruisong
  and Shan, Ying and Luo, Ping
author:
- given: Chengyue
  family: Wu
- given: Teng
  family: Wang
- given: Yixiao
  family: Ge
- given: Zeyu
  family: Lu
- given: Ruisong
  family: Zhou
- given: Ying
  family: Shan
- given: Ping
  family: Luo
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wu23t/wu23t.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
