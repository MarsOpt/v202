---
title: Improving Hyperparameter Learning under Approximate Inference in Gaussian Process
  Models
openreview: 8xAHeICO69
abstract: 'Approximate inference in Gaussian process (GP) models with non-conjugate
  likelihoods gets entangled with the learning of the model hyperparameters. We improve
  hyperparameter learning in GP models and focus on the interplay between variational
  inference (VI) and the learning target. While VIâ€™s lower bound to the marginal likelihood
  is a suitable objective for inferring the approximate posterior, we show that a
  direct approximation of the marginal likelihood as in Expectation Propagation (EP)
  is a better learning objective for hyperparameter optimization. We design a hybrid
  training procedure to bring the best of both worlds: it leverages conjugate-computation
  VI for inference and uses an EP-like marginal likelihood approximation for hyperparameter
  learning. We compare VI, EP, Laplace approximation, and our proposed training procedure
  and empirically demonstrate the effectiveness of our proposal across a wide range
  of data sets.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li23m
month: 0
tex_title: Improving Hyperparameter Learning under Approximate Inference in {G}aussian
  Process Models
firstpage: 19595
lastpage: 19615
page: 19595-19615
order: 19595
cycles: false
bibtex_author: Li, Rui and John, S. T. and Solin, Arno
author:
- given: Rui
  family: Li
- given: S. T.
  family: John
- given: Arno
  family: Solin
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/li23m/li23m.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
