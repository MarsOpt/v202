---
title: Revisiting Structured Variational Autoencoders
openreview: 0kRayuGKOP
abstract: Structured variational autoencoders (SVAEs) combine probabilistic graphical
  model priors on latent variables, deep neural networks to link latent variables
  to observed data, and structure-exploiting algorithms for approximate posterior
  inference. These models are particularly appealing for sequential data, where the
  prior can capture temporal dependencies. However, despite their conceptual elegance,
  SVAEs have proven difficult to implement, and more general approaches have been
  favored in practice. Here, we revisit SVAEs using modern machine learning tools
  and demonstrate their advantages over more general alternatives in terms of both
  accuracy and efficiency. First, we develop a modern implementation for hardware
  acceleration, parallelization, and automatic differentiation of the message passing
  algorithms at the core of the SVAE. Second, we show that by exploiting structure
  in the prior, the SVAE learns more accurate models and posterior distributions,
  which translate into improved performance on prediction tasks. Third, we show how
  the SVAE can naturally handle missing data, and we leverage this ability to develop
  a novel, self-supervised training approach. Altogether, these results show that
  the time is ripe to revisit structured variational autoencoders.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao23c
month: 0
tex_title: Revisiting Structured Variational Autoencoders
firstpage: 42046
lastpage: 42057
page: 42046-42057
order: 42046
cycles: false
bibtex_author: Zhao, Yixiu and Linderman, Scott
author:
- given: Yixiu
  family: Zhao
- given: Scott
  family: Linderman
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/zhao23c/zhao23c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
