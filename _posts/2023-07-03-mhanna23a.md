---
title: Single Point-Based Distributed Zeroth-Order Optimization with a Non-Convex
  Stochastic Objective Function
openreview: jqeMV8LrCB
abstract: Zero-order (ZO) optimization is a powerful tool for dealing with realistic
  constraints. On the other hand, the gradient-tracking (GT) technique proved to be
  an efficient method for distributed optimization aiming to achieve consensus. However,
  it is a first-order (FO) method that requires knowledge of the gradient, which is
  not always possible in practice. In this work, we introduce a zero-order distributed
  optimization method based on a one-point estimate of the gradient tracking technique.
  We prove that this new technique converges with a single noisy function query at
  a time in the non-convex setting. We then establish a convergence rate of $O(\frac{1}{\sqrt[3]{K}})$
  after a number of iterations K, which competes with that of $O(\frac{1}{\sqrt[4]{K}})$
  of its centralized counterparts. Finally, a numerical example validates our theoretical
  results.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mhanna23a
month: 0
tex_title: Single Point-Based Distributed Zeroth-Order Optimization with a Non-Convex
  Stochastic Objective Function
firstpage: 24701
lastpage: 24719
page: 24701-24719
order: 24701
cycles: false
bibtex_author: Mhanna, Elissa and Assaad, Mohamad
author:
- given: Elissa
  family: Mhanna
- given: Mohamad
  family: Assaad
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/mhanna23a/mhanna23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
