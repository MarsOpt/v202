---
title: Mitigating Memorization of Noisy Labels by Clipping the Model Prediction
openreview: g0ofsq1NRL
abstract: In the presence of noisy labels, designing robust loss functions is critical
  for securing the generalization performance of deep neural networks. Cross Entropy
  (CE) loss has been shown to be not robust to noisy labels due to its unboundedness.
  To alleviate this issue, existing works typically design specialized robust losses
  with the symmetric condition, which usually lead to the underfitting issue. In this
  paper, our key idea is to induce a loss bound at the logit level, thus universally
  enhancing the noise robustness of existing losses. Specifically, we propose logit
  clipping (LogitClip), which clamps the norm of the logit vector to ensure that it
  is upper bounded by a constant. In this manner, CE loss equipped with our LogitClip
  method is effectively bounded, mitigating the overfitting to examples with noisy
  labels. Moreover, we present theoretical analyses to certify the noise-tolerant
  ability of LogitClip. Extensive experiments show that LogitClip not only significantly
  improves the noise robustness of CE loss, but also broadly enhances the generalization
  performance of popular robust losses.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wei23e
month: 0
tex_title: Mitigating Memorization of Noisy Labels by Clipping the Model Prediction
firstpage: 36868
lastpage: 36886
page: 36868-36886
order: 36868
cycles: false
bibtex_author: Wei, Hongxin and Zhuang, Huiping and Xie, Renchunzi and Feng, Lei and
  Niu, Gang and An, Bo and Li, Yixuan
author:
- given: Hongxin
  family: Wei
- given: Huiping
  family: Zhuang
- given: Renchunzi
  family: Xie
- given: Lei
  family: Feng
- given: Gang
  family: Niu
- given: Bo
  family: An
- given: Yixuan
  family: Li
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/wei23e/wei23e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
