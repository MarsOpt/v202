---
title: Discrete Key-Value Bottleneck
openreview: LDBIVZCnLl
abstract: Deep neural networks perform well on classification tasks where data streams
  are i.i.d. and labeled data is abundant. Challenges emerge with non-stationary training
  data streams such as continual learning. One powerful approach that has addressed
  this challenge involves pre-training of large encoders on volumes of readily available
  data, followed by task-specific tuning. Given a new task, however, updating the
  weights of these encoders is challenging as a large number of weights needs to be
  fine-tuned, and as a result, they forget information about the previous tasks. In
  the present work, we propose a model architecture to address this issue, building
  upon a discrete bottleneck containing pairs of separate and learnable key-value
  codes. Our paradigm will be to encode; process the representation via a discrete
  bottleneck; and decode. Here, the input is fed to the pre-trained encoder, the output
  of the encoder is used to select the nearest keys, and the corresponding values
  are fed to the decoder to solve the current task. The model can only fetch and re-use
  a sparse number of these key-value pairs during inference, enabling <em>localized
  and context-dependent model updates</em>. We theoretically investigate the ability
  of the discrete key-value bottleneck to minimize the effect of learning under distribution
  shifts and show that it reduces the complexity of the hypothesis class. We empirically
  verify the proposed method under challenging class-incremental learning scenarios
  and show that the proposed model — without any task boundaries — reduces catastrophic
  forgetting across a wide variety of pre-trained models, outperforming relevant baselines
  on this task.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: trauble23a
month: 0
tex_title: Discrete Key-Value Bottleneck
firstpage: 34431
lastpage: 34455
page: 34431-34455
order: 34431
cycles: false
bibtex_author: Tr\"{a}uble, Frederik and Goyal, Anirudh and Rahaman, Nasim and Mozer,
  Michael Curtis and Kawaguchi, Kenji and Bengio, Yoshua and Sch\"{o}lkopf, Bernhard
author:
- given: Frederik
  family: Träuble
- given: Anirudh
  family: Goyal
- given: Nasim
  family: Rahaman
- given: Michael Curtis
  family: Mozer
- given: Kenji
  family: Kawaguchi
- given: Yoshua
  family: Bengio
- given: Bernhard
  family: Schölkopf
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/trauble23a/trauble23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
