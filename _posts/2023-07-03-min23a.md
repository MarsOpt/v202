---
title: 'Cooperative Multi-Agent Reinforcement Learning: Asynchronous Communication
  and Linear Function Approximation'
openreview: UDzgqDZc7Q
abstract: We study multi-agent reinforcement learning in the setting of episodic Markov
  decision processes, where many agents cooperate via communication through a central
  server. We propose a provably efficient algorithm based on value iteration that
  can simultaneously allow asynchronous communication and guarantee the benefit of
  cooperation with low communication complexity. Under linear function approximation,
  we prove that our algorithm enjoys a $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret
  upper bound with $\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$
  is the feature dimension, $H$ is the horizon length, $M$ is the total number of
  agents, and $K$ is the total number of episodes. We also provide a lower bound showing
  that an $\Omega(dM)$ communication complexity is necessary to improve the performance
  through collaboration.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: min23a
month: 0
tex_title: 'Cooperative Multi-Agent Reinforcement Learning: Asynchronous Communication
  and Linear Function Approximation'
firstpage: 24785
lastpage: 24811
page: 24785-24811
order: 24785
cycles: false
bibtex_author: Min, Yifei and He, Jiafan and Wang, Tianhao and Gu, Quanquan
author:
- given: Yifei
  family: Min
- given: Jiafan
  family: He
- given: Tianhao
  family: Wang
- given: Quanquan
  family: Gu
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/min23a/min23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
