---
title: 'Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples
  for Supervised Learning Contribute the Least'
openreview: fxy6ao4t7r
abstract: Self-supervised learning (SSL) learns high-quality representations from
  large pools of unlabeled training data. As datasets grow larger, it becomes crucial
  to identify the examples that contribute the most to learning such representations.
  This enables efficient SSL by reducing the volume of data required. Nevertheless,
  quantifying the value of examples for SSL has remained an open question. In this
  work, we address this problem for the first time, by proving that examples that
  contribute the most to contrastive SSL are those that have the most similar augmentations
  to other examples, in expectation. We provide rigorous guarantees for the generalization
  performance of contrastive learning on such subsets. Through extensive experiments,
  we show that we can safely exclude 20% of examples from CIFAR100 and 40% from STL10
  and TinyImageNet, without affecting downstream task performance. In general, subsets
  selected by our method outperform random subsets by over 3% across these datasets.
  Interestingly, we also discover the subsets that contribute the most to contrastive
  learning are those that contribute the least to supervised learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: joshi23b
month: 0
tex_title: 'Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples
  for Supervised Learning Contribute the Least'
firstpage: 15356
lastpage: 15370
page: 15356-15370
order: 15356
cycles: false
bibtex_author: Joshi, Siddharth and Mirzasoleiman, Baharan
author:
- given: Siddharth
  family: Joshi
- given: Baharan
  family: Mirzasoleiman
date: 2023-07-03
address: 
container-title: Proceedings of the 40th International Conference on Machine Learning
volume: '202'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 3
pdf: https://proceedings.mlr.press/v202/joshi23b/joshi23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
